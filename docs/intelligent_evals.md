# Intelligent Evaluations

This guide explains how to run automated evaluations and use the self-improvement agent introduced in v2.1.

## Quick start

```python
from pydantic_ai_orchestrator.application.eval_adapter import run_pipeline_async
from pydantic_ai_orchestrator.application.self_improvement import evaluate_and_improve, SelfImprovementAgent
from pydantic_ai_orchestrator.application.pipeline_runner import PipelineRunner
from pydantic_ai_orchestrator.domain import Step
from pydantic_evals import Dataset, Case
from pydantic_ai_orchestrator.infra.agents import self_improvement_agent

pipeline = Step.solution(lambda x: x)
runner = PipelineRunner(pipeline)
dataset = Dataset(cases=[Case(inputs="hi", expected_output="hi")])
agent = SelfImprovementAgent(self_improvement_agent)
report = await evaluate_and_improve(
    lambda x: run_pipeline_async(x, runner=runner),
    dataset,
    agent,
)
print(report)
```

The `ImprovementReport` contains structured suggestions for updating your pipeline or evaluation suite.

## ImprovementSuggestion Model

The self-improvement agent returns an `ImprovementReport` which contains a list of `ImprovementSuggestion` objects. Each suggestion has a `suggestion_type` indicating the general category of improvement along with additional fields describing the issue and proposed fix.

```
class ImprovementSuggestion(BaseModel):
    target_step_name: Optional[str]
    suggestion_type: SuggestionType
    failure_pattern_summary: str
    detailed_explanation: str
    prompt_modification_details: Optional[PromptModificationDetail]
    config_change_details: Optional[List[ConfigChangeDetail]]
    example_failing_input_snippets: List[str]
    suggested_new_eval_case_description: Optional[str]
    estimated_impact: Optional[Literal["HIGH", "MEDIUM", "LOW"]]
    estimated_effort_to_implement: Optional[Literal["HIGH", "MEDIUM", "LOW"]]
```

`SuggestionType` values include things like `PROMPT_MODIFICATION`, `CONFIG_ADJUSTMENT`, and `NEW_EVAL_CASE`. For prompt modifications or config adjustments, the relevant detail objects provide the exact change proposed.

## End-to-end Example

1. Define a simple pipeline and dataset with a failing case.
2. Run `orch improve pipeline.py data.py`.
3. Review the suggestions printed in the formatted table.
4. Apply one of the suggested prompt tweaks.
5. Re-run the evaluation to see the improvement.

Suggestions are advisory and may vary in quality depending on the underlying LLM.
### Interpreting Suggestion Types
The `suggestion_type` field indicates how you might act on the advice:
- **PROMPT_MODIFICATION** – adjust the text of a step's system prompt as described.
- **CONFIG_ADJUSTMENT** – tweak temperature or other parameters in the step configuration.
- **NEW_EVAL_CASE / EVAL_CASE_REFINEMENT** – create or refine dataset cases to exercise the pipeline more thoroughly.
- **OTHER** – miscellaneous guidance not captured by the above categories.

### Dataset Best Practices
When authoring evaluation datasets:
- Provide clear `expected_output` values so failures are easy to diagnose.
- Give cases descriptive names using the `name` field.
- Include metadata if extra context helps the agent understand the scenario.

### Limitations
Self‑improvement suggestions are generated by an LLM and should be reviewed
critically. The agent does not modify your code automatically.
