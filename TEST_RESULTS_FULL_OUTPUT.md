Makefile:181: warning: overriding commands for target `test-health'
Makefile:120: warning: ignoring old commands for target `test-health'
ðŸ” Running fast tests with verbose output...
CI=1 uv run pytest tests/ -m "not slow and not serial and not benchmark" -n 4 -v
/Users/alvaro/Documents/Code/flujo/.venv/lib/python3.11/site-packages/pytest_benchmark/logger.py:39: PytestBenchmarkWarning: Benchmarks are automatically disabled because xdist plugin is active.Benchmarks cannot be performed reliably in a parallelized environment.
  warner(PytestBenchmarkWarning(text))
============================= test session starts ==============================
platform darwin -- Python 3.11.0, pytest-8.4.1, pluggy-1.6.0 -- /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
cachedir: .pytest_cache
benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
hypothesis profile 'ci' -> database=None, deadline=None, print_blob=True, derandomize=True, suppress_health_check=(HealthCheck.too_slow,)
rootdir: /Users/alvaro/Documents/Code/flujo
configfile: pyproject.toml
plugins: logfire-4.0.1, anyio-4.9.0, asyncio-1.1.0, xdist-3.8.0, cov-6.2.1, benchmark-5.1.0, hypothesis-6.136.6
asyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
created: 4/4 workers
4 workers [2214 items]

scheduling tests via LoadScheduling

tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_updates_state_before_usage_check 
tests/integration/test_FSD11_bug_fix.py::test_fsd11_signature_analysis_fix 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_context_preservation 
tests/integration/test_default_recipe.py::test_default_recipe_data_flow 
[gw0] [  0%] PASSED tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_updates_state_before_usage_check 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_context_preservation 
tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_handles_multiple_steps_before_breach 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_basic_message 
[gw0] [  0%] PASSED tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_handles_multiple_steps_before_breach 
tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_no_usage_limits 
[gw0] [  0%] PASSED tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_no_usage_limits 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_basic_message 
tests/application/core/test_execution_manager_state_handling.py::test_usage_governor_check_usage_limits_efficient 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_custom_message 
[gw0] [  0%] PASSED tests/application/core/test_execution_manager_state_handling.py::test_usage_governor_check_usage_limits_efficient 
tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_failed_step_handling 
[gw3] [  0%] PASSED tests/integration/test_default_recipe.py::test_default_recipe_data_flow 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_custom_message 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_scratchpad 
[gw0] [  0%] PASSED tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_failed_step_handling 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_basic_context_updates 
tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_step_coordinator_integration 
[gw0] [  0%] PASSED tests/application/core/test_execution_manager_state_handling.py::test_execution_manager_step_coordinator_integration 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_scratchpad 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_successful_run_no_retries 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_paused_exception 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_paused_exception 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_data_handling 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_data_handling 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_preservation 
[gw3] [  0%] PASSED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_basic_context_updates 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_multiple_branches_context_updates 
[gw1] [  0%] FAILED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_preservation 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_scratchpad_updates 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_scratchpad_updates 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_isolation 
[gw3] [  0%] FAILED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_multiple_branches_context_updates 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_router_failure_context_preservation 
[gw3] [  0%] PASSED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_router_failure_context_preservation 
[gw1] [  0%] FAILED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_isolation 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_branch_failure_context_preservation 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_serialization 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_serialization 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_message_generation_errors 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_message_generation_errors 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_update_errors 
[gw1] [  0%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_update_errors 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_exception_propagation 
[gw3] [  1%] FAILED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_branch_failure_context_preservation 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_nested_context_updates 
[gw1] [  1%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_exception_propagation 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_invalid_context 
[gw3] [  1%] PASSED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_nested_context_updates 
[gw1] [  1%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_invalid_context 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_message_performance 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_context_field_mapping 
[gw1] [  1%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_message_performance 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_performance 
[gw3] [  1%] PASSED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_context_field_mapping 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_large_context_performance 
[gw1] [  1%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_performance 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_memory_usage 
[gw3] [  1%] PASSED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_large_context_performance 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_high_frequency_context_updates 
[gw3] [  1%] PASSED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_high_frequency_context_updates 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_empty_branch_selection 
[gw3] [  1%] PASSED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_empty_branch_selection 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_invalid_branch_selection 
[gw3] [  1%] PASSED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_invalid_branch_selection 
tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_complex_context_objects 
[gw3] [  1%] PASSED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_complex_context_objects 
tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_context_parameter_fix 
[gw3] [  1%] PASSED tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_context_parameter_fix 
tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_multiple_branches_context_fix 
[gw3] [  1%] FAILED tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_multiple_branches_context_fix 
tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_empty_selection_context_fix 
[gw3] [  1%] PASSED tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_empty_selection_context_fix 
tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_context_preservation_on_failure 
[gw3] [  1%] PASSED tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_context_preservation_on_failure 
tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_no_context_requirement 
[gw3] [  1%] PASSED tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_no_context_requirement 
tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_basic 
[gw3] [  1%] PASSED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_basic 
tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_numeric 
[gw3] [  1%] PASSED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_numeric 
tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_multiple_branches 
[gw3] [  1%] PASSED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_multiple_branches 
tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_error_handling 
[gw1] [  1%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_memory_usage 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_serialization_performance 
[gw1] [  1%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_serialization_performance 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_none_context 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_none_context 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_empty_message 
[gw0] [  2%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_successful_run_no_retries 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_successful_run_with_retry 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_empty_message 
[gw3] [  2%] FAILED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_error_handling 
tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_state_isolation 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_unicode_data 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_unicode_data 
tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_binary_data 
[gw3] [  2%] PASSED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_state_isolation 
tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_complex_routing 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_binary_data 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_method_exists 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_method_exists 
[gw0] [  2%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_successful_run_with_retry 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_signature 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_all_retries_failed 
[gw3] [  2%] PASSED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_complex_routing 
tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_router_metadata 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_signature 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_basic_execution 
[gw3] [  2%] PASSED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_router_metadata 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_basic_execution 
tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_nested_routing 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_error_handling 
[gw0] [  2%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_all_retries_failed 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_validator_failure_triggers_retry 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_error_handling 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_recursive_execution 
[gw3] [  2%] PASSED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_nested_routing 
tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_success 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_recursive_execution 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_parameter_passing 
[gw3] [  2%] PASSED tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_success 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_parameter_passing 
tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_strict_mode_failure 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_with_none_parameters 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_with_none_parameters 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_with_complex_limits 
[gw1] [  2%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_with_complex_limits 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_step_executor_functionality 
[gw3] [  2%] FAILED tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_strict_mode_failure 
tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_non_strict_mode 
[gw1] [  3%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_step_executor_functionality 
tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_step_executor_with_extra_kwargs 
[gw0] [  3%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_validator_failure_triggers_retry 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_usage_limit_exceeded_error_propagates 
[gw3] [  3%] PASSED tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_non_strict_mode 
tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_multiple_texts 
[gw1] [  3%] PASSED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_step_executor_with_extra_kwargs 
tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_routes_loopstep_to_new_handler 
[gw3] [  3%] PASSED tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_multiple_texts 
tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_with_chat_agent 
[gw1] [  3%] FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_routes_loopstep_to_new_handler 
tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_parameter_passing 
[gw3] [  3%] PASSED tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_with_chat_agent 
tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_model_pricing_configuration 
[gw1] [  3%] FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_parameter_passing 
[gw3] [  3%] PASSED tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_model_pricing_configuration 
tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_model_pricing_validation 
tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_legacy_import_removed 
[gw3] [  3%] PASSED tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_model_pricing_validation 
tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_existing_chat_cost_tracking_still_works 
[gw1] [  3%] FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_legacy_import_removed 
tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_error_propagation 
[gw3] [  3%] PASSED tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_existing_chat_cost_tracking_still_works 
tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_mixed_pipeline_cost_tracking 
[gw0] [  3%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_usage_limit_exceeded_error_propagates 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_missing_agent_error 
[gw1] [  3%] FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_error_propagation 
[gw3] [  3%] PASSED tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_mixed_pipeline_cost_tracking 
tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_telemetry_logging 
tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_strict_mode_behavior_unchanged 
[gw1] [  3%] FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_telemetry_logging 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_basic_iteration 
[gw1] [  3%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_basic_iteration 
[gw3] [  3%] FAILED tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_strict_mode_behavior_unchanged 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_with_exit_condition 
tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_non_strict_mode_behavior_unchanged 
[gw1] [  3%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_with_exit_condition 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_max_iterations 
[gw3] [  3%] PASSED tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_non_strict_mode_behavior_unchanged 
tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_configuration_loading_unchanged 
[gw3] [  3%] PASSED tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_configuration_loading_unchanged 
tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_basic 
[gw3] [  3%] PASSED tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_basic 
[gw0] [  4%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_missing_agent_error 
tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_successful_recovery 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_mock_object_detection 
[gw1] [  4%] FAILED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_max_iterations 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_input_mappers 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_input_mappers 
[gw3] [  4%] PASSED tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_successful_recovery 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_iteration_mappers 
tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_context_dependent 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_iteration_mappers 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_output_mappers 
[gw3] [  4%] PASSED tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_context_dependent 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_output_mappers 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_isolation 
tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_state_isolation 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_isolation 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_merge 
[gw3] [  4%] PASSED tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_state_isolation 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_merge 
tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_complex_recovery 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_preservation 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_preservation 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_mapper_errors 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_mapper_errors 
[gw3] [  4%] PASSED tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_complex_recovery 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_exit_condition_errors 
tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_metadata_conflicts 
[gw0] [  4%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_mock_object_detection 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_exit_condition_errors 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_body_step_failures 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_validation_failure_with_feedback 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_body_step_failures 
[gw3] [  4%] PASSED tests/integration/test_error_recovery_with_context_updates.py::test_error_recovery_with_context_updates_metadata_conflicts 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_cost_limits 
tests/integration/test_evals_adapter.py::test_adapter_returns_pipeline_result 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_cost_limits 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_token_limits 
[gw1] [  4%] FAILED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_token_limits 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_limits_accumulation 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_limits_accumulation 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_complex_pipeline_integration 
[gw1] [  4%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_complex_pipeline_integration 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_nested_control_flow 
[gw0] [  4%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_validation_failure_with_feedback 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_failure_propagates 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_nested_control_flow 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_with_caching 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_with_caching 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_with_telemetry 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_with_telemetry 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_existing_behavior_preservation 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_existing_behavior_preservation 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_edge_cases_regression 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_edge_cases_regression 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_error_scenarios_regression 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_error_scenarios_regression 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_migration_performance_improvement 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_migration_performance_improvement 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_memory_usage 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_memory_usage 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_concurrent_execution 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_concurrent_execution 
tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_execution_performance 
[gw1] [  5%] PASSED tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_execution_performance 
tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_with_multiple_branches_performance 
[gw1] [  5%] PASSED tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_with_multiple_branches_performance 
tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_with_input_mapping_performance 
[gw1] [  5%] PASSED tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_with_input_mapping_performance 
tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_with_output_mapping_performance 
[gw1] [  5%] PASSED tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_with_output_mapping_performance 
tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_memory_usage_performance 
[gw1] [  5%] PASSED tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_memory_usage_performance 
tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_concurrent_execution_performance 
[gw1] [  5%] PASSED tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_concurrent_execution_performance 
tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_error_handling_performance 
[gw1] [  5%] PASSED tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_error_handling_performance 
tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_optimization_impact 
[gw1] [  5%] PASSED tests/benchmarks/test_conditional_step_performance.py::TestConditionalStepPerformance::test_conditional_step_optimization_impact 
tests/benchmarks/test_performance_optimizations 2.py::TestPerfCounterPrecision::test_perf_counter_standard 
[gw0] [  5%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_failure_propagates 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_runner_not_called_when_plugins_empty 
[gw3] [  5%] PASSED tests/integration/test_evals_adapter.py::test_adapter_returns_pipeline_result 
tests/integration/test_execution_backend_protocol.py::test_pipeline_runs_correctly_with_custom_backend 
[gw3] [  5%] PASSED tests/integration/test_execution_backend_protocol.py::test_pipeline_runs_correctly_with_custom_backend 
tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_component_interface_optimization 
[gw0] [  5%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_runner_not_called_when_plugins_empty 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_runner_not_called_when_plugins_none 
[gw3] [  5%] FAILED tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_component_interface_optimization 
[gw0] [  6%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_runner_not_called_when_plugins_none 
tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_dependency_injection_performance 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_caching_behavior 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_dependency_injection_performance 
tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_component_lifecycle_optimization 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_component_lifecycle_optimization 
tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_error_handling_optimization 
[gw0] [  6%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_caching_behavior 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_error_handling_optimization 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_usage_tracking 
tests/integration/test_executor_core_architecture_validation.py::TestScalabilityValidation::test_concurrent_step_execution 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestScalabilityValidation::test_concurrent_step_execution 
tests/integration/test_executor_core_architecture_validation.py::TestScalabilityValidation::test_resource_management_optimization 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestScalabilityValidation::test_resource_management_optimization 
tests/integration/test_executor_core_architecture_validation.py::TestScalabilityValidation::test_usage_limit_enforcement_performance 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestScalabilityValidation::test_usage_limit_enforcement_performance 
tests/integration/test_executor_core_architecture_validation.py::TestScalabilityValidation::test_telemetry_performance 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestScalabilityValidation::test_telemetry_performance 
tests/integration/test_executor_core_architecture_validation.py::TestArchitecturalIntegrity::test_interface_compliance 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestArchitecturalIntegrity::test_interface_compliance 
tests/integration/test_executor_core_architecture_validation.py::TestArchitecturalIntegrity::test_component_isolation 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestArchitecturalIntegrity::test_component_isolation 
tests/integration/test_executor_core_architecture_validation.py::TestArchitecturalIntegrity::test_backward_compatibility 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestArchitecturalIntegrity::test_backward_compatibility 
tests/integration/test_executor_core_architecture_validation.py::TestArchitecturalIntegrity::test_configuration_flexibility 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestArchitecturalIntegrity::test_configuration_flexibility 
tests/integration/test_executor_core_architecture_validation.py::TestPerformanceRegression::test_no_performance_regression 
[gw3] [  6%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestPerformanceRegression::test_no_performance_regression 
tests/integration/test_executor_core_architecture_validation.py::TestPerformanceRegression::test_memory_regression 
[gw0] [  6%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_usage_tracking 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_streaming_behavior 
[gw0] [  6%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_streaming_behavior 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_feedback_accumulation 
[gw0] [  6%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_feedback_accumulation 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_pricing_not_configured_error_propagates 
[gw0] [  6%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_pricing_not_configured_error_propagates 
tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_context_persistence 
[gw0] [  6%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_context_persistence 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_is_complex_property_detection 
[gw0] [  6%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_is_complex_property_detection 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_fallback_steps_are_simple 
[gw0] [  6%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_fallback_steps_are_simple 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_complex_steps_remain_complex 
[gw0] [  6%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_complex_steps_remain_complex 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_validation_steps_remain_complex 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_validation_steps_remain_complex 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_plugin_steps_remain_complex 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_plugin_steps_remain_complex 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_simple_steps_without_fallbacks 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_simple_steps_without_fallbacks 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_steps_with_none_fallback 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_steps_with_none_fallback 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_cache_steps_remain_complex 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_cache_steps_remain_complex 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_conditional_steps_remain_complex 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_conditional_steps_remain_complex 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_hitl_steps_remain_complex 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_hitl_steps_remain_complex 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_dynamic_router_steps_remain_complex 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_dynamic_router_steps_remain_complex 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_steps_with_fallbacks_and_plugins 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_steps_with_fallbacks_and_plugins 
tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_steps_with_fallbacks_and_validation 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreComplexStepClassification::test_steps_with_fallbacks_and_validation 
tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_successful_fallback_execution 
[gw3] [  7%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestPerformanceRegression::test_memory_regression 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_execution 
[gw3] [  7%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_execution 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_failure 
[gw0] [  7%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_successful_fallback_execution 
tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_failed_fallback_execution 
[gw3] [  7%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_failure 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_not_triggered_on_success 
[gw3] [  7%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_not_triggered_on_success 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_complex_data_types 
[gw3] [  7%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_complex_data_types 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_context_updates 
[gw0] [  7%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_failed_fallback_execution 
tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_metric_accounting 
[gw3] [  7%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_context_updates 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_usage_limits 
[gw3] [  7%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_usage_limits 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_streaming 
[gw3] [  7%] FAILED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_streaming 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_multiple_retries 
[gw3] [  7%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_multiple_retries 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_validation_failure 
[gw3] [  7%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_validation_failure 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_plugin_failure 
[gw0] [  7%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_metric_accounting 
tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_latency_accumulation 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_plugin_failure 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_cache_interaction 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_cache_interaction 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_telemetry_logging 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_telemetry_logging 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_usage_meter_tracking 
[gw0] [  8%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_latency_accumulation 
tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_with_none_feedback 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_usage_meter_tracking 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_processor_pipeline 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_processor_pipeline 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_plugin_runner 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_plugin_runner 
[gw0] [  8%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_with_none_feedback 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_cache_backend 
tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_execution_exception_handling 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_cache_backend 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_telemetry 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_telemetry 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_integration_with_real_executor 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_integration_with_real_executor 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_backward_compatibility 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_backward_compatibility 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_critical_exceptions 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_critical_exceptions 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_pricing_not_configured 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_pricing_not_configured 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_missing_agent_error 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_missing_agent_error 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_breach_event 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_breach_event 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_none_feedback 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_none_feedback 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_latency_accumulation 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_latency_accumulation 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_metric_accounting_success 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_metric_accounting_success 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_metric_accounting_failure 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_metric_accounting_failure 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_metadata_preservation 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_metadata_preservation 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_no_fallback_step 
[gw3] [  8%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_no_fallback_step 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_execution_exception_handling 
[gw3] [  9%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_execution_exception_handling 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_migration_integration 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_migration_integration 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_migration_integration 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_migration_integration 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_with_usage_limits 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_with_usage_limits 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_with_complex_branches 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_with_complex_branches 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_context_isolation 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_context_isolation 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_context_propagation 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_context_propagation 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_migration_backward_compatibility 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_migration_backward_compatibility 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_edge_cases 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_edge_cases 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_edge_cases 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_edge_cases 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_migration_performance 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_migration_performance 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_with_input_output_mappers 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_with_input_output_mappers 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_with_input_output_mappers 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_with_input_output_mappers 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_with_context_modifications 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_with_context_modifications 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_with_context_modifications 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_with_context_modifications 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_error_handling 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_error_handling 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_error_handling 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_error_handling 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_with_resources_and_limits 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_with_resources_and_limits 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_with_resources_and_limits 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_with_resources_and_limits 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_complex_scenarios 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_loopstep_complex_scenarios 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_complex_scenarios 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_conditionalstep_complex_scenarios 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_migration_telemetry_integration 
[gw3] [  9%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_migration_telemetry_integration 
tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_migration_cache_integration 
[gw3] [ 10%] PASSED tests/integration/test_executor_core_loop_conditional_migration.py::TestExecutorCoreLoopConditionalMigration::test_migration_cache_integration 
tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_end_to_end_optimization_workflow 
[gw3] [ 10%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_end_to_end_optimization_workflow 
tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_optimization_component_interaction 
[gw3] [ 10%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_optimization_component_interaction 
tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_optimization_vs_baseline_performance 
[gw3] [ 10%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_optimization_vs_baseline_performance 
tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_concurrent_optimization_integration 
[gw3] [ 10%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_concurrent_optimization_integration 
tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_memory_optimization_integration 
[gw0] [ 10%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_execution_exception_handling 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_object_oriented_property_detection 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_object_oriented_property_detection 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_without_is_complex_property 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_without_is_complex_property 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_false_is_complex_property 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_false_is_complex_property 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_true_is_complex_property 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_true_is_complex_property 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_validation_steps_backward_compatibility 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_validation_steps_backward_compatibility 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_plugin_steps_backward_compatibility 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_plugin_steps_backward_compatibility 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_basic_steps_without_special_properties 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_basic_steps_without_special_properties 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_empty_plugins_list 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_empty_plugins_list 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_none_plugins 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_none_plugins 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_empty_meta 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_empty_meta 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_none_meta 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_none_meta 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_meta_but_no_validation_flag 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_meta_but_no_validation_flag 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_false_validation_flag 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_false_validation_flag 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_complex_nested_workflow_compatibility 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_complex_nested_workflow_compatibility 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_edge_case_missing_name_attribute 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_edge_case_missing_name_attribute 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_edge_case_step_with_dynamic_properties 
[gw0] [ 10%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_edge_case_step_with_dynamic_properties 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_edge_case_step_with_property_descriptor 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_edge_case_step_with_property_descriptor 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_edge_case_step_with_callable_is_complex 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_edge_case_step_with_callable_is_complex 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_comprehensive_step_type_coverage 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_comprehensive_step_type_coverage 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_object_oriented_principle_verification 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_object_oriented_principle_verification 
tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_basic_steps 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_basic_steps 
tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_complex_step_types 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_complex_step_types 
tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_validation_steps 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_validation_steps 
tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_plugin_steps 
[gw0] [ 11%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_plugin_steps 
tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_edge_cases 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_edge_cases 
tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_comprehensive_coverage 
[gw0] [ 11%] FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_comprehensive_coverage 
tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_no_behavioral_changes 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_no_behavioral_changes 
tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_backward_compatibility 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_backward_compatibility 
tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_key_improvement 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_key_improvement 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_method_exists 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_method_exists 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_signature 
[gw0] [ 11%] FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_signature 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_basic_execution 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_basic_execution 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_error_handling 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_error_handling 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_recursive_execution 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_recursive_execution 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_parameter_passing 
[gw0] [ 11%] FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_parameter_passing 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_step_executor_functionality 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_step_executor_functionality 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_with_limits_and_context_setter 
[gw0] [ 11%] FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_with_limits_and_context_setter 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_null_parameters 
[gw0] [ 11%] PASSED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_null_parameters 
tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_integration_with_execute_complex_step 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_integration_with_execute_complex_step 
tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_routes_conditionalstep_to_new_handler 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_routes_conditionalstep_to_new_handler 
tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_parameter_passing 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_parameter_passing 
tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_no_legacy_import 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_no_legacy_import 
tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_error_propagation 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_error_propagation 
tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_telemetry_logging 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_telemetry_logging 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_condition_evaluation_success 
[gw0] [ 12%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_condition_evaluation_success 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_condition_evaluation_failure 
[gw0] [ 12%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_condition_evaluation_failure 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_not_found_no_default 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_not_found_no_default 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_not_found_with_default 
[gw0] [ 12%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_not_found_with_default 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_success 
[gw0] [ 12%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_success 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_failure 
[gw0] [ 12%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_failure 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_input_mapping 
[gw0] [ 12%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_input_mapping 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_output_mapping 
[gw0] [ 12%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_output_mapping 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_output_mapper_exception 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_output_mapper_exception 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_metrics_accumulation 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_metrics_accumulation 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_metadata_executed_branch_key 
[gw0] [ 12%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_metadata_executed_branch_key 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_context_setter_called_on_success 
[gw3] [ 12%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestOptimizationComponentIntegration::test_memory_optimization_integration 
tests/integration/test_executor_core_optimization_integration.py::TestResourceManagementIntegration::test_adaptive_resource_management_integration 
[gw3] [ 12%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestResourceManagementIntegration::test_adaptive_resource_management_integration 
tests/integration/test_executor_core_optimization_integration.py::TestResourceManagementIntegration::test_load_balancer_integration 
[gw3] [ 12%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestResourceManagementIntegration::test_load_balancer_integration 
tests/integration/test_executor_core_optimization_integration.py::TestResourceManagementIntegration::test_graceful_degradation_integration 
[gw3] [ 12%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestResourceManagementIntegration::test_graceful_degradation_integration 
tests/integration/test_executor_core_optimization_integration.py::TestErrorHandlingIntegration::test_error_recovery_integration 
[gw0] [ 12%] FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_context_setter_called_on_success 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_multiple_branch_steps_execution 
[gw0] [ 13%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_multiple_branch_steps_execution 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_telemetry_logging 
[gw0] [ 13%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_telemetry_logging 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_error_handling_with_context 
[gw0] [ 13%] PASSED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_error_handling_with_context 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_with_resources 
[gw0] [ 13%] FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_with_resources 
tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_with_limits 
[gw2] [ 13%] PASSED tests/integration/test_FSD11_bug_fix.py::test_fsd11_signature_analysis_fix 
tests/integration/test_FSD11_bug_fix.py::test_fsd11_context_filtering_works 
[gw2] [ 13%] PASSED tests/integration/test_FSD11_bug_fix.py::test_fsd11_context_filtering_works 
tests/integration/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_integration 
[gw3] [ 13%] FAILED tests/integration/test_executor_core_optimization_integration.py::TestErrorHandlingIntegration::test_error_recovery_integration 
[gw0] [ 13%] FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_with_limits 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_not_triggered_on_primary_success 
tests/integration/test_executor_core_optimization_integration.py::TestErrorHandlingIntegration::test_circuit_breaker_integration 
[gw2] [ 13%] PASSED tests/integration/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_integration 
tests/integration/test_agentic_loop_recipe.py::test_agent_delegation_and_finish 
[gw0] [ 13%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_not_triggered_on_primary_success 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_triggered_on_primary_failure 
[gw0] [ 13%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_triggered_on_primary_failure 
[gw2] [ 13%] PASSED tests/integration/test_agentic_loop_recipe.py::test_agent_delegation_and_finish 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_failure_propagates 
tests/integration/test_agentic_loop_recipe.py::test_pause_and_resume_in_loop 
[gw0] [ 13%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_failure_propagates 
[gw2] [ 13%] FAILED tests/integration/test_agentic_loop_recipe.py::test_pause_and_resume_in_loop 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_metric_accounting_success 
tests/integration/test_agentic_loop_recipe.py::test_pause_preserves_command_log 
[gw2] [ 13%] PASSED tests/integration/test_agentic_loop_recipe.py::test_pause_preserves_command_log 
tests/integration/test_agentic_loop_recipe.py::test_sync_resume 
[gw0] [ 13%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_metric_accounting_success 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_metric_accounting_failure 
[gw0] [ 13%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_metric_accounting_failure 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_latency_accumulation 
[gw0] [ 13%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_latency_accumulation 
[gw3] [ 13%] FAILED tests/integration/test_executor_core_optimization_integration.py::TestErrorHandlingIntegration::test_circuit_breaker_integration 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_none_feedback 
tests/integration/test_executor_core_optimization_integration.py::TestErrorHandlingIntegration::test_error_handling_with_telemetry 
[gw3] [ 13%] SKIPPED tests/integration/test_executor_core_optimization_integration.py::TestErrorHandlingIntegration::test_error_handling_with_telemetry 
tests/integration/test_executor_core_optimization_integration.py::TestTelemetryIntegration::test_telemetry_data_collection_integration 
[gw3] [ 13%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestTelemetryIntegration::test_telemetry_data_collection_integration 
tests/integration/test_executor_core_optimization_integration.py::TestTelemetryIntegration::test_performance_monitoring_integration 
[gw3] [ 13%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestTelemetryIntegration::test_performance_monitoring_integration 
tests/integration/test_executor_core_optimization_integration.py::TestTelemetryIntegration::test_telemetry_overhead_measurement 
[gw3] [ 14%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestTelemetryIntegration::test_telemetry_overhead_measurement 
tests/integration/test_executor_core_optimization_integration.py::TestConfigurationIntegration::test_dynamic_optimization_configuration 
[gw3] [ 14%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestConfigurationIntegration::test_dynamic_optimization_configuration 
tests/integration/test_executor_core_optimization_integration.py::TestConfigurationIntegration::test_optimization_configuration_validation 
[gw3] [ 14%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestConfigurationIntegration::test_optimization_configuration_validation 
tests/integration/test_executor_core_optimization_integration.py::TestConfigurationIntegration::test_optimization_feature_interaction 
[gw3] [ 14%] PASSED tests/integration/test_executor_core_optimization_integration.py::TestConfigurationIntegration::test_optimization_feature_interaction 
tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_end_to_end_success_case 
[gw2] [ 14%] FAILED tests/integration/test_agentic_loop_recipe.py::test_sync_resume 
tests/integration/test_agentic_loop_recipe.py::test_max_loops_failure 
[gw3] [ 14%] PASSED tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_end_to_end_success_case 
tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_usage_limits 
[gw3] [ 14%] PASSED tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_usage_limits 
tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_token_counts 
[gw2] [ 14%] PASSED tests/integration/test_agentic_loop_recipe.py::test_max_loops_failure 
[gw3] [ 14%] PASSED tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_token_counts 
tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_zero_cost 
tests/integration/test_as_step_composition.py::test_agentic_loop_as_composable_step 
[gw3] [ 14%] PASSED tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_zero_cost 
tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_negative_cost 
[gw3] [ 14%] PASSED tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_negative_cost 
tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_multiple_steps 
[gw3] [ 14%] PASSED tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_multiple_steps 
tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_protocol_priority_over_usage_method 
[gw2] [ 14%] PASSED tests/integration/test_as_step_composition.py::test_agentic_loop_as_composable_step 
tests/integration/test_as_step_composition.py::test_pipeline_of_pipelines_via_as_step 
[gw3] [ 14%] PASSED tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_protocol_priority_over_usage_method 
tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_none_values 
[gw3] [ 14%] PASSED tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_with_none_values 
tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_regression_with_usage_method 
[gw2] [ 14%] PASSED tests/integration/test_as_step_composition.py::test_pipeline_of_pipelines_via_as_step 
tests/integration/test_as_step_composition.py::test_as_step_context_propagation 
[gw3] [ 14%] PASSED tests/integration/test_explicit_cost_integration.py::TestExplicitCostIntegration::test_explicit_cost_regression_with_usage_method 
tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_generation_and_persistence 
[gw2] [ 14%] PASSED tests/integration/test_as_step_composition.py::test_as_step_context_propagation 
tests/integration/test_as_step_composition.py::test_as_step_resource_propagation 
[gw2] [ 14%] PASSED tests/integration/test_as_step_composition.py::test_as_step_resource_propagation 
tests/integration/test_as_step_composition.py::test_as_step_initial_prompt_sync 
[gw2] [ 14%] PASSED tests/integration/test_as_step_composition.py::test_as_step_initial_prompt_sync 
tests/integration/test_as_step_composition.py::test_as_step_inherit_context_false 
[gw0] [ 14%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_none_feedback 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_execution_exception_handling 
[gw2] [ 14%] PASSED tests/integration/test_as_step_composition.py::test_as_step_inherit_context_false 
tests/integration/test_as_step_composition.py::test_as_step_context_inheritance_error 
[gw2] [ 14%] PASSED tests/integration/test_as_step_composition.py::test_as_step_context_inheritance_error 
tests/integration/test_as_step_composition.py::test_direct_context_inheritance_error 
[gw2] [ 15%] PASSED tests/integration/test_as_step_composition.py::test_direct_context_inheritance_error 
tests/integration/test_as_step_state_persistence.py::test_as_step_state_persistence_and_resumption 
[gw3] [ 15%] PASSED tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_generation_and_persistence 
tests/integration/test_nested_dsl_constructs.py::test_deeply_nested_error_propagation 
[gw3] [ 15%] PASSED tests/integration/test_nested_dsl_constructs.py::test_deeply_nested_error_propagation 
tests/integration/test_nested_dsl_constructs.py::test_deeply_nested_metric_aggregation 
[gw3] [ 15%] PASSED tests/integration/test_nested_dsl_constructs.py::test_deeply_nested_metric_aggregation 
tests/integration/test_orchestrator_sync.py::test_orchestrator_run_sync 
[gw3] [ 15%] PASSED tests/integration/test_orchestrator_sync.py::test_orchestrator_run_sync 
tests/integration/test_otel_integration.py::test_pipeline_runs_with_otel_hook 
[gw3] [ 15%] PASSED tests/integration/test_otel_integration.py::test_pipeline_runs_with_otel_hook 
tests/integration/test_parallel_step.py::test_parallel_step_context_isolation 
[gw3] [ 15%] PASSED tests/integration/test_parallel_step.py::test_parallel_step_context_isolation 
tests/integration/test_parallel_step.py::test_parallel_step_result_structure 
[gw3] [ 15%] PASSED tests/integration/test_parallel_step.py::test_parallel_step_result_structure 
tests/integration/test_parallel_step.py::test_parallel_merge_scratchpad 
[gw3] [ 15%] PASSED tests/integration/test_parallel_step.py::test_parallel_merge_scratchpad 
tests/integration/test_parallel_step.py::test_parallel_overwrite_conflict 
[gw0] [ 15%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_execution_exception_handling 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_usage_limits 
[gw0] [ 15%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_usage_limits 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_streaming 
[gw0] [ 15%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_streaming 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_context_and_resources 
[gw0] [ 15%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_context_and_resources 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_metadata_preservation 
[gw0] [ 15%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_metadata_preservation 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_no_fallback_step 
[gw3] [ 15%] PASSED tests/integration/test_parallel_step.py::test_parallel_overwrite_conflict 
tests/integration/test_parallel_step.py::test_parallel_overwrite_preserves_context 
[gw3] [ 15%] PASSED tests/integration/test_parallel_step.py::test_parallel_overwrite_preserves_context 
tests/integration/test_parallel_step.py::test_parallel_overwrite_multi_branch_order 
[gw0] [ 15%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_no_fallback_step 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_critical_exceptions 
[gw3] [ 15%] FAILED tests/integration/test_parallel_step.py::test_parallel_overwrite_multi_branch_order 
tests/integration/test_parallel_step.py::test_parallel_propagate_failure 
[gw3] [ 15%] PASSED tests/integration/test_parallel_step.py::test_parallel_propagate_failure 
tests/integration/test_parallel_step.py::test_parallel_ignore_failure 
[gw3] [ 15%] PASSED tests/integration/test_parallel_step.py::test_parallel_ignore_failure 
tests/integration/test_parallel_step.py::test_parallel_ignore_failure_all_fail 
[gw3] [ 15%] PASSED tests/integration/test_parallel_step.py::test_parallel_ignore_failure_all_fail 
tests/integration/test_parallel_step.py::test_governor_precedence_over_failure_strategy 
[gw0] [ 15%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_critical_exceptions 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_pricing_not_configured 
[gw0] [ 16%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_pricing_not_configured 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_missing_agent_error 
[gw0] [ 16%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_missing_agent_error 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_validation_failure 
[gw0] [ 16%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_validation_failure 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_plugin_failure 
[gw0] [ 16%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_plugin_failure 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_cache_hit 
[gw0] [ 16%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_cache_hit 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_breach_event 
[gw0] [ 16%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_breach_event 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_complex_data_types 
[gw0] [ 16%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_complex_data_types 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_multiple_retries 
[gw0] [ 16%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_multiple_retries 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_telemetry_logging 
[gw0] [ 16%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_telemetry_logging 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_usage_meter_tracking 
[gw0] [ 16%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_usage_meter_tracking 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_processor_pipeline 
[gw0] [ 16%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_processor_pipeline 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_plugin_runner 
[gw0] [ 16%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_plugin_runner 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_cache_backend 
[gw2] [ 16%] FAILED tests/integration/test_as_step_state_persistence.py::test_as_step_state_persistence_and_resumption 
tests/integration/test_async_runner_detection.py::test_run_succeeds_in_synchronous_context 
[gw2] [ 16%] PASSED tests/integration/test_async_runner_detection.py::test_run_succeeds_in_synchronous_context 
tests/integration/test_async_runner_detection.py::test_run_raises_type_error_in_asynchronous_context 
[gw2] [ 16%] PASSED tests/integration/test_async_runner_detection.py::test_run_raises_type_error_in_asynchronous_context 
tests/integration/test_async_runner_detection.py::test_run_async_is_unaffected_and_works_correctly 
[gw2] [ 16%] PASSED tests/integration/test_async_runner_detection.py::test_run_async_is_unaffected_and_works_correctly 
tests/integration/test_async_runner_detection.py::test_run_in_simulated_jupyter_environment 
[gw2] [ 16%] PASSED tests/integration/test_async_runner_detection.py::test_run_in_simulated_jupyter_environment 
tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_basic 
[gw2] [ 16%] PASSED tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_basic 
tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_error_handling 
[gw2] [ 16%] PASSED tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_error_handling 
tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_context_dependent 
[gw2] [ 16%] PASSED tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_context_dependent 
tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_state_isolation 
[gw2] [ 16%] PASSED tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_state_isolation 
tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_complex_interaction 
[gw2] [ 16%] PASSED tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_complex_interaction 
tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_metadata_conflicts 
[gw2] [ 17%] PASSED tests/integration/test_cache_with_context_updates.py::test_cache_with_context_updates_metadata_conflicts 
tests/integration/test_caching_and_fallbacks.py::test_caching_pipeline_speed_and_hits 
[gw0] [ 17%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_cache_backend 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_telemetry 
[gw2] [ 17%] FAILED tests/integration/test_caching_and_fallbacks.py::test_caching_pipeline_speed_and_hits 
tests/integration/test_caching_and_fallbacks.py::test_cache_keys_distinct_for_same_name_steps 
[gw2] [ 17%] PASSED tests/integration/test_caching_and_fallbacks.py::test_cache_keys_distinct_for_same_name_steps 
tests/integration/test_caching_and_fallbacks.py::test_pipeline_step_fallback 
[gw2] [ 17%] PASSED tests/integration/test_caching_and_fallbacks.py::test_pipeline_step_fallback 
[gw0] [ 17%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_telemetry 
tests/integration/test_caching_and_fallbacks.py::test_loop_step_fallback_continues 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_integration_with_real_executor 
[gw0] [ 17%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_integration_with_real_executor 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_integration_real_pipeline 
[gw2] [ 17%] PASSED tests/integration/test_caching_and_fallbacks.py::test_loop_step_fallback_continues 
tests/integration/test_caching_and_fallbacks.py::test_conditional_branch_with_fallback 
[gw2] [ 17%] PASSED tests/integration/test_caching_and_fallbacks.py::test_conditional_branch_with_fallback 
tests/integration/test_caching_and_fallbacks.py::test_cache_with_custom_type_in_input 
[gw0] [ 17%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_integration_real_pipeline 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_on_plugin_failure 
[gw2] [ 17%] PASSED tests/integration/test_caching_and_fallbacks.py::test_cache_with_custom_type_in_input 
tests/integration/test_caching_and_fallbacks.py::test_cache_custom_key_serialization_and_hit 
[gw2] [ 17%] PASSED tests/integration/test_caching_and_fallbacks.py::test_cache_custom_key_serialization_and_hit 
tests/integration/test_cli_mermaid.py::test_pipeline_mermaid_command 
[gw2] [ 17%] PASSED tests/integration/test_cli_mermaid.py::test_pipeline_mermaid_command 
tests/integration/test_cli_run.py::test_run_basic 
[gw0] [ 17%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_on_plugin_failure 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_on_validator_failure 
[gw2] [ 17%] PASSED tests/integration/test_cli_run.py::test_run_basic 
tests/integration/test_cli_run.py::test_run_with_context_json 
[gw0] [ 17%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_on_validator_failure 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_on_complex_failure_chain 
[gw0] [ 17%] FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_on_complex_failure_chain 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_retry_logic 
[gw0] [ 17%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_retry_logic 
[gw2] [ 17%] PASSED tests/integration/test_cli_run.py::test_run_with_context_json 
tests/integration/test_cli_run.py::test_run_with_context_yaml 
tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_streaming_output 
[gw0] [ 17%] PASSED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_streaming_output 
tests/integration/test_redirect_loop_unhashable.py::test_redirect_loop_detected_with_unhashable_agents 
[gw0] [ 17%] FAILED tests/integration/test_redirect_loop_unhashable.py::test_redirect_loop_detected_with_unhashable_agents 
tests/integration/test_refine_until.py::test_refine_until_basic 
[gw0] [ 17%] PASSED tests/integration/test_refine_until.py::test_refine_until_basic 
tests/integration/test_refine_until.py::test_refine_until_with_feedback_mapper 
[gw2] [ 18%] PASSED tests/integration/test_cli_run.py::test_run_with_context_yaml 
tests/integration/test_cli_run.py::test_run_missing_pipeline 
[gw0] [ 18%] PASSED tests/integration/test_refine_until.py::test_refine_until_with_feedback_mapper 
tests/integration/test_refine_until.py::test_refine_until_with_custom_context 
[gw0] [ 18%] PASSED tests/integration/test_refine_until.py::test_refine_until_with_custom_context 
tests/integration/test_refine_until.py::test_refine_until_concurrent_runs_isolated 
[gw2] [ 18%] PASSED tests/integration/test_cli_run.py::test_run_missing_pipeline 
tests/integration/test_cli_run.py::test_run_missing_context_model 
[gw0] [ 18%] PASSED tests/integration/test_refine_until.py::test_refine_until_concurrent_runs_isolated 
tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_basic 
[gw2] [ 18%] PASSED tests/integration/test_cli_run.py::test_run_missing_context_model 
tests/integration/test_conditional_step_execution.py::test_branch_a_executes 
[gw0] [ 18%] PASSED tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_basic 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_branch_a_executes 
tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_error_handling 
tests/integration/test_conditional_step_execution.py::test_branch_b_executes 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_branch_b_executes 
tests/integration/test_conditional_step_execution.py::test_default_branch_used 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_default_branch_used 
tests/integration/test_conditional_step_execution.py::test_no_match_no_default_fails 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_no_match_no_default_fails 
tests/integration/test_conditional_step_execution.py::test_condition_uses_context 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_condition_uses_context 
tests/integration/test_conditional_step_execution.py::test_mappers_applied 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_mappers_applied 
tests/integration/test_conditional_step_execution.py::test_failure_in_branch_propagates 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_failure_in_branch_propagates 
[gw0] [ 18%] FAILED tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_error_handling 
tests/integration/test_conditional_step_execution.py::test_condition_exception_fails_step 
tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_context_dependent 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_condition_exception_fails_step 
tests/integration/test_conditional_step_execution.py::test_conditional_step_error_in_condition_callable 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_error_in_condition_callable 
tests/integration/test_conditional_step_execution.py::test_conditional_step_error_in_branch_input_mapper 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_error_in_branch_input_mapper 
tests/integration/test_conditional_step_execution.py::test_conditional_step_error_in_branch_output_mapper 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_error_in_branch_output_mapper 
tests/integration/test_conditional_step_execution.py::test_conditional_step_branch_input_mapper_flow 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_branch_input_mapper_flow 
tests/integration/test_conditional_step_execution.py::test_conditional_step_branch_output_mapper_flow 
[gw0] [ 18%] PASSED tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_context_dependent 
tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_state_isolation 
[gw2] [ 18%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_branch_output_mapper_flow 
tests/integration/test_conditional_step_execution.py::test_conditional_step_mappers_with_context_modification 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_mappers_with_context_modification 
tests/integration/test_conditional_step_execution.py::test_conditional_step_default_mapper_behavior 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_default_mapper_behavior 
tests/integration/test_conditional_step_execution.py::test_conditional_step_overall_span 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_overall_span 
[gw0] [ 19%] PASSED tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_state_isolation 
tests/integration/test_conditional_step_execution.py::test_conditional_step_branch_selection_logging_and_span_attributes 
tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_complex_feedback 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_branch_selection_logging_and_span_attributes 
tests/integration/test_conditional_step_execution.py::test_conditional_step_no_branch_match_logging 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_no_branch_match_logging 
tests/integration/test_conditional_step_execution.py::test_conditional_step_error_logging_in_callables 
[gw3] [ 19%] PASSED tests/integration/test_parallel_step.py::test_governor_precedence_over_failure_strategy 
tests/integration/test_parallel_step_enhancements.py::test_context_include_keys_optimization 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_execution.py::test_conditional_step_error_logging_in_callables 
[gw0] [ 19%] PASSED tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_complex_feedback 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_integration_with_real_agents 
tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_metadata_conflicts 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_integration_with_real_agents 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_context_updates 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_context_updates 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_input_output_mapping 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_input_output_mapping 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_default_branch 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_default_branch 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_multiple_steps_in_branch 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_multiple_steps_in_branch 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_branch_failure 
[gw0] [ 19%] FAILED tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_metadata_conflicts 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_branch_failure 
tests/integration/test_resilience_features.py::test_cached_fallback_result_is_reused 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_complex_context 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_complex_context 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_resources_and_limits 
[gw0] [ 19%] PASSED tests/integration/test_resilience_features.py::test_cached_fallback_result_is_reused 
tests/integration/test_resilience_features.py::test_no_cache_when_fallback_fails 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_resources_and_limits 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_error_propagation 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_error_propagation 
tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_empty_branch 
[gw2] [ 19%] PASSED tests/integration/test_conditional_step_logic_migration.py::TestConditionalStepLogicMigration::test_conditional_step_with_empty_branch 
tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_basic 
[gw0] [ 19%] PASSED tests/integration/test_resilience_features.py::test_no_cache_when_fallback_fails 
tests/integration/test_self_improvement.py::test_e2e_self_improvement_with_mocked_llm_suggestions 
[gw2] [ 20%] PASSED tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_basic 
tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_error_handling 
[gw2] [ 20%] PASSED tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_error_handling 
tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_context_dependent 
[gw2] [ 20%] PASSED tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_context_dependent 
tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_state_isolation 
[gw2] [ 20%] PASSED tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_state_isolation 
tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_complex_branching 
[gw2] [ 20%] PASSED tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_complex_branching 
tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_metadata_conflicts 
[gw2] [ 20%] PASSED tests/integration/test_conditional_with_context_updates.py::test_conditional_with_context_updates_metadata_conflicts 
tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_configuration_file_loading_integration 
[gw2] [ 20%] PASSED tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_configuration_file_loading_integration 
tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_cli_defaults_integration 
[gw2] [ 20%] PASSED tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_cli_defaults_integration 
tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_configuration_precedence 
[gw2] [ 20%] PASSED tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_configuration_precedence 
tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_state_uri_configuration 
[gw2] [ 20%] PASSED tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_state_uri_configuration 
tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_configuration_file_discovery 
[gw2] [ 20%] PASSED tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_configuration_file_discovery 
tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_backward_compatibility 
[gw2] [ 20%] PASSED tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_backward_compatibility 
tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_error_handling 
[gw2] [ 20%] PASSED tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_error_handling 
tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_missing_config_file_handling 
[gw2] [ 20%] PASSED tests/integration/test_configuration_integration.py::TestConfigurationIntegration::test_missing_config_file_handling 
tests/integration/test_console_tracer_depth.py::test_console_tracer_indentation 
[gw2] [ 20%] PASSED tests/integration/test_console_tracer_depth.py::test_console_tracer_indentation 
tests/integration/test_console_tracer_depth.py::test_console_tracer_indentation_reset_on_failure 
[gw2] [ 20%] PASSED tests/integration/test_console_tracer_depth.py::test_console_tracer_indentation_reset_on_failure 
tests/integration/test_console_tracer_depth.py::test_console_tracer_pipeline_success 
[gw2] [ 20%] PASSED tests/integration/test_console_tracer_depth.py::test_console_tracer_pipeline_success 
tests/integration/test_console_tracer_depth.py::test_console_tracer_pipeline_failure 
[gw2] [ 20%] PASSED tests/integration/test_console_tracer_depth.py::test_console_tracer_pipeline_failure 
tests/integration/test_context_injection.py::test_stateless_agent_no_context_injection 
[gw2] [ 20%] PASSED tests/integration/test_context_injection.py::test_stateless_agent_no_context_injection 
tests/integration/test_context_injection.py::test_stateless_agent_custom_no_context_injection 
[gw2] [ 20%] PASSED tests/integration/test_context_injection.py::test_stateless_agent_custom_no_context_injection 
tests/integration/test_context_injection.py::test_context_aware_agent_explicit_context 
[gw2] [ 20%] PASSED tests/integration/test_context_injection.py::test_context_aware_agent_explicit_context 
tests/integration/test_context_injection.py::test_context_aware_agent_kwargs 
[gw2] [ 20%] PASSED tests/integration/test_context_injection.py::test_context_aware_agent_kwargs 
tests/integration/test_context_injection.py::test_error_propagation 
[gw2] [ 21%] PASSED tests/integration/test_context_injection.py::test_error_propagation 
tests/integration/test_context_injection.py::test_backward_compatibility_existing_context_aware 
[gw2] [ 21%] PASSED tests/integration/test_context_injection.py::test_backward_compatibility_existing_context_aware 
tests/integration/test_context_injection.py::test_mixed_pipeline_stateless_and_context_aware 
[gw2] [ 21%] PASSED tests/integration/test_context_injection.py::test_mixed_pipeline_stateless_and_context_aware 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug1FixUnconfiguredAgentCostCalculation::test_fix_agent_without_model_id_returns_zero_cost 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestBug1FixUnconfiguredAgentCostCalculation::test_fix_agent_without_model_id_returns_zero_cost 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug1FixUnconfiguredAgentCostCalculation::test_fix_agent_without_model_id_logs_warning 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestBug1FixUnconfiguredAgentCostCalculation::test_fix_agent_without_model_id_logs_warning 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug1FixUnconfiguredAgentCostCalculation::test_fix_integration_pipeline_with_agent_without_model_id 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestBug1FixUnconfiguredAgentCostCalculation::test_fix_integration_pipeline_with_agent_without_model_id 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug2FixParallelStepRaceCondition::test_fix_parallel_steps_with_atomic_usage_tracking 
[gw2] [ 21%] FAILED tests/integration/test_cost_tracking_bug_fixes.py::TestBug2FixParallelStepRaceCondition::test_fix_parallel_steps_with_atomic_usage_tracking 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug2FixParallelStepRaceCondition::test_fix_parallel_steps_breach_detection 
[gw0] [ 21%] PASSED tests/integration/test_self_improvement.py::test_e2e_self_improvement_with_mocked_llm_suggestions 
tests/integration/test_self_improvement.py::test_build_context_for_self_improvement_agent 
[gw0] [ 21%] PASSED tests/integration/test_self_improvement.py::test_build_context_for_self_improvement_agent 
tests/integration/test_self_improvement.py::test_self_improvement_context_includes_config_and_prompts 
[gw0] [ 21%] PASSED tests/integration/test_self_improvement.py::test_self_improvement_context_includes_config_and_prompts 
tests/integration/test_settings_validation.py::test_invalid_env_vars 
[gw0] [ 21%] PASSED tests/integration/test_settings_validation.py::test_invalid_env_vars 
tests/integration/test_silent_context_modification_fix.py::test_loop_step_context_modification_fix 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestBug2FixParallelStepRaceCondition::test_fix_parallel_steps_breach_detection 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug3FixBrittleProviderInference::test_fix_provider_inference_with_ambiguous_model_names 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestBug3FixBrittleProviderInference::test_fix_provider_inference_with_ambiguous_model_names 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug3FixBrittleProviderInference::test_fix_provider_inference_with_new_provider_patterns 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestBug3FixBrittleProviderInference::test_fix_provider_inference_with_new_provider_patterns 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug3FixBrittleProviderInference::test_fix_explicit_provider_format_is_recommended 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestBug3FixBrittleProviderInference::test_fix_explicit_provider_format_is_recommended 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug4FixMissingIntegrationTest::test_fix_agent_without_model_id_in_pipeline 
[gw0] [ 21%] PASSED tests/integration/test_silent_context_modification_fix.py::test_loop_step_context_modification_fix 
tests/integration/test_silent_context_modification_fix.py::test_parallel_step_context_modification_fix 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestBug4FixMissingIntegrationTest::test_fix_agent_without_model_id_in_pipeline 
tests/integration/test_cost_tracking_bug_fixes.py::TestBug4FixMissingIntegrationTest::test_fix_agent_without_model_id_with_usage_limits 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestBug4FixMissingIntegrationTest::test_fix_agent_without_model_id_with_usage_limits 
tests/integration/test_cost_tracking_bug_fixes.py::TestIntegrationBugFixes::test_fix_mixed_agent_types_in_pipeline 
[gw3] [ 21%] FAILED tests/integration/test_parallel_step_enhancements.py::test_context_include_keys_optimization 
tests/integration/test_parallel_step_enhancements.py::test_context_include_keys_isolation 
[gw0] [ 21%] PASSED tests/integration/test_silent_context_modification_fix.py::test_parallel_step_context_modification_fix 
tests/integration/test_silent_context_modification_fix.py::test_conditional_step_context_modification_fix 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestIntegrationBugFixes::test_fix_mixed_agent_types_in_pipeline 
tests/integration/test_cost_tracking_bug_fixes.py::TestIntegrationBugFixes::test_fix_cost_tracking_with_explicit_costs 
[gw2] [ 21%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestIntegrationBugFixes::test_fix_cost_tracking_with_explicit_costs 
tests/integration/test_cost_tracking_bug_fixes.py::TestIntegrationBugFixes::test_fix_cost_tracking_error_handling 
[gw0] [ 21%] PASSED tests/integration/test_silent_context_modification_fix.py::test_conditional_step_context_modification_fix 
tests/integration/test_silent_context_modification_fix.py::test_nested_complex_steps_context_modification_fix 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_bug_fixes.py::TestIntegrationBugFixes::test_fix_cost_tracking_error_handling 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug1UnconfiguredAgentCostCalculation::test_agent_without_model_id_returns_zero_cost 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestBug1UnconfiguredAgentCostCalculation::test_agent_without_model_id_returns_zero_cost 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug1UnconfiguredAgentCostCalculation::test_agent_without_model_id_logs_critical_warning 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestBug1UnconfiguredAgentCostCalculation::test_agent_without_model_id_logs_critical_warning 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug1UnconfiguredAgentCostCalculation::test_integration_pipeline_with_agent_without_model_id 
[gw0] [ 22%] PASSED tests/integration/test_silent_context_modification_fix.py::test_nested_complex_steps_context_modification_fix 
tests/integration/test_silent_context_modification_fix.py::test_context_modification_with_mappers 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestBug1UnconfiguredAgentCostCalculation::test_integration_pipeline_with_agent_without_model_id 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug2ParallelStepRaceCondition::test_parallel_steps_breach_limit_concurrently 
[gw0] [ 22%] PASSED tests/integration/test_silent_context_modification_fix.py::test_context_modification_with_mappers 
tests/integration/test_silent_context_modification_fix.py::test_context_modification_regression_prevention 
[gw0] [ 22%] PASSED tests/integration/test_silent_context_modification_fix.py::test_context_modification_regression_prevention 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_filename_conflicts_handling 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestBug2ParallelStepRaceCondition::test_parallel_steps_breach_limit_concurrently 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug2ParallelStepRaceCondition::test_parallel_steps_with_atomic_usage_tracking 
[gw2] [ 22%] FAILED tests/integration/test_cost_tracking_critical_bugs.py::TestBug2ParallelStepRaceCondition::test_parallel_steps_with_atomic_usage_tracking 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug3BrittleProviderInference::test_provider_inference_with_ambiguous_model_names 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestBug3BrittleProviderInference::test_provider_inference_with_ambiguous_model_names 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug3BrittleProviderInference::test_provider_inference_with_new_provider_patterns 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestBug3BrittleProviderInference::test_provider_inference_with_new_provider_patterns 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug3BrittleProviderInference::test_explicit_provider_format_is_recommended 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestBug3BrittleProviderInference::test_explicit_provider_format_is_recommended 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug4MissingIntegrationTest::test_agent_without_model_id_in_pipeline 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestBug4MissingIntegrationTest::test_agent_without_model_id_in_pipeline 
tests/integration/test_cost_tracking_critical_bugs.py::TestBug4MissingIntegrationTest::test_agent_without_model_id_with_usage_limits 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestBug4MissingIntegrationTest::test_agent_without_model_id_with_usage_limits 
tests/integration/test_cost_tracking_critical_bugs.py::TestIntegrationCostTrackingRobustness::test_mixed_agent_types_in_pipeline 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestIntegrationCostTrackingRobustness::test_mixed_agent_types_in_pipeline 
tests/integration/test_cost_tracking_critical_bugs.py::TestIntegrationCostTrackingRobustness::test_cost_tracking_with_explicit_costs 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestIntegrationCostTrackingRobustness::test_cost_tracking_with_explicit_costs 
tests/integration/test_cost_tracking_critical_bugs.py::TestIntegrationCostTrackingRobustness::test_cost_tracking_error_handling 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_critical_bugs.py::TestIntegrationCostTrackingRobustness::test_cost_tracking_error_handling 
tests/integration/test_cost_tracking_integration.py::test_cost_tracking_in_pipeline 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_integration.py::test_cost_tracking_in_pipeline 
tests/integration/test_cost_tracking_integration.py::test_cost_limit_enforcement 
[gw3] [ 22%] FAILED tests/integration/test_parallel_step_enhancements.py::test_context_include_keys_isolation 
tests/integration/test_parallel_step_enhancements.py::test_proactive_governor_cancellation 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_integration.py::test_cost_limit_enforcement 
tests/integration/test_cost_tracking_integration.py::test_token_limit_enforcement 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_integration.py::test_token_limit_enforcement 
tests/integration/test_cost_tracking_integration.py::test_cost_tracking_without_config 
[gw2] [ 22%] PASSED tests/integration/test_cost_tracking_integration.py::test_cost_tracking_without_config 
tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_on_success_case 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_on_success_case 
tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_on_failure_case 
[gw2] [ 23%] FAILED tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_on_failure_case 
tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_off_fallback_case 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_off_fallback_case 
tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_on_with_unknown_provider 
[gw2] [ 23%] FAILED tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_on_with_unknown_provider 
tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_regression_no_flujo_toml_present 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_regression_no_flujo_toml_present 
tests/integration/test_cost_tracking_mock.py::test_cost_tracking_basic 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_mock.py::test_cost_tracking_basic 
tests/integration/test_cost_tracking_mock.py::test_cost_tracking_with_limits 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_mock.py::test_cost_tracking_with_limits 
tests/integration/test_cost_tracking_mock.py::test_cost_tracking_with_token_limits 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_mock.py::test_cost_tracking_with_token_limits 
tests/integration/test_cost_tracking_mock.py::test_cost_tracking_no_usage_info 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_mock.py::test_cost_tracking_no_usage_info 
tests/integration/test_cost_tracking_mock.py::test_cost_tracking_multiple_steps 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_mock.py::test_cost_tracking_multiple_steps 
tests/integration/test_cost_tracking_real_agent.py::test_cost_tracking_with_real_agent 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_real_agent.py::test_cost_tracking_with_real_agent 
tests/integration/test_cost_tracking_real_agent.py::test_cost_tracking_without_pricing_config 
[gw2] [ 23%] PASSED tests/integration/test_cost_tracking_real_agent.py::test_cost_tracking_without_pricing_config 
tests/integration/test_crash_recovery.py::test_resume_after_crash_file_backend 
[gw0] [ 23%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_filename_conflicts_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_filename_conflicts_with_existing_files 
[gw1] [ 23%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestPerfCounterPrecision::test_perf_counter_standard 
tests/benchmarks/test_performance_optimizations 2.py::TestPerfCounterPrecision::test_perf_counter_ns_precision 
[gw0] [ 23%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_filename_conflicts_with_existing_files 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_rename_failure_fallback 
[gw0] [ 23%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_rename_failure_fallback 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_remove_failure_handling 
[gw2] [ 23%] FAILED tests/integration/test_crash_recovery.py::test_resume_after_crash_file_backend 
[gw0] [ 23%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_remove_failure_handling 
tests/integration/test_crash_recovery.py::test_resume_after_crash_sqlite_backend 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_special_characters_in_filename 
[gw3] [ 23%] PASSED tests/integration/test_parallel_step_enhancements.py::test_proactive_governor_cancellation 
tests/integration/test_parallel_step_enhancements.py::test_proactive_cancellation_with_multiple_branches 
[gw0] [ 23%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_special_characters_in_filename 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_very_long_filename 
[gw0] [ 23%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_very_long_filename 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_no_write_permissions 
[gw0] [ 23%] PASSED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_no_write_permissions 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_disk_full_scenario 
[gw0] [ 24%] PASSED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_disk_full_scenario 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_concurrent_backup_operations 
[gw2] [ 24%] FAILED tests/integration/test_crash_recovery.py::test_resume_after_crash_sqlite_backend 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationErrorHandling::test_resource_exhaustion_handling 
[gw2] [ 24%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationErrorHandling::test_resource_exhaustion_handling 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationErrorHandling::test_configuration_error_handling 
[gw2] [ 24%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationErrorHandling::test_configuration_error_handling 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationPerformanceRegression::test_execution_time_regression 
[gw2] [ 24%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationPerformanceRegression::test_execution_time_regression 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationPerformanceRegression::test_memory_usage_regression 
[gw2] [ 24%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationPerformanceRegression::test_memory_usage_regression 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationPerformanceRegression::test_concurrent_execution_performance 
[gw2] [ 24%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationPerformanceRegression::test_concurrent_execution_performance 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_component_interaction 
[gw2] [ 24%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_component_interaction 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_statistics_collection 
[gw2] [ 24%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_statistics_collection 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_configuration_management 
[gw2] [ 24%] FAILED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_configuration_management 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_performance_recommendations 
[gw2] [ 24%] FAILED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_performance_recommendations 
tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_existing_behavior_preservation 
[gw2] [ 24%] PASSED tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_existing_behavior_preservation 
tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_edge_cases_regression 
[gw2] [ 24%] PASSED tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_edge_cases_regression 
tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_error_scenarios_regression 
[gw2] [ 24%] PASSED tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_error_scenarios_regression 
tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_legacy_compatibility_comparison 
[gw2] [ 24%] PASSED tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_legacy_compatibility_comparison 
tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_backward_compatibility_regression 
[gw2] [ 24%] PASSED tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_backward_compatibility_regression 
tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_performance_regression 
[gw2] [ 24%] PASSED tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_performance_regression 
tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_memory_regression 
[gw2] [ 24%] PASSED tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_memory_regression 
tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_functionality_regression 
[gw2] [ 24%] PASSED tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_functionality_regression 
tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_robustness_regression 
[gw2] [ 24%] PASSED tests/regression/test_hitl_step_migration_regression.py::TestHITLStepMigrationRegression::test_hitl_step_robustness_regression 
tests/security/test_file_backend_security.py::test_save_state_path_traversal[../outside_file.txt] 
[gw2] [ 24%] PASSED tests/security/test_file_backend_security.py::test_save_state_path_traversal[../outside_file.txt] 
tests/security/test_file_backend_security.py::test_save_state_path_traversal[..%2foutside_file.txt] 
[gw2] [ 24%] PASSED tests/security/test_file_backend_security.py::test_save_state_path_traversal[..%2foutside_file.txt] 
tests/security/test_file_backend_security.py::test_save_state_path_traversal[....//outside_file.txt] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_save_state_path_traversal[....//outside_file.txt] 
tests/security/test_file_backend_security.py::test_save_state_path_traversal[/etc/hostname] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_save_state_path_traversal[/etc/hostname] 
tests/security/test_file_backend_security.py::test_save_state_path_traversal[C:\\Windows\\System32\\drivers\\etc\\hosts] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_save_state_path_traversal[C:\\Windows\\System32\\drivers\\etc\\hosts] 
tests/security/test_file_backend_security.py::test_load_state_path_traversal[../outside_file.txt] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_load_state_path_traversal[../outside_file.txt] 
tests/security/test_file_backend_security.py::test_load_state_path_traversal[..%2foutside_file.txt] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_load_state_path_traversal[..%2foutside_file.txt] 
tests/security/test_file_backend_security.py::test_load_state_path_traversal[....//outside_file.txt] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_load_state_path_traversal[....//outside_file.txt] 
tests/security/test_file_backend_security.py::test_load_state_path_traversal[/etc/hostname] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_load_state_path_traversal[/etc/hostname] 
tests/security/test_file_backend_security.py::test_load_state_path_traversal[C:\\Windows\\System32\\drivers\\etc\\hosts] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_load_state_path_traversal[C:\\Windows\\System32\\drivers\\etc\\hosts] 
tests/security/test_file_backend_security.py::test_delete_state_path_traversal[../outside_file.txt] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_delete_state_path_traversal[../outside_file.txt] 
tests/security/test_file_backend_security.py::test_delete_state_path_traversal[..%2foutside_file.txt] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_delete_state_path_traversal[..%2foutside_file.txt] 
tests/security/test_file_backend_security.py::test_delete_state_path_traversal[....//outside_file.txt] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_delete_state_path_traversal[....//outside_file.txt] 
tests/security/test_file_backend_security.py::test_delete_state_path_traversal[/etc/hostname] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_delete_state_path_traversal[/etc/hostname] 
tests/security/test_file_backend_security.py::test_delete_state_path_traversal[C:\\Windows\\System32\\drivers\\etc\\hosts] 
[gw2] [ 25%] PASSED tests/security/test_file_backend_security.py::test_delete_state_path_traversal[C:\\Windows\\System32\\drivers\\etc\\hosts] 
tests/security/test_prompt_injection.py::test_placeholder_injection_literal 
[gw2] [ 25%] PASSED tests/security/test_prompt_injection.py::test_placeholder_injection_literal 
tests/security/test_prompt_injection.py::test_conditional_injection_literal 
[gw2] [ 25%] PASSED tests/security/test_prompt_injection.py::test_conditional_injection_literal 
tests/security/test_prompt_injection.py::test_loop_injection_literal 
[gw2] [ 25%] PASSED tests/security/test_prompt_injection.py::test_loop_injection_literal 
tests/security/test_prompt_injection.py::test_injection_inside_loop 
[gw2] [ 25%] PASSED tests/security/test_prompt_injection.py::test_injection_inside_loop 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_sql_identifier_safe_identifiers 
[gw2] [ 25%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_sql_identifier_safe_identifiers 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_sql_identifier_dangerous_identifiers 
[gw2] [ 25%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_sql_identifier_dangerous_identifiers 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_sql_identifier_sql_keywords 
[gw2] [ 25%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_sql_identifier_sql_keywords 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_sql_identifier_invalid_types 
[gw2] [ 25%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_sql_identifier_invalid_types 
[gw0] [ 25%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_concurrent_backup_operations 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_column_definition_safe_definitions 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_infinite_loop_bug_fix 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_column_definition_safe_definitions 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_column_definition_dangerous_definitions 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_column_definition_dangerous_definitions 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_column_definition_invalid_types 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_validate_column_definition_invalid_types 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_schema_migration_sql_injection_prevention 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_schema_migration_sql_injection_prevention 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_safe_schema_migration 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_safe_schema_migration 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_parameterized_queries_used 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_parameterized_queries_used 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_healthcare_data_security 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_healthcare_data_security 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_legal_data_security 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_legal_data_security 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_finance_data_security 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_finance_data_security 
tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_edge_case_security 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSQLInjectionPrevention::test_edge_case_security 
tests/security/test_sql_injection_prevention.py::TestSecurityLogging::test_security_violation_logging 
[gw2] [ 26%] PASSED tests/security/test_sql_injection_prevention.py::TestSecurityLogging::test_security_violation_logging 
tests/security/test_state_deserialization.py::test_resuming_from_malicious_state_fails_safely 
[gw2] [ 26%] PASSED tests/security/test_state_deserialization.py::test_resuming_from_malicious_state_fails_safely 
tests/smoke/test_smoke.py::test_core_imports 
[gw2] [ 26%] PASSED tests/smoke/test_smoke.py::test_core_imports 
tests/smoke/test_smoke.py::test_basic_pipeline_runs 
[gw2] [ 26%] PASSED tests/smoke/test_smoke.py::test_basic_pipeline_runs 
tests/smoke/test_smoke.py::test_cli_version 
[gw3] [ 26%] FAILED tests/integration/test_parallel_step_enhancements.py::test_proactive_cancellation_with_multiple_branches 
tests/integration/test_parallel_step_enhancements.py::test_proactive_cancellation_token_limits 
[gw2] [ 26%] PASSED tests/smoke/test_smoke.py::test_cli_version 
tests/static_analysis/test_contracts.py::test_parallel_step_context_contract_static_analysis 
[gw2] [ 26%] PASSED tests/static_analysis/test_contracts.py::test_parallel_step_context_contract_static_analysis 
tests/unit/test_adapter_step.py::test_adapter_pipeline_runs 
[gw2] [ 26%] PASSED tests/unit/test_adapter_step.py::test_adapter_pipeline_runs 
tests/unit/test_adapter_step.py::test_is_adapter_meta 
[gw2] [ 26%] PASSED tests/unit/test_adapter_step.py::test_is_adapter_meta 
tests/unit/test_adapter_step.py::test_docstring_example 
[gw2] [ 26%] PASSED tests/unit/test_adapter_step.py::test_docstring_example 
tests/unit/test_agent_decorators.py::test_validated_agent_raises 
[gw2] [ 26%] PASSED tests/unit/test_agent_decorators.py::test_validated_agent_raises 
tests/unit/test_agent_decorators.py::test_monitored_agent_records 
[gw2] [ 26%] PASSED tests/unit/test_agent_decorators.py::test_monitored_agent_records 
tests/unit/test_agent_decorators.py::test_decorator_composition_success 
[gw2] [ 27%] PASSED tests/unit/test_agent_decorators.py::test_decorator_composition_success 
tests/unit/test_agent_decorators.py::test_decorator_composition_validation_error 
[gw2] [ 27%] PASSED tests/unit/test_agent_decorators.py::test_decorator_composition_validation_error 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_single_command_logging 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_single_command_logging 
[gw1] [ 27%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestPerfCounterPrecision::test_perf_counter_ns_precision 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_multiple_commands_logging 
tests/benchmarks/test_performance_optimizations 2.py::TestSerializationPerformance::test_json_serialization 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_multiple_commands_logging 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_max_loops_logging 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_max_loops_logging 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_command_log_structure 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_command_log_structure 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_iteration_mapper_logging 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_iteration_mapper_logging 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_output_mapper_logging 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_output_mapper_logging 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_no_double_logging 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_no_double_logging 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_command_executor_logging 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_command_executor_logging 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_pause_and_resume_logging 
[gw2] [ 27%] FAILED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_pause_and_resume_logging 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_command_log_persistence 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_command_log_persistence 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_error_logging 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_error_logging 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_validation_error_logging 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_validation_error_logging 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLoggingRegressionPrevention::test_no_double_logging_regression 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLoggingRegressionPrevention::test_no_double_logging_regression 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLoggingRegressionPrevention::test_logging_consistency 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLoggingRegressionPrevention::test_logging_consistency 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLoggingRegressionPrevention::test_logging_order 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLoggingRegressionPrevention::test_logging_order 
tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLoggingRegressionGuard::test_no_duplicate_or_missing_logs 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLoggingRegressionGuard::test_no_duplicate_or_missing_logs 
tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_factory_creates_pipeline 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_factory_creates_pipeline 
tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_execution 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_execution 
tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_with_multiple_commands 
[gw2] [ 27%] PASSED tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_with_multiple_commands 
tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_resume 
[gw0] [ 28%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_infinite_loop_bug_fix 
[gw2] [ 28%] PASSED tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_resume 
tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_as_step 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_infinite_loop_bug_with_continue_fix 
[gw2] [ 28%] PASSED tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_as_step 
tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_with_context 
[gw2] [ 28%] PASSED tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_with_context 
tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_without_context 
[gw2] [ 28%] PASSED tests/unit/test_agentic_loop_recipe.py::test_agentic_loop_pipeline_without_context 
tests/unit/test_agents.py::test_async_agent_wrapper_success 
[gw2] [ 28%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_success 
tests/unit/test_agents.py::test_async_agent_wrapper_retry_then_success 
[gw0] [ 28%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_infinite_loop_bug_with_continue_fix 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_path_update_after_cleanup 
[gw1] [ 28%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestSerializationPerformance::test_json_serialization 
tests/benchmarks/test_performance_optimizations 2.py::TestSerializationPerformance::test_orjson_serialization 
[gw1] [ 28%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestSerializationPerformance::test_orjson_serialization 
tests/benchmarks/test_performance_optimizations 2.py::TestHashingPerformance::test_hashlib_hashing 
[gw3] [ 28%] FAILED tests/integration/test_parallel_step_enhancements.py::test_proactive_cancellation_token_limits 
tests/integration/test_parallel_step_enhancements.py::test_backward_compatibility_no_context_include_keys 
[gw3] [ 28%] PASSED tests/integration/test_parallel_step_enhancements.py::test_backward_compatibility_no_context_include_keys 
tests/integration/test_parallel_step_enhancements.py::test_backward_compatibility_no_usage_limits 
[gw0] [ 28%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_path_update_after_cleanup 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_race_condition_in_backup_creation 
[gw3] [ 28%] FAILED tests/integration/test_parallel_step_enhancements.py::test_backward_compatibility_no_usage_limits 
tests/integration/test_parallel_step_enhancements.py::test_context_include_keys_with_nonexistent_fields 
[gw3] [ 28%] FAILED tests/integration/test_parallel_step_enhancements.py::test_context_include_keys_with_nonexistent_fields 
tests/integration/test_parallel_step_enhancements.py::test_proactive_cancellation_error_handling 
[gw0] [ 28%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_race_condition_in_backup_creation 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_readonly_directory_fallback 
[gw0] [ 28%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_readonly_directory_fallback 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_pattern_glob_handling 
[gw0] [ 28%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_pattern_glob_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_stat_error_handling 
[gw2] [ 28%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_retry_then_success 
tests/unit/test_agents.py::test_async_agent_wrapper_timeout 
[gw0] [ 28%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_stat_error_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_unlink_error_handling 
[gw3] [ 28%] PASSED tests/integration/test_parallel_step_enhancements.py::test_proactive_cancellation_error_handling 
tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_high_concurrency 
[gw3] [ 28%] PASSED tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_high_concurrency 
tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_breach_detection 
[gw3] [ 28%] FAILED tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_breach_detection 
tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_mixed_operations 
[gw3] [ 28%] PASSED tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_mixed_operations 
tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_rapid_succession 
[gw3] [ 29%] PASSED tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_rapid_succession 
tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_token_limits 
[gw3] [ 29%] FAILED tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_token_limits 
tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_no_limits 
[gw3] [ 29%] FAILED tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_no_limits 
tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_basic 
[gw0] [ 29%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_unlink_error_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_min_function_with_none_values 
[gw1] [ 29%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestHashingPerformance::test_hashlib_hashing 
tests/benchmarks/test_performance_optimizations 2.py::TestHashingPerformance::test_blake3_hashing 
[gw0] [ 29%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_min_function_with_none_values 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_empty_directory_handling 
[gw3] [ 29%] PASSED tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_basic 
tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_large_context 
[gw3] [ 29%] PASSED tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_large_context 
tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_high_frequency 
[gw0] [ 29%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_empty_directory_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_max_attempts_exceeded_handling 
[gw3] [ 29%] PASSED tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_high_frequency 
tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_memory_intensive 
[gw3] [ 29%] PASSED tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_memory_intensive 
tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_parallel 
[gw3] [ 29%] PASSED tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_parallel 
tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_complex_pipeline 
[gw0] [ 29%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_max_attempts_exceeded_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_continue_statement_effectiveness 
[gw3] [ 29%] PASSED tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_complex_pipeline 
tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_error_handling 
[gw3] [ 29%] PASSED tests/integration/test_performance_with_context_updates.py::test_performance_with_context_updates_error_handling 
tests/integration/test_persistence_backends.py::test_file_backend_resume_after_crash 
[gw2] [ 29%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_timeout 
tests/unit/test_agents.py::test_async_agent_wrapper_exception 
[gw2] [ 29%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_exception 
tests/unit/test_agents.py::test_async_agent_wrapper_temperature_passed_directly 
[gw2] [ 29%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_temperature_passed_directly 
tests/unit/test_agents.py::test_noop_reflection_agent 
[gw2] [ 29%] PASSED tests/unit/test_agents.py::test_noop_reflection_agent 
tests/unit/test_agents.py::test_get_reflection_agent_disabled 
[gw2] [ 29%] PASSED tests/unit/test_agents.py::test_get_reflection_agent_disabled 
tests/unit/test_agents.py::test_get_reflection_agent_creation_failure 
[gw2] [ 29%] PASSED tests/unit/test_agents.py::test_get_reflection_agent_creation_failure 
tests/unit/test_agents.py::test_logging_review_agent_success 
[gw2] [ 29%] PASSED tests/unit/test_agents.py::test_logging_review_agent_success 
tests/unit/test_agents.py::test_logging_review_agent_error 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_logging_review_agent_error 
tests/unit/test_agents.py::test_async_agent_wrapper_agent_failed_string 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_agent_failed_string 
tests/unit/test_agents.py::test_logging_review_agent_run_async_fallback 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_logging_review_agent_run_async_fallback 
tests/unit/test_agents.py::test_logging_review_agent_run_async_non_callable 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_logging_review_agent_run_async_non_callable 
tests/unit/test_agents.py::test_async_agent_wrapper_agent_failed_string_only 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_agent_failed_string_only 
tests/unit/test_agents.py::test_make_agent_async_injects_key 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_make_agent_async_injects_key 
tests/unit/test_agents.py::test_make_agent_async_missing_key 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_make_agent_async_missing_key 
tests/unit/test_agents.py::test_async_agent_wrapper_timeout_validation 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_timeout_validation 
tests/unit/test_agents.py::test_async_agent_wrapper_with_dummy_agent 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_with_dummy_agent 
tests/unit/test_agents.py::test_async_agent_wrapper_init_valid_args 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_init_valid_args 
tests/unit/test_agents.py::test_async_agent_wrapper_init_default_timeout 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_init_default_timeout 
tests/unit/test_agents.py::test_async_agent_wrapper_init_invalid_max_retries_type 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_init_invalid_max_retries_type 
tests/unit/test_agents.py::test_async_agent_wrapper_init_negative_max_retries_value 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_init_negative_max_retries_value 
tests/unit/test_agents.py::test_async_agent_wrapper_init_invalid_timeout_type 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_init_invalid_timeout_type 
tests/unit/test_agents.py::test_async_agent_wrapper_init_non_positive_timeout_value 
[gw2] [ 30%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_init_non_positive_timeout_value 
tests/unit/test_agents.py::test_async_agent_wrapper_runtime_timeout 
[gw0] [ 30%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_continue_statement_effectiveness 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_path_reset_after_cleanup 
[gw1] [ 30%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestHashingPerformance::test_blake3_hashing 
tests/benchmarks/test_performance_optimizations 2.py::TestAsyncPerformance::test_async_performance_with_uvloop 
[gw3] [ 30%] FAILED tests/integration/test_persistence_backends.py::test_file_backend_resume_after_crash 
tests/integration/test_persistence_backends.py::test_sqlite_backend_resume_after_crash 
[gw0] [ 30%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_path_reset_after_cleanup 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_counter_reset_after_cleanup 
[gw0] [ 30%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_counter_reset_after_cleanup 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_infinite_loop_prevention 
[gw3] [ 30%] FAILED tests/integration/test_persistence_backends.py::test_sqlite_backend_resume_after_crash 
tests/integration/test_persistence_backends.py::test_file_backend_concurrent 
[gw1] [ 30%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestAsyncPerformance::test_async_performance_with_uvloop 
tests/benchmarks/test_performance_optimizations 2.py::TestMeasureTimeDecorators::test_measure_time_decorator 
[gw3] [ 31%] PASSED tests/integration/test_persistence_backends.py::test_file_backend_concurrent 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_runtime_timeout 
tests/unit/test_agents.py::test_make_self_improvement_agent_uses_settings_default 
tests/integration/test_persistence_backends.py::test_sqlite_backend_admin_queries_integration 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_make_self_improvement_agent_uses_settings_default 
tests/unit/test_agents.py::test_make_repair_agent_uses_settings_default 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_make_repair_agent_uses_settings_default 
tests/unit/test_agents.py::test_make_self_improvement_agent_uses_override_model 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_make_self_improvement_agent_uses_override_model 
tests/unit/test_agents.py::test_make_repair_agent_uses_override_model 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_make_repair_agent_uses_override_model 
tests/unit/test_agents.py::test_async_agent_wrapper_serializes_pydantic_input 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_serializes_pydantic_input 
tests/unit/test_agents.py::test_async_agent_wrapper_passthrough_non_model 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_passthrough_non_model 
tests/unit/test_agents.py::test_async_agent_wrapper_serializes_pydantic_kwarg 
[gw3] [ 31%] PASSED tests/integration/test_persistence_backends.py::test_sqlite_backend_admin_queries_integration 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_async_agent_wrapper_serializes_pydantic_kwarg 
tests/integration/test_persistence_backends.py::test_sqlite_backend_concurrent_integration 
tests/unit/test_agents.py::test_make_agent_async_no_extra_processors 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_make_agent_async_no_extra_processors 
tests/unit/test_agents.py::test_make_agent_async_custom_processor_order 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_make_agent_async_custom_processor_order 
tests/unit/test_agents.py::test_pydantic_output_parsed_by_agent 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_pydantic_output_parsed_by_agent 
tests/unit/test_agents.py::test_make_agent_async_type_adapter 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_make_agent_async_type_adapter 
tests/unit/test_agents.py::test_make_agent_async_type_adapter_complex_nested 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_make_agent_async_type_adapter_complex_nested 
tests/unit/test_agents.py::test_make_agent_async_type_adapter_union_types 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_make_agent_async_type_adapter_union_types 
tests/unit/test_agents.py::test_unwrap_type_adapter_function 
[gw2] [ 31%] PASSED tests/unit/test_agents.py::test_unwrap_type_adapter_function 
tests/unit/test_as_step.py::test_agentic_loop_as_step_basic 
[gw2] [ 31%] PASSED tests/unit/test_as_step.py::test_agentic_loop_as_step_basic 
tests/unit/test_as_step.py::test_flujo_as_step_basic 
[gw2] [ 31%] PASSED tests/unit/test_as_step.py::test_flujo_as_step_basic 
tests/unit/test_auto_repair.py::test_deterministic_processor_cleans_trailing_text 
[gw2] [ 31%] PASSED tests/unit/test_auto_repair.py::test_deterministic_processor_cleans_trailing_text 
tests/unit/test_auto_repair.py::test_async_agent_wrapper_deterministic_repair 
[gw3] [ 31%] PASSED tests/integration/test_persistence_backends.py::test_sqlite_backend_concurrent_integration 
tests/integration/test_persistence_backends.py::test_file_backend_custom_type_serialization 
[gw2] [ 31%] PASSED tests/unit/test_auto_repair.py::test_async_agent_wrapper_deterministic_repair 
tests/unit/test_auto_repair.py::test_async_agent_wrapper_llm_repair 
[gw2] [ 32%] PASSED tests/unit/test_auto_repair.py::test_async_agent_wrapper_llm_repair 
tests/unit/test_auto_repair.py::test_balance_removes_and_adds_braces 
[gw2] [ 32%] PASSED tests/unit/test_auto_repair.py::test_balance_removes_and_adds_braces 
tests/unit/test_auto_repair.py::test_balance_ignores_braces_in_strings 
[gw2] [ 32%] PASSED tests/unit/test_auto_repair.py::test_balance_ignores_braces_in_strings 
tests/unit/test_auto_repair.py::test_async_agent_wrapper_llm_repair_invalid_json 
[gw3] [ 32%] PASSED tests/integration/test_persistence_backends.py::test_file_backend_custom_type_serialization 
[gw2] [ 32%] PASSED tests/unit/test_auto_repair.py::test_async_agent_wrapper_llm_repair_invalid_json 
tests/integration/test_pipeline_composition.py::test_pipeline_composition_basic 
tests/unit/test_auto_repair.py::test_repair_prompt_handles_braces 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_composition.py::test_pipeline_composition_basic 
tests/integration/test_pipeline_composition.py::test_pipeline_composition_execution 
[gw2] [ 32%] PASSED tests/unit/test_auto_repair.py::test_repair_prompt_handles_braces 
tests/unit/test_bug_regression.py::TestCircularReferenceSerialization::test_circular_reference_detection 
[gw2] [ 32%] PASSED tests/unit/test_bug_regression.py::TestCircularReferenceSerialization::test_circular_reference_detection 
tests/unit/test_bug_regression.py::TestCircularReferenceSerialization::test_self_reference 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_composition.py::test_pipeline_composition_execution 
tests/integration/test_pipeline_composition.py::test_pipeline_composition_context_sharing 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_composition.py::test_pipeline_composition_context_sharing 
tests/integration/test_pipeline_composition.py::test_pipeline_composition_multiple_chains 
[gw2] [ 32%] FAILED tests/unit/test_bug_regression.py::TestCircularReferenceSerialization::test_self_reference 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_composition.py::test_pipeline_composition_multiple_chains 
tests/unit/test_bug_regression.py::TestCircularReferenceSerialization::test_deep_circular_reference 
tests/integration/test_pipeline_composition.py::test_pipeline_composition_backward_compatibility 
[gw2] [ 32%] PASSED tests/unit/test_bug_regression.py::TestCircularReferenceSerialization::test_deep_circular_reference 
tests/unit/test_bug_regression.py::TestFailedStepRecording::test_failed_step_recording 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_composition.py::test_pipeline_composition_backward_compatibility 
tests/integration/test_pipeline_composition.py::test_pipeline_composition_type_safety 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_composition.py::test_pipeline_composition_type_safety 
tests/integration/test_pipeline_composition.py::test_pipeline_composition_error_handling 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_composition.py::test_pipeline_composition_error_handling 
tests/integration/test_pipeline_context_updates.py::test_update_nested_model 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_context_updates.py::test_update_nested_model 
tests/integration/test_pipeline_context_updates.py::test_update_list_of_nested_models 
[gw3] [ 32%] FAILED tests/integration/test_pipeline_context_updates.py::test_update_list_of_nested_models 
tests/integration/test_pipeline_context_updates.py::test_invalid_field_type_fails 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_context_updates.py::test_invalid_field_type_fails 
tests/integration/test_pipeline_context_updates.py::test_model_level_validation_failure 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_context_updates.py::test_model_level_validation_failure 
tests/integration/test_pipeline_context_updates.py::test_incompatible_output_type_skips_update 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_context_updates.py::test_incompatible_output_type_skips_update 
tests/integration/test_pipeline_context_updates.py::test_context_update_with_custom_type 
[gw3] [ 32%] PASSED tests/integration/test_pipeline_context_updates.py::test_context_update_with_custom_type 
tests/integration/test_pipeline_hooks.py::test_all_hooks_are_called_in_correct_order 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_hooks.py::test_all_hooks_are_called_in_correct_order 
tests/integration/test_pipeline_hooks.py::test_on_step_failure_hook_is_called 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_hooks.py::test_on_step_failure_hook_is_called 
tests/integration/test_pipeline_hooks.py::test_hook_receives_correct_arguments 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_hooks.py::test_hook_receives_correct_arguments 
tests/integration/test_pipeline_hooks.py::test_pipeline_aborts_gracefully_from_hook 
[gw2] [ 33%] FAILED tests/unit/test_bug_regression.py::TestFailedStepRecording::test_failed_step_recording 
tests/unit/test_bug_regression.py::TestLambdaSerializationNullHandling::test_lambda_null_handling 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_hooks.py::test_pipeline_aborts_gracefully_from_hook 
tests/integration/test_pipeline_hooks.py::test_faulty_hook_does_not_crash_pipeline 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_hooks.py::test_faulty_hook_does_not_crash_pipeline 
tests/integration/test_pipeline_hooks.py::test_hooks_receive_context_and_resources 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_hooks.py::test_hooks_receive_context_and_resources 
tests/integration/test_pipeline_hooks.py::test_post_run_abort_does_not_mask_errors 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_hooks.py::test_post_run_abort_does_not_mask_errors 
tests/integration/test_pipeline_hooks.py::test_incrementing_stub_agent 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_hooks.py::test_incrementing_stub_agent 
tests/integration/test_pipeline_runner.py::test_runner_respects_max_retries 
[gw3] [ 33%] FAILED tests/integration/test_pipeline_runner.py::test_runner_respects_max_retries 
tests/integration/test_pipeline_runner.py::test_feedback_enriches_prompt 
[gw3] [ 33%] FAILED tests/integration/test_pipeline_runner.py::test_feedback_enriches_prompt 
tests/integration/test_pipeline_runner.py::test_conditional_redirection 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_runner.py::test_conditional_redirection 
tests/integration/test_pipeline_runner.py::test_on_failure_called_with_fluent_api 
[gw2] [ 33%] FAILED tests/unit/test_bug_regression.py::TestLambdaSerializationNullHandling::test_lambda_null_handling 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_runner.py::test_on_failure_called_with_fluent_api 
tests/unit/test_bug_regression.py::TestSerializationEdgeCases::test_unknown_type_serialization 
[gw2] [ 33%] PASSED tests/unit/test_bug_regression.py::TestSerializationEdgeCases::test_unknown_type_serialization 
tests/integration/test_pipeline_runner.py::test_timeout_and_redirect_loop_detection 
tests/unit/test_bug_regression.py::TestSerializationEdgeCases::test_custom_object_with_circular_ref 
[gw2] [ 33%] PASSED tests/unit/test_bug_regression.py::TestSerializationEdgeCases::test_custom_object_with_circular_ref 
tests/unit/test_bug_regression.py::TestSerializationEdgeCases::test_serialization_error_handling 
[gw2] [ 33%] PASSED tests/unit/test_bug_regression.py::TestSerializationEdgeCases::test_serialization_error_handling 
tests/unit/test_bug_regression.py::TestPerformanceRegression::test_serialization_performance 
[gw2] [ 33%] PASSED tests/unit/test_bug_regression.py::TestPerformanceRegression::test_serialization_performance 
tests/unit/test_bug_regression.py::TestPerformanceRegression::test_database_operation_performance 
[gw2] [ 33%] PASSED tests/unit/test_bug_regression.py::TestPerformanceRegression::test_database_operation_performance 
tests/unit/test_bug_regression.py::TestCIEnvironmentCompatibility::test_memory_efficient_serialization 
[gw0] [ 33%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_infinite_loop_prevention 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_cleanup_attempts_limit 
[gw3] [ 33%] FAILED tests/integration/test_pipeline_runner.py::test_timeout_and_redirect_loop_detection 
tests/integration/test_pipeline_runner.py::test_pipeline_cancellation 
[gw3] [ 33%] PASSED tests/integration/test_pipeline_runner.py::test_pipeline_cancellation 
tests/integration/test_pipeline_runner.py::test_runner_unpacks_agent_result 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner.py::test_runner_unpacks_agent_result 
tests/integration/test_pipeline_runner.py::test_step_config_temperature_passed 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner.py::test_step_config_temperature_passed 
tests/integration/test_pipeline_runner.py::test_step_config_temperature_omitted 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner.py::test_step_config_temperature_omitted 
tests/integration/test_pipeline_runner.py::test_pipeline_with_temperature_setting 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner.py::test_pipeline_with_temperature_setting 
tests/integration/test_pipeline_runner.py::test_failure_handler_exception_propagates 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner.py::test_failure_handler_exception_propagates 
tests/integration/test_pipeline_runner_with_context.py::test_pipeline_runner_shared_context_flow 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner_with_context.py::test_pipeline_runner_shared_context_flow 
tests/integration/test_pipeline_runner_with_context.py::test_existing_agents_without_context 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner_with_context.py::test_existing_agents_without_context 
tests/integration/test_pipeline_runner_with_context.py::test_concurrent_runs_with_typed_context_are_isolated 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner_with_context.py::test_concurrent_runs_with_typed_context_are_isolated 
tests/integration/test_pipeline_runner_with_resources.py::test_resources_passed_to_agent 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner_with_resources.py::test_resources_passed_to_agent 
tests/integration/test_pipeline_runner_with_resources.py::test_resources_passed_to_plugin 
[gw3] [ 34%] FAILED tests/integration/test_pipeline_runner_with_resources.py::test_resources_passed_to_plugin 
tests/integration/test_pipeline_runner_with_resources.py::test_resource_instance_is_shared_across_steps 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner_with_resources.py::test_resource_instance_is_shared_across_steps 
tests/integration/test_pipeline_runner_with_resources.py::test_pipeline_with_no_resources_succeeds 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner_with_resources.py::test_pipeline_with_no_resources_succeeds 
tests/integration/test_pipeline_runner_with_resources.py::test_mixing_resources_and_context 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_runner_with_resources.py::test_mixing_resources_and_context 
tests/integration/test_pipeline_versioning.py::test_resume_uses_original_pipeline_version 
[gw3] [ 34%] PASSED tests/integration/test_pipeline_versioning.py::test_resume_uses_original_pipeline_version 
tests/integration/test_processors.py::test_prompt_processor_modifies_input 
[gw3] [ 34%] FAILED tests/integration/test_processors.py::test_prompt_processor_modifies_input 
tests/integration/test_processors.py::test_output_processor_modifies_output 
[gw3] [ 34%] PASSED tests/integration/test_processors.py::test_output_processor_modifies_output 
tests/integration/test_processors.py::test_processor_receives_context 
[gw3] [ 34%] FAILED tests/integration/test_processors.py::test_processor_receives_context 
tests/integration/test_processors.py::test_failing_processor_does_not_crash 
[gw3] [ 34%] PASSED tests/integration/test_processors.py::test_failing_processor_does_not_crash 
tests/integration/test_processors.py::test_serialize_pydantic_output_to_dict 
[gw3] [ 34%] PASSED tests/integration/test_processors.py::test_serialize_pydantic_output_to_dict 
tests/integration/test_processors.py::test_serialize_pydantic_is_idempotent 
[gw3] [ 34%] PASSED tests/integration/test_processors.py::test_serialize_pydantic_is_idempotent 
tests/integration/test_prometheus_server.py::test_prometheus_metrics_endpoint 
[gw2] [ 34%] PASSED tests/unit/test_bug_regression.py::TestCIEnvironmentCompatibility::test_memory_efficient_serialization 
tests/unit/test_bug_regression.py::TestCIEnvironmentCompatibility::test_concurrent_serialization 
[gw2] [ 34%] PASSED tests/unit/test_bug_regression.py::TestCIEnvironmentCompatibility::test_concurrent_serialization 
tests/unit/test_bug_regression.py::TestEdgeCaseRobustness::test_extremely_deep_nesting 
[gw2] [ 35%] PASSED tests/unit/test_bug_regression.py::TestEdgeCaseRobustness::test_extremely_deep_nesting 
tests/unit/test_bug_regression.py::TestEdgeCaseRobustness::test_large_string_handling 
[gw2] [ 35%] PASSED tests/unit/test_bug_regression.py::TestEdgeCaseRobustness::test_large_string_handling 
tests/unit/test_bug_regression.py::TestEdgeCaseRobustness::test_special_characters 
[gw2] [ 35%] PASSED tests/unit/test_bug_regression.py::TestEdgeCaseRobustness::test_special_characters 
tests/unit/test_cache_step.py::TestCacheStep::test_cache_step_cached_classmethod 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestCacheStep::test_cache_step_cached_classmethod 
tests/unit/test_cache_step.py::TestCacheStep::test_cache_step_cached_classmethod_with_none_backend 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestCacheStep::test_cache_step_cached_classmethod_with_none_backend 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_none 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_none 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_step 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_step 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_node 
[gw3] [ 35%] PASSED tests/integration/test_prometheus_server.py::test_prometheus_metrics_endpoint 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_node 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_with_model_dump 
tests/integration/test_pydantic_ai_compatibility.py::test_pydantic_models_are_serialized_for_agents 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_with_model_dump 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_dict 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_dict 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_list 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_list 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_set 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_set 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_generic 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_circular_reference_generic 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_with_custom_serializer_exception 
[gw3] [ 35%] PASSED tests/integration/test_pydantic_ai_compatibility.py::test_pydantic_models_are_serialized_for_agents 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_with_custom_serializer_exception 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_step_with_agent_field 
tests/integration/test_pydantic_ai_compatibility.py::test_pipeline_context_serialized_for_agent_kwargs 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_step_with_agent_field 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_node_with_circular_next 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_node_with_circular_next 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_node_with_non_circular_next 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_node_with_non_circular_next 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_dict_with_run_id_and_initial_prompt 
[gw3] [ 35%] PASSED tests/integration/test_pydantic_ai_compatibility.py::test_pipeline_context_serialized_for_agent_kwargs 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_dict_with_run_id_and_initial_prompt 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_dict_with_custom_serializer_for_values 
tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_user_profile_pipeline_with_scalar_values 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_dict_with_custom_serializer_for_values 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_dict_with_model_dump_value_exception 
[gw2] [ 35%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_dict_with_model_dump_value_exception 
[gw3] [ 36%] PASSED tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_user_profile_pipeline_with_scalar_values 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_dict_with_model_dump_value_recursion_error 
tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_product_catalog_pipeline_with_mixed_data_types 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_dict_with_model_dump_value_recursion_error 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_callable 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_callable 
[gw3] [ 36%] PASSED tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_product_catalog_pipeline_with_mixed_data_types 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_callable_without_name 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_callable_without_name 
tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_order_processing_pipeline_with_nested_structures 
tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_exception_fallback 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestSerializeForCacheKey::test_serialize_exception_fallback 
tests/unit/test_cache_step.py::TestSortSetDeterministically::test_sort_set_deterministically_success 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestSortSetDeterministically::test_sort_set_deterministically_success 
[gw3] [ 36%] PASSED tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_order_processing_pipeline_with_nested_structures 
tests/unit/test_cache_step.py::TestSortSetDeterministically::test_sort_set_deterministically_with_complex_objects 
tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_multi_step_pipeline_with_context_updates 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestSortSetDeterministically::test_sort_set_deterministically_with_complex_objects 
tests/unit/test_cache_step.py::TestSortSetDeterministically::test_sort_set_deterministically_frozen_set 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestSortSetDeterministically::test_sort_set_deterministically_frozen_set 
tests/unit/test_cache_step.py::TestSortSetDeterministically::test_sort_set_deterministically_with_exception 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestSortSetDeterministically::test_sort_set_deterministically_with_exception 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_none 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_none 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_circular_reference 
[gw3] [ 36%] PASSED tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_multi_step_pipeline_with_context_updates 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_circular_reference 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_primitive_types 
tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_regression_bug_integration_test 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_primitive_types 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_list 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_list 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_tuple 
[gw3] [ 36%] PASSED tests/integration/test_reconstruction_integration.py::TestReconstructionIntegration::test_regression_bug_integration_test 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_tuple 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_dict 
tests/integration/test_reconstruction_integration.py::test_circular_reference_in_dict_keys_in_pipeline 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_dict 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_set 
[gw3] [ 36%] PASSED tests/integration/test_reconstruction_integration.py::test_circular_reference_in_dict_keys_in_pipeline 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_set 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_frozen_set 
tests/unit/test_cli.py::test_cli_solve_weights_invalid_structure 
[gw2] [ 36%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_frozen_set 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_model_with_run_id 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_model_with_run_id 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_model_with_recursion_error 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_model_with_recursion_error 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_model_with_value_error 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_model_with_value_error 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_callable 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_callable 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_callable_without_module 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_callable_without_module 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_hashable_object 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_hashable_object 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_unhashable_object 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_unhashable_object 
tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_object_with_hash_exception 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestGetStableRepr::test_get_stable_repr_object_with_hash_exception 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_model 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_model 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_dict 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_dict 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_list 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_list 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_set 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_set 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_generic 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_circular_reference_generic 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_model_dump 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_model_dump 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_dict 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_dict 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_list 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_list 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_set 
[gw3] [ 37%] PASSED tests/unit/test_cli.py::test_cli_solve_weights_invalid_structure 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_set 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_custom_serializer 
tests/unit/test_cli.py::test_cli_solve_weights_missing_keys 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_custom_serializer 
tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_custom_serializer_exception 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestSerializeListForKey::test_serialize_list_for_key_with_custom_serializer_exception 
tests/unit/test_cache_step.py::TestCreateStepFingerprint::test_create_step_fingerprint_basic 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestCreateStepFingerprint::test_create_step_fingerprint_basic 
tests/unit/test_cache_step.py::TestCreateStepFingerprint::test_create_step_fingerprint_without_agent 
[gw2] [ 37%] PASSED tests/unit/test_cache_step.py::TestCreateStepFingerprint::test_create_step_fingerprint_without_agent 
tests/unit/test_cache_step.py::TestGenerateCacheKey::test_generate_cache_key_success 
[gw2] [ 38%] PASSED tests/unit/test_cache_step.py::TestGenerateCacheKey::test_generate_cache_key_success 
tests/unit/test_cache_step.py::TestGenerateCacheKey::test_generate_cache_key_with_none_values 
[gw2] [ 38%] PASSED tests/unit/test_cache_step.py::TestGenerateCacheKey::test_generate_cache_key_with_none_values 
tests/unit/test_cache_step.py::TestGenerateCacheKey::test_generate_cache_key_json_exception_pickle_success 
[gw2] [ 38%] PASSED tests/unit/test_cache_step.py::TestGenerateCacheKey::test_generate_cache_key_json_exception_pickle_success 
tests/unit/test_cache_step.py::TestGenerateCacheKey::test_generate_cache_key_both_json_and_pickle_fail 
[gw2] [ 38%] PASSED tests/unit/test_cache_step.py::TestGenerateCacheKey::test_generate_cache_key_both_json_and_pickle_fail 
tests/unit/test_ci_compatibility.py::TestCISerializationCompatibility::test_serialization_consistency_across_environments 
[gw2] [ 38%] PASSED tests/unit/test_ci_compatibility.py::TestCISerializationCompatibility::test_serialization_consistency_across_environments 
tests/unit/test_ci_compatibility.py::TestCISerializationCompatibility::test_memory_efficient_serialization 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_solve_weights_missing_keys 
tests/unit/test_cli.py::test_cli_solve_context_data_safe_deserialize 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_solve_context_data_safe_deserialize 
tests/unit/test_cli.py::test_cli_add_eval_case_uses_safe_deserialize 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_add_eval_case_uses_safe_deserialize 
tests/unit/test_cli.py::test_cli_solve_weights_file_safe_deserialize 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_solve_weights_file_safe_deserialize 
tests/unit/test_cli.py::test_cli_solve_keyboard_interrupt 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_solve_keyboard_interrupt 
tests/unit/test_cli.py::test_cli_bench_keyboard_interrupt 
[gw3] [ 38%] SKIPPED tests/unit/test_cli.py::test_cli_bench_keyboard_interrupt 
tests/unit/test_cli.py::test_cli_version_cmd_package_not_found 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_version_cmd_package_not_found 
tests/unit/test_cli.py::test_cli_main_callback_profile 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_main_callback_profile 
tests/unit/test_cli.py::test_cli_solve_configuration_error 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_solve_configuration_error 
tests/unit/test_cli.py::test_cli_explain 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_explain 
tests/unit/test_cli.py::test_cli_validate_success 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_validate_success 
tests/unit/test_cli.py::test_cli_validate_failure 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_validate_failure 
tests/unit/test_cli.py::test_cli_improve_output_formatting 
[gw3] [ 38%] PASSED tests/unit/test_cli.py::test_cli_improve_output_formatting 
tests/unit/test_cli.py::test_cli_improve_json_output 
[gw2] [ 38%] PASSED tests/unit/test_ci_compatibility.py::TestCISerializationCompatibility::test_memory_efficient_serialization 
tests/unit/test_ci_compatibility.py::TestCISerializationCompatibility::test_error_handling_robustness 
[gw2] [ 38%] PASSED tests/unit/test_ci_compatibility.py::TestCISerializationCompatibility::test_error_handling_robustness 
tests/unit/test_ci_compatibility.py::TestCISerializationCompatibility::test_circular_reference_in_ci 
[gw2] [ 38%] PASSED tests/unit/test_ci_compatibility.py::TestCISerializationCompatibility::test_circular_reference_in_ci 
tests/unit/test_ci_compatibility.py::TestCIResourceConstraints::test_low_memory_serialization 
[gw2] [ 38%] PASSED tests/unit/test_ci_compatibility.py::TestCIResourceConstraints::test_low_memory_serialization 
tests/unit/test_ci_compatibility.py::TestCIResourceConstraints::test_concurrent_access_safety 
[gw3] [ 39%] PASSED tests/unit/test_cli.py::test_cli_improve_json_output 
[gw2] [ 39%] PASSED tests/unit/test_ci_compatibility.py::TestCIResourceConstraints::test_concurrent_access_safety 
tests/unit/test_ci_compatibility.py::TestCIPlatformCompatibility::test_platform_independent_serialization 
tests/unit/test_cli.py::test_cli_help 
[gw2] [ 39%] PASSED tests/unit/test_ci_compatibility.py::TestCIPlatformCompatibility::test_platform_independent_serialization 
tests/unit/test_ci_compatibility.py::TestCIPlatformCompatibility::test_python_version_compatibility 
[gw2] [ 39%] PASSED tests/unit/test_ci_compatibility.py::TestCIPlatformCompatibility::test_python_version_compatibility 
tests/unit/test_ci_compatibility.py::TestCITimingIssues::test_slow_serialization_handling 
[gw3] [ 39%] PASSED tests/unit/test_cli.py::test_cli_help 
tests/unit/test_cli.py::test_cli_version 
[gw2] [ 39%] PASSED tests/unit/test_ci_compatibility.py::TestCITimingIssues::test_slow_serialization_handling 
tests/unit/test_ci_compatibility.py::TestCITimingIssues::test_timeout_robustness 
[gw2] [ 39%] PASSED tests/unit/test_ci_compatibility.py::TestCITimingIssues::test_timeout_robustness 
tests/unit/test_ci_compatibility.py::TestCIErrorRecovery::test_graceful_degradation 
[gw2] [ 39%] PASSED tests/unit/test_ci_compatibility.py::TestCIErrorRecovery::test_graceful_degradation 
tests/unit/test_ci_compatibility.py::TestCIErrorRecovery::test_partial_failure_handling 
[gw2] [ 39%] FAILED tests/unit/test_ci_compatibility.py::TestCIErrorRecovery::test_partial_failure_handling 
[gw3] [ 39%] PASSED tests/unit/test_cli.py::test_cli_version 
tests/unit/test_cli.py::test_cli_solve_happy_path 
tests/unit/test_cli.py::test_cli_run 
[gw2] [ 39%] PASSED tests/unit/test_cli.py::test_cli_solve_happy_path 
tests/unit/test_cli.py::test_cli_solve_custom_models 
[gw3] [ 39%] PASSED tests/unit/test_cli.py::test_cli_run 
tests/unit/test_cli.py::test_cli_run_with_args 
[gw2] [ 39%] PASSED tests/unit/test_cli.py::test_cli_solve_custom_models 
tests/unit/test_cli.py::test_cli_bench_command 
[gw3] [ 39%] PASSED tests/unit/test_cli.py::test_cli_run_with_args 
[gw2] [ 39%] SKIPPED tests/unit/test_cli.py::test_cli_bench_command 
tests/unit/test_cli.py::test_cli_run_with_invalid_args 
tests/unit/test_cli.py::test_cli_show_config_masks_secrets 
[gw2] [ 39%] PASSED tests/unit/test_cli.py::test_cli_show_config_masks_secrets 
tests/unit/test_cli.py::test_cli_version_command 
[gw2] [ 39%] PASSED tests/unit/test_cli.py::test_cli_version_command 
tests/unit/test_cli.py::test_cli_solve_with_weights 
[gw3] [ 39%] PASSED tests/unit/test_cli.py::test_cli_run_with_invalid_args 
tests/unit/test_cli.py::test_cli_run_with_invalid_model 
[gw2] [ 39%] PASSED tests/unit/test_cli.py::test_cli_solve_with_weights 
tests/unit/test_cli.py::test_cli_solve_weights_file_not_found 
[gw3] [ 39%] PASSED tests/unit/test_cli.py::test_cli_run_with_invalid_model 
tests/unit/test_cli.py::test_cli_run_with_invalid_retries 
[gw2] [ 39%] PASSED tests/unit/test_cli.py::test_cli_solve_weights_file_not_found 
tests/unit/test_cli.py::test_cli_solve_weights_file_invalid_json 
[gw3] [ 39%] PASSED tests/unit/test_cli.py::test_cli_run_with_invalid_retries 
tests/unit/test_cli.py::test_cli_run_with_invalid_agent_timeout 
[gw2] [ 40%] PASSED tests/unit/test_cli.py::test_cli_solve_weights_file_invalid_json 
tests/unit/test_conditional_step.py::test_step_factory_branch_on 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::test_step_factory_branch_on 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_init_with_valid_branches 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_init_with_valid_branches 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_init_with_default_branch 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_init_with_default_branch 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_init_with_mappers 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_init_with_mappers 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_validation_empty_branches_dict_args 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_validation_empty_branches_dict_args 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_validation_empty_branches_kwargs 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_validation_empty_branches_kwargs 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_validation_invalid_branch_type 
[gw3] [ 40%] PASSED tests/unit/test_cli.py::test_cli_run_with_invalid_agent_timeout 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_validation_invalid_branch_type 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_validation_invalid_default_branch_type 
tests/unit/test_cli.py::test_cli_run_with_invalid_review_model 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_validation_invalid_default_branch_type 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_repr 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_repr 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_repr_multiple_branches 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_repr_multiple_branches 
tests/unit/test_conditional_step.py::TestConditionalStep::test_step_factory_branch_on_with_default_branch 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_step_factory_branch_on_with_default_branch 
tests/unit/test_conditional_step.py::TestConditionalStep::test_step_factory_branch_on_with_mappers 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_step_factory_branch_on_with_mappers 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_model_validate_with_kwargs 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_model_validate_with_kwargs 
tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_model_validate_with_dict_args 
[gw2] [ 40%] PASSED tests/unit/test_conditional_step.py::TestConditionalStep::test_conditional_step_model_validate_with_dict_args 
tests/unit/test_config_manager.py::TestConfigManager::test_empty_config 
[gw2] [ 40%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_empty_config 
tests/unit/test_config_manager.py::TestConfigManager::test_basic_config_loading 
[gw2] [ 40%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_basic_config_loading 
tests/unit/test_config_manager.py::TestConfigManager::test_cli_defaults 
[gw2] [ 40%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_cli_defaults 
tests/unit/test_config_manager.py::TestConfigManager::test_settings_override 
[gw3] [ 40%] PASSED tests/unit/test_cli.py::test_cli_run_with_invalid_review_model 
tests/unit/test_cli.py::test_cli_run_with_invalid_review_model_path 
[gw2] [ 40%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_settings_override 
tests/unit/test_config_manager.py::TestConfigManager::test_config_file_discovery 
[gw2] [ 40%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_config_file_discovery 
tests/unit/test_config_manager.py::TestConfigManager::test_parent_directory_search 
[gw2] [ 41%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_parent_directory_search 
tests/unit/test_config_manager.py::TestConfigManager::test_invalid_config_file 
[gw2] [ 41%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_invalid_config_file 
tests/unit/test_config_manager.py::TestConfigManager::test_missing_config_file 
[gw2] [ 41%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_missing_config_file 
tests/unit/test_config_manager.py::TestConfigManager::test_global_functions 
[gw2] [ 41%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_global_functions 
tests/unit/test_config_manager.py::TestConfigManager::test_settings_integration 
[gw2] [ 41%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_settings_integration 
tests/unit/test_config_manager.py::TestConfigManager::test_invalid_k_variants_env 
[gw3] [ 41%] PASSED tests/unit/test_cli.py::test_cli_run_with_invalid_review_model_path 
tests/unit/test_cli.py::test_cli_add_eval_case_prints_correct_case_string 
[gw2] [ 41%] PASSED tests/unit/test_config_manager.py::TestConfigManager::test_invalid_k_variants_env 
tests/unit/test_config_manager.py::TestConfigurationModels::test_flujo_config 
[gw2] [ 41%] PASSED tests/unit/test_config_manager.py::TestConfigurationModels::test_flujo_config 
tests/unit/test_config_manager.py::TestConfigurationModels::test_solve_config 
[gw2] [ 41%] PASSED tests/unit/test_config_manager.py::TestConfigurationModels::test_solve_config 
tests/unit/test_config_manager.py::TestConfigurationModels::test_settings_overrides 
[gw2] [ 41%] PASSED tests/unit/test_config_manager.py::TestConfigurationModels::test_settings_overrides 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_register_custom_type_integration 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_register_custom_type_integration 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_type_resolution_context_thread_safety 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_type_resolution_context_thread_safety 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_module_scope_resolution 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_module_scope_resolution 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_type_validation 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_type_validation 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_extract_union_types_type_system 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_extract_union_types_type_system 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_extract_union_types_modern_syntax 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_extract_union_types_modern_syntax 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_resolve_actual_type 
[gw3] [ 41%] PASSED tests/unit/test_cli.py::test_cli_add_eval_case_prints_correct_case_string 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_resolve_actual_type 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_deserialize_value_integration 
tests/unit/test_cli.py::test_cli_add_eval_case_handles_missing_dataset_file_gracefully 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_deserialize_value_integration 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_performance_improvement 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_performance_improvement 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_deterministic_behavior 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_deterministic_behavior 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_backward_compatibility 
[gw2] [ 41%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_backward_compatibility 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_error_handling 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_error_handling 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_complex_union_handling 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_complex_union_handling 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_module_resolver_caching 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_module_resolver_caching 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_type_hints_integration 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_type_hints_integration 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_concurrent_type_resolution 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_concurrent_type_resolution 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_serialization_integration 
[gw3] [ 42%] PASSED tests/unit/test_cli.py::test_cli_add_eval_case_handles_missing_dataset_file_gracefully 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_serialization_integration 
tests/unit/test_cli.py::test_cli_add_eval_case_invalid_metadata_json 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_context_injection_with_type_system 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_context_injection_with_type_system 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_future_proof_design 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_future_proof_design 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_security_no_frame_access 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_security_no_frame_access 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_type_system_integration 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_type_system_integration 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_robust_error_recovery 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_robust_error_recovery 
tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_performance_under_load 
[gw3] [ 42%] PASSED tests/unit/test_cli.py::test_cli_add_eval_case_invalid_metadata_json 
tests/unit/test_cli.py::test_cli_improve_uses_custom_improvement_model 
[gw2] [ 42%] PASSED tests/unit/test_context_adapter_type_resolution.py::TestTypeResolution::test_performance_under_load 
tests/unit/test_context_aware_protocol.py::test_context_aware_agent_no_warning 
[gw2] [ 42%] PASSED tests/unit/test_context_aware_protocol.py::test_context_aware_agent_no_warning 
tests/unit/test_context_aware_protocol.py::test_legacy_agent_works_with_context 
[gw3] [ 42%] PASSED tests/unit/test_cli.py::test_cli_improve_uses_custom_improvement_model 
tests/unit/test_cli.py::test_apply_cli_defaults_helper 
[gw2] [ 42%] PASSED tests/unit/test_context_aware_protocol.py::test_legacy_agent_works_with_context 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_usage_info 
[gw3] [ 42%] PASSED tests/unit/test_cli.py::test_apply_cli_defaults_helper 
tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///foo.db-foo.db] 
[gw2] [ 42%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_usage_info 
[gw3] [ 42%] PASSED tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///foo.db-foo.db] 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_without_usage_info 
tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///./foo.db-./foo.db] 
[gw2] [ 42%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_without_usage_info 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_exception 
[gw3] [ 42%] PASSED tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///./foo.db-./foo.db] 
[gw2] [ 42%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_exception 
tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:////abs/path.db-/abs/path.db] 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_different_agent_types 
[gw3] [ 43%] PASSED tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:////abs/path.db-/abs/path.db] 
tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///../data/ops.db-../data/ops.db] 
[gw3] [ 43%] PASSED tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///../data/ops.db-../data/ops.db] 
tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///subdir/bar.db-subdir/bar.db] 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_different_agent_types 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_explicit_metrics 
[gw3] [ 43%] PASSED tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///subdir/bar.db-subdir/bar.db] 
tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///./subdir/bar.db-./subdir/bar.db] 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_explicit_metrics 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_model_id_parsing 
[gw3] [ 43%] PASSED tests/unit/test_cli_config.py::test_normalize_sqlite_path_relative[sqlite:///./subdir/bar.db-./subdir/bar.db] 
tests/unit/test_cli_config.py::test_normalize_sqlite_path_absolute 
[gw3] [ 43%] PASSED tests/unit/test_cli_config.py::test_normalize_sqlite_path_absolute 
tests/unit/test_cli_config.py::test_normalize_sqlite_path_edge_cases 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_with_model_id_parsing 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_warning_for_missing_model 
[gw3] [ 43%] PASSED tests/unit/test_cli_config.py::test_normalize_sqlite_path_edge_cases 
tests/unit/test_cli_config.py::TestConfigWarning::test_warning_displayed_when_no_env_and_no_config 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_warning_for_missing_model 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_no_model_info_available 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_no_model_info_available 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_graceful_fallback 
[gw3] [ 43%] PASSED tests/unit/test_cli_config.py::TestConfigWarning::test_warning_displayed_when_no_env_and_no_config 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_extract_usage_metrics_graceful_fallback 
tests/unit/test_cli_config.py::TestConfigWarning::test_no_warning_when_env_var_set 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_priority 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_priority 
tests/unit/test_cost_tracking_bug_fixes.py::TestBug2InconsistentUsageLimitEnforcement::test_step_level_limits_logic 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking_bug_fixes.py::TestBug2InconsistentUsageLimitEnforcement::test_step_level_limits_logic 
tests/unit/test_cost_tracking_bug_fixes.py::TestBug3RedundantCostCalculation::test_explicit_metrics_take_priority 
[gw3] [ 43%] PASSED tests/unit/test_cli_config.py::TestConfigWarning::test_no_warning_when_env_var_set 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking_bug_fixes.py::TestBug3RedundantCostCalculation::test_explicit_metrics_take_priority 
tests/unit/test_cost_tracking_bug_fixes.py::TestBug3RedundantCostCalculation::test_usage_extraction_when_no_explicit_metrics 
[gw0] [ 43%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_cleanup_attempts_limit 
tests/unit/test_cli_config.py::TestConfigWarning::test_no_warning_when_config_file_has_state_uri 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_stat_exception_handling 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking_bug_fixes.py::TestBug3RedundantCostCalculation::test_usage_extraction_when_no_explicit_metrics 
tests/unit/test_cost_tracking_bug_fixes.py::TestBug4BrittleModelIdExtraction::test_model_id_parsing_with_provider 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking_bug_fixes.py::TestBug4BrittleModelIdExtraction::test_model_id_parsing_with_provider 
tests/unit/test_cost_tracking_bug_fixes.py::TestBug4BrittleModelIdExtraction::test_model_id_parsing_without_provider 
[gw2] [ 43%] PASSED tests/unit/test_cost_tracking_bug_fixes.py::TestBug4BrittleModelIdExtraction::test_model_id_parsing_without_provider 
[gw3] [ 43%] PASSED tests/unit/test_cli_config.py::TestConfigWarning::test_no_warning_when_config_file_has_state_uri 
tests/unit/test_cost_tracking_bug_fixes.py::TestBug6MaintenanceFragility::test_critical_warning_for_hardcoded_prices 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_cli_solve_command_parameters 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_bug_fixes.py::TestBug6MaintenanceFragility::test_critical_warning_for_hardcoded_prices 
tests/unit/test_cost_tracking_bug_fixes.py::TestIntegrationCostTracking::test_cost_calculation_logic 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_bug_fixes.py::TestIntegrationCostTracking::test_cost_calculation_logic 
tests/unit/test_cost_tracking_bug_fixes.py::TestIntegrationCostTracking::test_usage_limits_logic 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_bug_fixes.py::TestIntegrationCostTracking::test_usage_limits_logic 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug1CustomOutputCostCalculation::test_custom_output_with_explicit_cost_should_not_split_tokens 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug1CustomOutputCostCalculation::test_custom_output_with_explicit_cost_should_not_split_tokens 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug1CustomOutputCostCalculation::test_custom_output_with_cost_only_should_trust_explicit_cost 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug1CustomOutputCostCalculation::test_custom_output_with_cost_only_should_trust_explicit_cost 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug1CustomOutputCostCalculation::test_custom_output_should_not_recalculate_cost_from_tokens 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug1CustomOutputCostCalculation::test_custom_output_should_not_recalculate_cost_from_tokens 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug2UsageLimitPrecedence::test_step_level_limits_should_override_pipeline_level_limits 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug2UsageLimitPrecedence::test_step_level_limits_should_override_pipeline_level_limits 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug2UsageLimitPrecedence::test_step_level_limits_with_none_pipeline_limits 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug2UsageLimitPrecedence::test_step_level_limits_with_none_pipeline_limits 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug2UsageLimitPrecedence::test_pipeline_level_limits_with_none_step_limits 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug2UsageLimitPrecedence::test_pipeline_level_limits_with_none_step_limits 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug2UsageLimitPrecedence::test_both_limits_none 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug2UsageLimitPrecedence::test_both_limits_none 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug3AttributeErrorOnAgentType::test_simple_function_agent_should_not_crash 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug3AttributeErrorOnAgentType::test_simple_function_agent_should_not_crash 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug3AttributeErrorOnAgentType::test_plain_class_agent_should_not_crash 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug3AttributeErrorOnAgentType::test_plain_class_agent_should_not_crash 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug3AttributeErrorOnAgentType::test_agent_with_missing_model_attributes_should_gracefully_handle 
[gw3] [ 44%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_cli_solve_command_parameters 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug3AttributeErrorOnAgentType::test_agent_with_missing_model_attributes_should_gracefully_handle 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_cli_bench_command_parameters 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug3AttributeErrorOnAgentType::test_agent_with_attribute_error_should_gracefully_handle 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug3AttributeErrorOnAgentType::test_agent_with_attribute_error_should_gracefully_handle 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug4ImprovedModelIdExtraction::test_agent_with_explicit_model_id_should_work 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug4ImprovedModelIdExtraction::test_agent_with_explicit_model_id_should_work 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug4ImprovedModelIdExtraction::test_agent_with_model_attribute_should_work 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug4ImprovedModelIdExtraction::test_agent_with_model_attribute_should_work 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug4ImprovedModelIdExtraction::test_agent_with_private_model_name_should_work 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug4ImprovedModelIdExtraction::test_agent_with_private_model_name_should_work 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug4ImprovedModelIdExtraction::test_agent_without_model_id_logs_helpful_warning 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug4ImprovedModelIdExtraction::test_agent_without_model_id_logs_helpful_warning 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_unknown_provider_should_return_zero_cost 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_unknown_provider_should_return_zero_cost 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_ambiguous_model_names_should_not_infer_provider 
[gw2] [ 44%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_ambiguous_model_names_should_not_infer_provider 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_known_provider_with_unknown_model_should_return_zero_cost 
[gw3] [ 44%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_cli_bench_command_parameters 
[gw2] [ 45%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_known_provider_with_unknown_model_should_return_zero_cost 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_empty_model_name_should_return_zero_cost 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_parameter_mapping_consistency 
[gw2] [ 45%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_empty_model_name_should_return_zero_cost 
[gw3] [ 45%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_parameter_mapping_consistency 
tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_none_model_name_should_return_zero_cost 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_default_parameter_values 
[gw3] [ 45%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_default_parameter_values 
[gw2] [ 45%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestBug5RobustProviderInference::test_none_model_name_should_return_zero_cost 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_parameter_validation 
tests/unit/test_cost_tracking_regression_bugs.py::TestIntegrationRegressionTests::test_custom_output_with_step_limits_and_simple_agent 
[gw2] [ 45%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestIntegrationRegressionTests::test_custom_output_with_step_limits_and_simple_agent 
tests/unit/test_cost_tracking_regression_bugs.py::TestIntegrationRegressionTests::test_usage_limit_precedence_with_custom_output 
[gw2] [ 45%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestIntegrationRegressionTests::test_usage_limit_precedence_with_custom_output 
tests/unit/test_cost_tracking_regression_bugs.py::TestEdgeCasesAndRobustness::test_custom_output_with_zero_cost 
[gw3] [ 45%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_parameter_validation 
[gw2] [ 45%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestEdgeCasesAndRobustness::test_custom_output_with_zero_cost 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_async_execution_compatibility 
tests/unit/test_cost_tracking_regression_bugs.py::TestEdgeCasesAndRobustness::test_custom_output_with_none_cost 
[gw2] [ 45%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestEdgeCasesAndRobustness::test_custom_output_with_none_cost 
tests/unit/test_cost_tracking_regression_bugs.py::TestEdgeCasesAndRobustness::test_agent_with_complex_model_id_parsing 
[gw2] [ 45%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestEdgeCasesAndRobustness::test_agent_with_complex_model_id_parsing 
tests/unit/test_cost_tracking_regression_bugs.py::TestEdgeCasesAndRobustness::test_usage_limit_precedence_with_falsy_values 
[gw2] [ 45%] PASSED tests/unit/test_cost_tracking_regression_bugs.py::TestEdgeCasesAndRobustness::test_usage_limit_precedence_with_falsy_values 
tests/unit/test_default_backend.py::test_runner_uses_sqlite_by_default 
[gw2] [ 45%] FAILED tests/unit/test_default_backend.py::test_runner_uses_sqlite_by_default 
tests/unit/test_default_recipe.py::test_default_recipe_deprecation_warning 
[gw3] [ 45%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterIntegration::test_async_execution_compatibility 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_no_parameter_loss 
[gw2] [ 45%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_deprecation_warning 
tests/unit/test_default_recipe.py::test_default_recipe_initialization 
[gw3] [ 45%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_no_parameter_loss 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_parameter_documentation 
[gw2] [ 45%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_initialization 
[gw3] [ 45%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_parameter_documentation 
tests/unit/test_default_recipe.py::test_default_recipe_initialization_with_reflection 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_cli_integration_consistency 
[gw2] [ 45%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_initialization_with_reflection 
[gw3] [ 45%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_cli_integration_consistency 
tests/unit/test_default_recipe.py::test_default_recipe_initialization_with_optional_params 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_parameter_type_consistency 
[gw3] [ 45%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_parameter_type_consistency 
[gw2] [ 45%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_initialization_with_optional_params 
tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_optional_parameter_handling 
tests/unit/test_default_recipe.py::test_default_recipe_run_async 
[gw3] [ 46%] PASSED tests/unit/test_cli_parameter_integration.py::TestCLIParameterRegressionPrevention::test_optional_parameter_handling 
tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_lens_list_with_large_mixed_database 
[gw2] [ 46%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_run_async 
tests/unit/test_default_recipe.py::test_default_recipe_run_async_with_reflection 
[gw2] [ 46%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_run_async_with_reflection 
tests/unit/test_default_recipe.py::test_default_recipe_run_async_no_solution 
[gw2] [ 46%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_run_async_no_solution 
tests/unit/test_default_recipe.py::test_default_recipe_run_async_no_checklist 
[gw2] [ 46%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_run_async_no_checklist 
tests/unit/test_default_recipe.py::test_default_recipe_run_sync 
[gw2] [ 46%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_run_sync 
tests/unit/test_default_recipe.py::test_default_recipe_run_sync_with_reflection 
[gw2] [ 46%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_run_sync_with_reflection 
tests/unit/test_default_recipe.py::test_default_recipe_agent_wrappers 
[gw2] [ 46%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_agent_wrappers 
tests/unit/test_default_recipe.py::test_default_recipe_with_callable_agents 
[gw2] [ 46%] PASSED tests/unit/test_default_recipe.py::test_default_recipe_with_callable_agents 
tests/unit/test_domain_models.py::test_improvement_models_round_trip 
[gw2] [ 46%] PASSED tests/unit/test_domain_models.py::test_improvement_models_round_trip 
tests/unit/test_domain_models.py::test_improvement_models_validation 
[gw2] [ 46%] PASSED tests/unit/test_domain_models.py::test_improvement_models_validation 
tests/unit/test_domain_models.py::test_improvement_models_config_and_new_case 
[gw2] [ 46%] PASSED tests/unit/test_domain_models.py::test_improvement_models_config_and_new_case 
tests/unit/test_domain_models.py::test_global_custom_serializer_registry 
[gw2] [ 46%] PASSED tests/unit/test_domain_models.py::test_global_custom_serializer_registry 
tests/unit/test_domain_models.py::test_base_model_circular_reference_serialization 
[gw2] [ 46%] PASSED tests/unit/test_domain_models.py::test_base_model_circular_reference_serialization 
tests/unit/test_dsl.py::test_step_chaining_operator 
[gw2] [ 46%] PASSED tests/unit/test_dsl.py::test_step_chaining_operator 
tests/unit/test_dsl.py::test_role_based_constructor 
[gw2] [ 46%] PASSED tests/unit/test_dsl.py::test_role_based_constructor 
tests/unit/test_dsl.py::test_step_configuration 
[gw2] [ 46%] PASSED tests/unit/test_dsl.py::test_step_configuration 
tests/unit/test_dsl.py::test_dsl 
[gw2] [ 46%] PASSED tests/unit/test_dsl.py::test_dsl 
tests/unit/test_dsl.py::test_dsl_with_step 
[gw2] [ 46%] PASSED tests/unit/test_dsl.py::test_dsl_with_step 
tests/unit/test_dsl.py::test_dsl_with_agent 
[gw2] [ 46%] PASSED tests/unit/test_dsl.py::test_dsl_with_agent 
tests/unit/test_dsl.py::test_dsl_with_agent_and_step 
[gw2] [ 46%] PASSED tests/unit/test_dsl.py::test_dsl_with_agent_and_step 
tests/unit/test_dsl.py::test_step_class_methods_create_correct_steps 
[gw2] [ 46%] PASSED tests/unit/test_dsl.py::test_step_class_methods_create_correct_steps 
tests/unit/test_dsl.py::test_step_fluent_builder_methods 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_fluent_builder_methods 
tests/unit/test_dsl.py::test_step_init_handles_mixed_plugin_formats 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_init_handles_mixed_plugin_formats 
tests/unit/test_dsl.py::test_step_from_callable_basic 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_from_callable_basic 
tests/unit/test_dsl.py::test_step_from_callable_name_and_config 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_from_callable_name_and_config 
tests/unit/test_dsl.py::test_step_from_callable_bound_method 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_from_callable_bound_method 
tests/unit/test_dsl.py::test_step_from_callable_untyped_defaults_any 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_from_callable_untyped_defaults_any 
tests/unit/test_dsl.py::test_step_from_mapper_basic 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_from_mapper_basic 
tests/unit/test_dsl.py::test_step_decorator_basic 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_decorator_basic 
tests/unit/test_dsl.py::test_step_decorator_name_and_config 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_decorator_name_and_config 
tests/unit/test_dsl.py::test_step_arun_basic 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_arun_basic 
tests/unit/test_dsl.py::test_step_arun_with_context 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_arun_with_context 
tests/unit/test_dsl.py::test_step_arun_no_agent 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_arun_no_agent 
tests/unit/test_dsl.py::test_pipeline_chaining_operator 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_pipeline_chaining_operator 
tests/unit/test_dsl.py::test_step_decorator_matches_from_callable 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_decorator_matches_from_callable 
tests/unit/test_dsl.py::test_step_decorator_matches_kwargs 
[gw2] [ 47%] PASSED tests/unit/test_dsl.py::test_step_decorator_matches_kwargs 
tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_preserves_context 
[gw2] [ 47%] PASSED tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_preserves_context 
tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_roundtrip_complex_input 
[gw2] [ 47%] PASSED tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_roundtrip_complex_input 
tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_simple_nested_structures 
[gw2] [ 47%] PASSED tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_simple_nested_structures 
tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_list_of_primitives 
[gw2] [ 47%] PASSED tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_list_of_primitives 
tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_dict_of_primitives 
[gw2] [ 47%] PASSED tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_dict_of_primitives 
tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_edge_cases 
[gw2] [ 47%] PASSED tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_edge_cases 
tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_string_encoded_lists 
[gw2] [ 47%] PASSED tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_string_encoded_lists 
tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_preserves_types 
[gw2] [ 48%] PASSED tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_preserves_types 
tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_multiple_roundtrips 
[gw2] [ 48%] PASSED tests/unit/test_dummy_remote_backend.py::test_dummy_remote_backend_multiple_roundtrips 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_init_validation_empty_branches 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_init_validation_empty_branches 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_init_with_step_branches 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_init_with_step_branches 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_init_with_pipeline_branches 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_init_with_pipeline_branches 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_with_context_include_keys 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_with_context_include_keys 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_with_merge_strategy 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_with_merge_strategy 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_with_branch_failure_strategy 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_with_branch_failure_strategy 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_with_custom_merge_function 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_with_custom_merge_function 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_repr 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_repr 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_factory_method 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_factory_method 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_factory_method_with_all_options 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_factory_method_with_all_options 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_model_validate_with_dict_args 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_model_validate_with_dict_args 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_model_validate_with_kwargs 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_model_validate_with_kwargs 
tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_default_values 
[gw2] [ 48%] PASSED tests/unit/test_dynamic_router.py::TestDynamicParallelRouterStep::test_dynamic_parallel_router_step_default_values 
tests/unit/test_embeddings.py::TestEmbeddingResult::test_embedding_result_creation 
[gw2] [ 48%] PASSED tests/unit/test_embeddings.py::TestEmbeddingResult::test_embedding_result_creation 
tests/unit/test_embeddings.py::TestEmbeddingResult::test_embedding_result_usage_method 
[gw2] [ 48%] PASSED tests/unit/test_embeddings.py::TestEmbeddingResult::test_embedding_result_usage_method 
tests/unit/test_embeddings.py::TestEmbeddingResult::test_embedding_result_with_zero_tokens 
[gw2] [ 48%] PASSED tests/unit/test_embeddings.py::TestEmbeddingResult::test_embedding_result_with_zero_tokens 
tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_initialization 
[gw2] [ 48%] PASSED tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_initialization 
tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_embed_method 
[gw2] [ 48%] PASSED tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_embed_method 
tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_embed_single_text 
[gw2] [ 48%] PASSED tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_embed_single_text 
tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_embed_empty_list 
[gw2] [ 48%] PASSED tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_embed_empty_list 
tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_api_error_handling 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_api_error_handling 
tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_model_id_format 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestOpenAIEmbeddingClient::test_openai_embedding_client_model_id_format 
tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_openai 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_openai 
tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_unknown_provider 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_unknown_provider 
tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_invalid_format 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_invalid_format 
tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_missing_model 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_missing_model 
tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_empty_provider 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestGetEmbeddingClient::test_get_embedding_client_empty_provider 
tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model 
tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_with_completion_tokens 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_with_completion_tokens 
tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_no_pricing 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_no_pricing 
tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_strict_mode 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_strict_mode 
tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_provider_inference 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_provider_inference 
tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_edge_cases 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestCostCalculatorForEmbeddings::test_calculate_cost_for_embedding_model_edge_cases 
tests/unit/test_embeddings.py::TestEmbeddingIntegrationWithCostTracking::test_embedding_result_with_cost_tracking 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestEmbeddingIntegrationWithCostTracking::test_embedding_result_with_cost_tracking 
tests/unit/test_embeddings.py::TestEmbeddingIntegrationWithCostTracking::test_embedding_result_without_model_id 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestEmbeddingIntegrationWithCostTracking::test_embedding_result_without_model_id 
tests/unit/test_embeddings.py::TestEmbeddingIntegrationWithCostTracking::test_embedding_result_with_strict_mode 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestEmbeddingIntegrationWithCostTracking::test_embedding_result_with_strict_mode 
tests/unit/test_embeddings.py::TestEmbeddingModelPricing::test_embedding_model_pricing_configuration 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestEmbeddingModelPricing::test_embedding_model_pricing_configuration 
tests/unit/test_embeddings.py::TestEmbeddingModelPricing::test_embedding_model_pricing_validation 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestEmbeddingModelPricing::test_embedding_model_pricing_validation 
tests/unit/test_embeddings.py::TestEmbeddingModelPricing::test_embedding_model_pricing_different_values 
[gw2] [ 49%] PASSED tests/unit/test_embeddings.py::TestEmbeddingModelPricing::test_embedding_model_pricing_different_values 
tests/unit/test_error_messages.py::test_improper_step_call 
[gw2] [ 49%] PASSED tests/unit/test_error_messages.py::test_improper_step_call 
tests/unit/test_error_messages.py::test_missing_agent_errors 
[gw2] [ 49%] PASSED tests/unit/test_error_messages.py::test_missing_agent_errors 
tests/unit/test_error_messages.py::test_type_mismatch_errors 
[gw2] [ 49%] PASSED tests/unit/test_error_messages.py::test_type_mismatch_errors 
tests/unit/test_error_messages.py::test_union_optional_handling 
[gw2] [ 50%] PASSED tests/unit/test_error_messages.py::test_union_optional_handling 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_literal_escape_marker_in_user_input 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_literal_escape_marker_in_user_input 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_template_syntax_in_user_input 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_template_syntax_in_user_input 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_each_block_with_literal_escape_marker 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_each_block_with_literal_escape_marker 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_nested_object_with_literal_escape_marker 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_nested_object_with_literal_escape_marker 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_mixed_content_with_literal_and_template_syntax 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_mixed_content_with_literal_and_template_syntax 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_multiple_literal_escape_markers 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_multiple_literal_escape_markers 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_in_complex_objects 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_in_complex_objects 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_in_base_model 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_in_base_model 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_conditional_blocks_with_escape_marker 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_conditional_blocks_with_escape_marker 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_nested_conditional_blocks 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_nested_conditional_blocks 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_each_block_with_nested_objects 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_each_block_with_nested_objects 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_edge_cases 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_edge_cases 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_unique_escape_markers_per_instance 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_unique_escape_markers_per_instance 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_with_special_characters 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_with_special_characters 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_in_json_strings 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_in_json_strings 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_with_unicode 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_with_unicode 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_in_empty_and_none_values 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_in_empty_and_none_values 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_regression_original_functionality 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_regression_original_functionality 
tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_performance 
[gw2] [ 50%] PASSED tests/unit/test_escape_marker_fix.py::TestEscapeMarkerCollisionFix::test_escape_marker_performance 
tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_matching_output 
[gw2] [ 50%] PASSED tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_matching_output 
tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_non_matching_output 
[gw2] [ 50%] PASSED tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_non_matching_output 
tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_empty_step_history 
[gw2] [ 50%] PASSED tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_empty_step_history 
tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_none_expected_output 
[gw2] [ 51%] PASSED tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_none_expected_output 
tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_multiple_steps 
[gw2] [ 51%] PASSED tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_multiple_steps 
tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_different_types 
[gw2] [ 51%] PASSED tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_different_types 
tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_complex_objects 
[gw2] [ 51%] PASSED tests/unit/test_evaluators.py::TestFinalSolutionEvaluator::test_evaluate_with_complex_objects 
tests/unit/test_execution_backend.py::test_custom_backend_invoked 
[gw2] [ 51%] PASSED tests/unit/test_execution_backend.py::test_custom_backend_invoked 
tests/unit/test_execution_manager.py::TestStateManager::test_load_workflow_state_no_backend 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestStateManager::test_load_workflow_state_no_backend 
tests/unit/test_execution_manager.py::TestStateManager::test_load_workflow_state_no_run_id 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestStateManager::test_load_workflow_state_no_run_id 
tests/unit/test_execution_manager.py::TestStateManager::test_load_workflow_state_not_found 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestStateManager::test_load_workflow_state_not_found 
tests/unit/test_execution_manager.py::TestStateManager::test_persist_workflow_state_no_backend 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestStateManager::test_persist_workflow_state_no_backend 
tests/unit/test_execution_manager.py::TestStateManager::test_persist_workflow_state_no_run_id 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestStateManager::test_persist_workflow_state_no_run_id 
tests/unit/test_execution_manager.py::TestStateManager::test_get_run_id_from_context 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestStateManager::test_get_run_id_from_context 
tests/unit/test_execution_manager.py::TestStateManager::test_get_run_id_from_context_none 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestStateManager::test_get_run_id_from_context_none 
tests/unit/test_execution_manager.py::TestUsageGovernor::test_check_usage_limits_no_limits 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestUsageGovernor::test_check_usage_limits_no_limits 
tests/unit/test_execution_manager.py::TestUsageGovernor::test_check_usage_limits_cost_exceeded 
[gw2] [ 51%] FAILED tests/unit/test_execution_manager.py::TestUsageGovernor::test_check_usage_limits_cost_exceeded 
tests/unit/test_execution_manager.py::TestUsageGovernor::test_check_usage_limits_tokens_exceeded 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestUsageGovernor::test_check_usage_limits_tokens_exceeded 
tests/unit/test_execution_manager.py::TestUsageGovernor::test_update_telemetry_span 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestUsageGovernor::test_update_telemetry_span 
tests/unit/test_execution_manager.py::TestTypeValidator::test_validate_step_output_no_next_step 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestTypeValidator::test_validate_step_output_no_next_step 
tests/unit/test_execution_manager.py::TestTypeValidator::test_validate_step_output_compatible_types 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestTypeValidator::test_validate_step_output_compatible_types 
tests/unit/test_execution_manager.py::TestTypeValidator::test_validate_step_output_incompatible_types 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestTypeValidator::test_validate_step_output_incompatible_types 
tests/unit/test_execution_manager.py::TestTypeValidator::test_validate_step_output_none_value 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestTypeValidator::test_validate_step_output_none_value 
tests/unit/test_execution_manager.py::TestTypeValidator::test_get_step_input_type 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestTypeValidator::test_get_step_input_type 
tests/unit/test_execution_manager.py::TestTypeValidator::test_get_step_output_type 
[gw2] [ 51%] PASSED tests/unit/test_execution_manager.py::TestTypeValidator::test_get_step_output_type 
tests/unit/test_execution_manager.py::TestStepCoordinator::test_execute_step_success 
[gw2] [ 52%] PASSED tests/unit/test_execution_manager.py::TestStepCoordinator::test_execute_step_success 
tests/unit/test_execution_manager.py::TestStepCoordinator::test_execute_step_failure 
[gw2] [ 52%] PASSED tests/unit/test_execution_manager.py::TestStepCoordinator::test_execute_step_failure 
tests/unit/test_execution_manager.py::TestStepCoordinator::test_update_pipeline_result 
[gw2] [ 52%] PASSED tests/unit/test_execution_manager.py::TestStepCoordinator::test_update_pipeline_result 
tests/unit/test_execution_manager.py::TestExecutionManager::test_execute_steps_basic 
[gw2] [ 52%] PASSED tests/unit/test_execution_manager.py::TestExecutionManager::test_execute_steps_basic 
tests/unit/test_execution_manager.py::TestExecutionManager::test_set_final_context 
[gw2] [ 52%] PASSED tests/unit/test_execution_manager.py::TestExecutionManager::test_set_final_context 
tests/unit/test_execution_manager.py::TestExecutionManager::test_persist_final_state 
[gw2] [ 52%] PASSED tests/unit/test_execution_manager.py::TestExecutionManager::test_persist_final_state 
tests/unit/test_executor_components.py::TestOrjsonSerializer::test_serialize_deserialize_roundtrip 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestOrjsonSerializer::test_serialize_deserialize_roundtrip 
tests/unit/test_executor_components.py::TestOrjsonSerializer::test_serialize_deserialize_complex_data 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestOrjsonSerializer::test_serialize_deserialize_complex_data 
tests/unit/test_executor_components.py::TestBlake3Hasher::test_digest_consistency 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestBlake3Hasher::test_digest_consistency 
tests/unit/test_executor_components.py::TestBlake3Hasher::test_digest_uniqueness 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestBlake3Hasher::test_digest_uniqueness 
tests/unit/test_executor_components.py::TestInMemoryLRUBackend::test_get_put_basic 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestInMemoryLRUBackend::test_get_put_basic 
tests/unit/test_executor_components.py::TestInMemoryLRUBackend::test_cache_ttl_expiration 
[gw0] [ 52%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_stat_exception_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_unlink_exception_handling 
[gw1] [ 52%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestMeasureTimeDecorators::test_measure_time_decorator 
tests/benchmarks/test_performance_optimizations 2.py::TestMeasureTimeDecorators::test_measure_time_async_decorator 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestInMemoryLRUBackend::test_cache_ttl_expiration 
tests/unit/test_executor_components.py::TestInMemoryLRUBackend::test_cache_lru_eviction 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestInMemoryLRUBackend::test_cache_lru_eviction 
tests/unit/test_executor_components.py::TestInMemoryLRUBackend::test_cache_mutation_protection 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestInMemoryLRUBackend::test_cache_mutation_protection 
tests/unit/test_executor_components.py::TestThreadSafeMeter::test_add_and_snapshot 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestThreadSafeMeter::test_add_and_snapshot 
tests/unit/test_executor_components.py::TestThreadSafeMeter::test_guard_usage_limits 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestThreadSafeMeter::test_guard_usage_limits 
tests/unit/test_executor_components.py::TestThreadSafeMeter::test_concurrent_usage_tracking 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestThreadSafeMeter::test_concurrent_usage_tracking 
tests/unit/test_executor_components.py::TestDefaultAgentRunner::test_basic_agent_execution 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestDefaultAgentRunner::test_basic_agent_execution 
tests/unit/test_executor_components.py::TestDefaultAgentRunner::test_agent_with_context_and_resources 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestDefaultAgentRunner::test_agent_with_context_and_resources 
tests/unit/test_executor_components.py::TestDefaultProcessorPipeline::test_apply_prompt_processors 
[gw2] [ 52%] PASSED tests/unit/test_executor_components.py::TestDefaultProcessorPipeline::test_apply_prompt_processors 
tests/unit/test_executor_components.py::TestDefaultProcessorPipeline::test_apply_output_processors 
[gw2] [ 53%] PASSED tests/unit/test_executor_components.py::TestDefaultProcessorPipeline::test_apply_output_processors 
tests/unit/test_executor_components.py::TestDefaultValidatorRunner::test_validation_success 
[gw2] [ 53%] PASSED tests/unit/test_executor_components.py::TestDefaultValidatorRunner::test_validation_success 
tests/unit/test_executor_components.py::TestDefaultValidatorRunner::test_validation_failure 
[gw2] [ 53%] PASSED tests/unit/test_executor_components.py::TestDefaultValidatorRunner::test_validation_failure 
tests/unit/test_executor_components.py::TestDefaultPluginRunner::test_plugin_execution 
[gw2] [ 53%] FAILED tests/unit/test_executor_components.py::TestDefaultPluginRunner::test_plugin_execution 
tests/unit/test_executor_components.py::TestDefaultPluginRunner::test_plugin_with_priority_ordering 
[gw2] [ 53%] FAILED tests/unit/test_executor_components.py::TestDefaultPluginRunner::test_plugin_with_priority_ordering 
tests/unit/test_executor_components.py::TestDefaultTelemetry::test_trace_decorator 
[gw2] [ 53%] PASSED tests/unit/test_executor_components.py::TestDefaultTelemetry::test_trace_decorator 
tests/unit/test_executor_components.py::TestFlujoCompositionRoot::test_create_default_backend 
[gw2] [ 53%] PASSED tests/unit/test_executor_components.py::TestFlujoCompositionRoot::test_create_default_backend 
tests/unit/test_executor_components.py::TestFlujoCompositionRoot::test_executor_core_dependency_injection 
[gw2] [ 53%] PASSED tests/unit/test_executor_components.py::TestFlujoCompositionRoot::test_executor_core_dependency_injection 
tests/unit/test_executor_components.py::TestFlujoCompositionRoot::test_executor_core_default_dependencies 
[gw2] [ 53%] PASSED tests/unit/test_executor_components.py::TestFlujoCompositionRoot::test_executor_core_default_dependencies 
tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_handles_parallel_step 
[gw2] [ 53%] PASSED tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_handles_parallel_step 
tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_handles_dynamic_router_step 
[gw2] [ 53%] PASSED tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_handles_dynamic_router_step 
tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_is_complex_step_detection 
[gw2] [ 53%] PASSED tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_is_complex_step_detection 
tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_parallel_step_recursive_execution 
[gw2] [ 53%] PASSED tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_parallel_step_recursive_execution 
tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_dynamic_router_delegates_to_parallel 
[gw2] [ 53%] FAILED tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_dynamic_router_delegates_to_parallel 
tests/unit/test_extract.py::TestExtractPathExclusion::test_exact_folder_exclusion 
[gw2] [ 53%] PASSED tests/unit/test_extract.py::TestExtractPathExclusion::test_exact_folder_exclusion 
tests/unit/test_extract.py::TestExtractPathExclusion::test_folder_pattern_exclusion 
[gw2] [ 53%] PASSED tests/unit/test_extract.py::TestExtractPathExclusion::test_folder_pattern_exclusion 
tests/unit/test_extract.py::TestExtractPathExclusion::test_no_exclusion 
[gw2] [ 53%] PASSED tests/unit/test_extract.py::TestExtractPathExclusion::test_no_exclusion 
tests/unit/test_extract.py::TestExtractPathExclusion::test_specific_path_exclusion_exact_match 
[gw2] [ 53%] PASSED tests/unit/test_extract.py::TestExtractPathExclusion::test_specific_path_exclusion_exact_match 
tests/unit/test_extract.py::TestExtractPathExclusion::test_specific_path_exclusion_subdirectory 
[gw2] [ 53%] PASSED tests/unit/test_extract.py::TestExtractPathExclusion::test_specific_path_exclusion_subdirectory 
tests/unit/test_extract.py::TestExtractPathExclusion::test_specific_path_exclusion_no_match 
[gw2] [ 53%] PASSED tests/unit/test_extract.py::TestExtractPathExclusion::test_specific_path_exclusion_no_match 
tests/unit/test_extract.py::TestExtractPathExclusion::test_specific_path_exclusion_nested_no_match 
[gw2] [ 53%] PASSED tests/unit/test_extract.py::TestExtractPathExclusion::test_specific_path_exclusion_nested_no_match 
tests/unit/test_extract.py::TestExtractFileExclusion::test_file_pattern_exclusion 
[gw2] [ 53%] PASSED tests/unit/test_extract.py::TestExtractFileExclusion::test_file_pattern_exclusion 
tests/unit/test_extract.py::TestExtractDockerFileHandling::test_docker_files_not_excluded 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractDockerFileHandling::test_docker_files_not_excluded 
tests/unit/test_extract.py::TestExtractTreeStructure::test_build_tree_structure_basic 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractTreeStructure::test_build_tree_structure_basic 
tests/unit/test_extract.py::TestExtractTreeStructure::test_build_tree_structure_with_exclusions 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractTreeStructure::test_build_tree_structure_with_exclusions 
tests/unit/test_extract.py::TestExtractTreeStructure::test_build_tree_structure_with_file_pattern_exclusions 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractTreeStructure::test_build_tree_structure_with_file_pattern_exclusions 
tests/unit/test_extract.py::TestExtractIntegration::test_default_exclude_folders_structure 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractIntegration::test_default_exclude_folders_structure 
tests/unit/test_extract.py::TestExtractIntegration::test_default_exclude_folder_patterns_structure 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractIntegration::test_default_exclude_folder_patterns_structure 
tests/unit/test_extract.py::TestExtractIntegration::test_excluded_file_patterns_structure 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractIntegration::test_excluded_file_patterns_structure 
tests/unit/test_extract.py::TestExtractIntegration::test_git_specific_files_structure 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractIntegration::test_git_specific_files_structure 
tests/unit/test_extract.py::TestExtractEdgeCases::test_should_exclude_path_with_empty_parts 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractEdgeCases::test_should_exclude_path_with_empty_parts 
tests/unit/test_extract.py::TestExtractEdgeCases::test_should_exclude_file_with_empty_patterns 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractEdgeCases::test_should_exclude_file_with_empty_patterns 
tests/unit/test_extract.py::TestExtractEdgeCases::test_should_exclude_file_with_none_patterns 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractEdgeCases::test_should_exclude_file_with_none_patterns 
tests/unit/test_extract.py::TestExtractEdgeCases::test_should_exclude_path_with_none_parameters 
[gw2] [ 54%] PASSED tests/unit/test_extract.py::TestExtractEdgeCases::test_should_exclude_path_with_none_parameters 
tests/unit/test_factories.py::TestMakeDefaultPipeline::test_creates_pipeline_object 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestMakeDefaultPipeline::test_creates_pipeline_object 
tests/unit/test_factories.py::TestMakeDefaultPipeline::test_creates_pipeline_with_reflection 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestMakeDefaultPipeline::test_creates_pipeline_with_reflection 
tests/unit/test_factories.py::TestMakeDefaultPipeline::test_custom_max_retries 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestMakeDefaultPipeline::test_custom_max_retries 
tests/unit/test_factories.py::TestMakeAgenticLoopPipeline::test_creates_pipeline_object 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestMakeAgenticLoopPipeline::test_creates_pipeline_object 
tests/unit/test_factories.py::TestMakeAgenticLoopPipeline::test_custom_max_loops_and_retries 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestMakeAgenticLoopPipeline::test_custom_max_loops_and_retries 
tests/unit/test_factories.py::TestRunDefaultPipeline::test_runs_pipeline_successfully 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestRunDefaultPipeline::test_runs_pipeline_successfully 
tests/unit/test_factories.py::TestRunDefaultPipeline::test_returns_none_on_failure 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestRunDefaultPipeline::test_returns_none_on_failure 
tests/unit/test_factories.py::TestRunAgenticLoopPipeline::test_runs_pipeline_successfully 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestRunAgenticLoopPipeline::test_runs_pipeline_successfully 
tests/unit/test_factories.py::TestMakeStateMachinePipeline::test_creates_pipeline_object 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestMakeStateMachinePipeline::test_creates_pipeline_object 
tests/unit/test_factories.py::TestMakeStateMachinePipeline::test_validates_context_fields 
[gw2] [ 54%] PASSED tests/unit/test_factories.py::TestMakeStateMachinePipeline::test_validates_context_fields 
tests/unit/test_factories.py::TestMakeStateMachinePipeline::test_runs_until_complete 
[gw2] [ 55%] PASSED tests/unit/test_factories.py::TestMakeStateMachinePipeline::test_runs_until_complete 
tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_accepts_all_parameters 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_accepts_all_parameters 
tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_parameter_defaults 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_parameter_defaults 
tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_parameter_validation 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_parameter_validation 
tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_cli_parameter_compatibility 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_cli_parameter_compatibility 
tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_reflection_agent_optional 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestMakeDefaultPipelineParameters::test_reflection_agent_optional 
tests/unit/test_factories_parameter_validation.py::TestRunDefaultPipelineAsync::test_async_execution 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestRunDefaultPipelineAsync::test_async_execution 
tests/unit/test_factories_parameter_validation.py::TestRunDefaultPipelineAsync::test_async_with_context 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestRunDefaultPipelineAsync::test_async_with_context 
tests/unit/test_factories_parameter_validation.py::TestCLIParameterMapping::test_cli_k_maps_to_k_variants 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestCLIParameterMapping::test_cli_k_maps_to_k_variants 
tests/unit/test_factories_parameter_validation.py::TestCLIParameterMapping::test_cli_max_iters_maps_correctly 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestCLIParameterMapping::test_cli_max_iters_maps_correctly 
tests/unit/test_factories_parameter_validation.py::TestCLIParameterMapping::test_cli_reflection_maps_correctly 
[gw2] [ 55%] PASSED tests/unit/test_factories_parameter_validation.py::TestCLIParameterMapping::test_cli_reflection_maps_correctly 
tests/unit/test_fallback.py::test_fallback_assignment 
[gw2] [ 55%] PASSED tests/unit/test_fallback.py::test_fallback_assignment 
tests/unit/test_fallback.py::test_fallback_not_triggered_on_success 
[gw2] [ 55%] PASSED tests/unit/test_fallback.py::test_fallback_not_triggered_on_success 
tests/unit/test_fallback.py::test_fallback_triggered_on_failure 
[gw2] [ 55%] PASSED tests/unit/test_fallback.py::test_fallback_triggered_on_failure 
tests/unit/test_fallback.py::test_fallback_failure_propagates 
[gw2] [ 55%] PASSED tests/unit/test_fallback.py::test_fallback_failure_propagates 
tests/unit/test_fallback.py::test_fallback_latency_accumulated 
[gw2] [ 55%] PASSED tests/unit/test_fallback.py::test_fallback_latency_accumulated 
tests/unit/test_fallback.py::test_failed_fallback_accumulates_metrics 
[gw2] [ 55%] PASSED tests/unit/test_fallback.py::test_failed_fallback_accumulates_metrics 
tests/unit/test_fallback.py::test_successful_fallback_correctly_sets_metrics 
[gw2] [ 55%] FAILED tests/unit/test_fallback.py::test_successful_fallback_correctly_sets_metrics 
tests/unit/test_fallback.py::test_infinite_fallback_loop_detected 
[gw2] [ 55%] PASSED tests/unit/test_fallback.py::test_infinite_fallback_loop_detected 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_zero_cost_agents 
[gw2] [ 55%] PASSED tests/unit/test_fallback_edge_cases.py::test_fallback_with_zero_cost_agents 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_high_cost_agents 
[gw2] [ 55%] FAILED tests/unit/test_fallback_edge_cases.py::test_fallback_with_high_cost_agents 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_exception_raising_agents 
[gw2] [ 55%] PASSED tests/unit/test_fallback_edge_cases.py::test_fallback_with_exception_raising_agents 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_mixed_cost_scenarios 
[gw2] [ 56%] PASSED tests/unit/test_fallback_edge_cases.py::test_fallback_with_mixed_cost_scenarios 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_negative_metrics 
[gw2] [ 56%] FAILED tests/unit/test_fallback_edge_cases.py::test_fallback_with_negative_metrics 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_missing_metrics 
[gw2] [ 56%] PASSED tests/unit/test_fallback_edge_cases.py::test_fallback_with_missing_metrics 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_very_long_feedback 
[gw2] [ 56%] PASSED tests/unit/test_fallback_edge_cases.py::test_fallback_with_very_long_feedback 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_none_feedback 
[gw2] [ 56%] FAILED tests/unit/test_fallback_edge_cases.py::test_fallback_with_none_feedback 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_empty_string_feedback 
[gw2] [ 56%] PASSED tests/unit/test_fallback_edge_cases.py::test_fallback_with_empty_string_feedback 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_unicode_feedback 
[gw2] [ 56%] PASSED tests/unit/test_fallback_edge_cases.py::test_fallback_with_unicode_feedback 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_very_small_latency 
[gw2] [ 56%] PASSED tests/unit/test_fallback_edge_cases.py::test_fallback_with_very_small_latency 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_retry_scenarios 
[gw2] [ 56%] PASSED tests/unit/test_fallback_edge_cases.py::test_fallback_with_retry_scenarios 
tests/unit/test_fallback_edge_cases.py::test_fallback_with_complex_metadata 
[gw0] [ 56%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_unlink_exception_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_glob_exception_handling 
[gw2] [ 56%] FAILED tests/unit/test_fallback_edge_cases.py::test_fallback_with_complex_metadata 
tests/unit/test_file_sqlite_backends.py::test_file_backend_roundtrip 
[gw2] [ 56%] PASSED tests/unit/test_file_sqlite_backends.py::test_file_backend_roundtrip 
tests/unit/test_file_sqlite_backends.py::test_file_backend_load_during_delete 
[gw2] [ 56%] PASSED tests/unit/test_file_sqlite_backends.py::test_file_backend_load_during_delete 
tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_roundtrip 
[gw2] [ 56%] PASSED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_roundtrip 
tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_migrates_existing_db 
[gw2] [ 56%] FAILED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_migrates_existing_db 
tests/unit/test_file_sqlite_backends.py::test_backends_serialize_pydantic 
[gw2] [ 56%] FAILED tests/unit/test_file_sqlite_backends.py::test_backends_serialize_pydantic 
tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_admin_queries 
[gw2] [ 56%] PASSED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_admin_queries 
tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_concurrent 
[gw2] [ 56%] PASSED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_concurrent 
tests/unit/test_file_sqlite_backends.py::test_backends_deserialize_special_types 
[gw2] [ 56%] FAILED tests/unit/test_file_sqlite_backends.py::test_backends_deserialize_special_types 
tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_list_workflows_empty_database 
[gw2] [ 56%] PASSED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_list_workflows_empty_database 
tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_get_workflow_stats_empty_database 
[gw2] [ 56%] PASSED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_get_workflow_stats_empty_database 
tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_cleanup_old_workflows_no_old_workflows 
[gw2] [ 56%] PASSED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_cleanup_old_workflows_no_old_workflows 
tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_list_workflows_filter_by_nonexistent_pipeline_id 
[gw2] [ 57%] PASSED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_list_workflows_filter_by_nonexistent_pipeline_id 
tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_admin_queries_edge_cases 
[gw2] [ 57%] PASSED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_admin_queries_edge_cases 
tests/unit/test_hooks_typed.py::test_hook_receives_typed_payload 
[gw2] [ 57%] PASSED tests/unit/test_hooks_typed.py::test_hook_receives_typed_payload 
tests/unit/test_lens_cli.py::test_lens_commands 
[gw2] [ 57%] FAILED tests/unit/test_lens_cli.py::test_lens_commands 
tests/unit/test_lens_cli.py::test_lens_commands_with_filters 
[gw2] [ 57%] FAILED tests/unit/test_lens_cli.py::test_lens_commands_with_filters 
tests/unit/test_lens_cli.py::test_lens_show_detailed_run 
[gw2] [ 57%] FAILED tests/unit/test_lens_cli.py::test_lens_show_detailed_run 
tests/unit/test_lens_cli.py::test_lens_show_nonexistent_run 
[gw2] [ 57%] PASSED tests/unit/test_lens_cli.py::test_lens_show_nonexistent_run 
tests/unit/test_lens_cli.py::test_lens_commands_with_empty_database 
[gw2] [ 57%] FAILED tests/unit/test_lens_cli.py::test_lens_commands_with_empty_database 
tests/unit/test_lens_cli.py::test_lens_commands_with_failed_run 
[gw2] [ 57%] FAILED tests/unit/test_lens_cli.py::test_lens_commands_with_failed_run 
tests/unit/test_lens_cli.py::test_lens_commands_with_environment_configuration 
[gw2] [ 57%] FAILED tests/unit/test_lens_cli.py::test_lens_commands_with_environment_configuration 
tests/unit/test_lens_cli.py::test_lens_cli_relative_path_resolution 
[gw2] [ 57%] FAILED tests/unit/test_lens_cli.py::test_lens_cli_relative_path_resolution 
tests/unit/test_lens_cli.py::test_lens_show_with_verbose_options 
[gw2] [ 57%] FAILED tests/unit/test_lens_cli.py::test_lens_show_with_verbose_options 
tests/unit/test_lens_cli.py::test_lens_trace_command 
[gw2] [ 57%] PASSED tests/unit/test_lens_cli.py::test_lens_trace_command 
tests/unit/test_lens_cli.py::test_lens_trace_command_regression_timestamps 
[gw0] [ 57%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_glob_exception_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_fallback_timestamp_naming 
[gw2] [ 57%] PASSED tests/unit/test_lens_cli.py::test_lens_trace_command_regression_timestamps 
tests/unit/test_lens_cli.py::test_lens_commands_error_handling 
[gw2] [ 57%] PASSED tests/unit/test_lens_cli.py::test_lens_commands_error_handling 
tests/unit/test_loop_step.py::test_loop_step_init_validation 
[gw2] [ 57%] PASSED tests/unit/test_loop_step.py::test_loop_step_init_validation 
tests/unit/test_loop_step.py::test_step_factory_loop_until 
[gw2] [ 57%] PASSED tests/unit/test_loop_step.py::test_step_factory_loop_until 
tests/unit/test_loop_step.py::test_loopstep_context_isolation_unit 
[gw2] [ 57%] PASSED tests/unit/test_loop_step.py::test_loopstep_context_isolation_unit 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_model_id_attribute 
[gw2] [ 57%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_model_id_attribute 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_private_model_name 
[gw2] [ 57%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_private_model_name 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_model_attribute 
[gw2] [ 57%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_model_attribute 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_model_name_attribute 
[gw2] [ 57%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_model_name_attribute 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_llm_model_attribute 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_from_llm_model_attribute 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_priority_order 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_priority_order 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_none_value 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_none_value 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_no_attributes 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_no_attributes 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_none_agent 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_none_agent 
tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_converts_to_string 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDExtraction::test_extract_model_id_converts_to_string 
tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_valid 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_valid 
tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_none 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_none 
tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_not_string 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_not_string 
tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_empty_string 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_empty_string 
tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_whitespace_padded 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestModelIDValidation::test_validate_model_id_whitespace_padded 
tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_with_provider 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_with_provider 
tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_without_provider 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_without_provider 
tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_multiple_colons 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_multiple_colons 
tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_whitespace_handling 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_whitespace_handling 
tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_empty_parts 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_empty_parts 
tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_various_formats 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestProviderAndModelExtraction::test_extract_provider_and_model_various_formats 
tests/unit/test_model_utils.py::TestIntegrationWithCostTracking::test_model_id_extraction_for_cost_calculation 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestIntegrationWithCostTracking::test_model_id_extraction_for_cost_calculation 
tests/unit/test_model_utils.py::TestIntegrationWithCostTracking::test_model_id_extraction_for_unknown_models 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestIntegrationWithCostTracking::test_model_id_extraction_for_unknown_models 
tests/unit/test_model_utils.py::TestIntegrationWithCostTracking::test_model_id_extraction_consistency_across_modules 
[gw2] [ 58%] PASSED tests/unit/test_model_utils.py::TestIntegrationWithCostTracking::test_model_id_extraction_consistency_across_modules 
tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_records_agent_calls 
[gw2] [ 58%] PASSED tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_records_agent_calls 
tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_records_failed_calls 
[gw2] [ 58%] FAILED tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_records_failed_calls 
tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_multiple_calls 
[gw2] [ 59%] PASSED tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_multiple_calls 
tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_execution_time 
[gw2] [ 59%] PASSED tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_execution_time 
tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_clear_functionality 
[gw2] [ 59%] PASSED tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_clear_functionality 
tests/unit/test_monitor_integration.py::TestMonitorIntegrationWithCaplog::test_monitor_with_caplog 
[gw2] [ 59%] PASSED tests/unit/test_monitor_integration.py::TestMonitorIntegrationWithCaplog::test_monitor_with_caplog 
tests/unit/test_orchestrator_typing.py::test_agents_conform_to_protocol 
[gw2] [ 59%] PASSED tests/unit/test_orchestrator_typing.py::test_agents_conform_to_protocol 
tests/unit/test_otel_hook.py::test_hook_initialization_console 
[gw2] [ 59%] PASSED tests/unit/test_otel_hook.py::test_hook_initialization_console 
tests/unit/test_override_agent.py::test_override_agent_basic_functionality 
[gw2] [ 59%] PASSED tests/unit/test_override_agent.py::test_override_agent_basic_functionality 
tests/unit/test_override_agent.py::test_override_agent_with_exception 
[gw2] [ 59%] PASSED tests/unit/test_override_agent.py::test_override_agent_with_exception 
tests/unit/test_override_agent.py::test_override_agent_none_agent 
[gw2] [ 59%] PASSED tests/unit/test_override_agent.py::test_override_agent_none_agent 
tests/unit/test_override_agent.py::test_override_agent_multiple_steps 
[gw2] [ 59%] PASSED tests/unit/test_override_agent.py::test_override_agent_multiple_steps 
tests/unit/test_override_agent.py::test_override_agent_integration_with_arun 
[gw2] [ 59%] PASSED tests/unit/test_override_agent.py::test_override_agent_integration_with_arun 
tests/unit/test_override_agent.py::test_override_agent_with_stub_agent 
[gw2] [ 59%] PASSED tests/unit/test_override_agent.py::test_override_agent_with_stub_agent 
tests/unit/test_parallel_imports.py::test_parallel_imports 
[gw2] [ 59%] PASSED tests/unit/test_parallel_imports.py::test_parallel_imports 
tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_usage_governor_receives_individual_step_results 
[gw2] [ 59%] PASSED tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_usage_governor_receives_individual_step_results 
tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_cancelled_branches_populate_dictionaries 
[gw1] [ 59%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestMeasureTimeDecorators::test_measure_time_async_decorator 
tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_scratch_buffer_reuse 
[gw2] [ 59%] PASSED tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_cancelled_branches_populate_dictionaries 
[gw1] [ 59%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_scratch_buffer_reuse 
tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_usage_limit_breach_propagates_correctly 
tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_pooling_functionality 
[gw1] [ 59%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_pooling_functionality 
tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_pooling_concurrent_access 
[gw1] [ 59%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_pooling_concurrent_access 
tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_pooling_memory_efficiency 
[gw2] [ 59%] PASSED tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_usage_limit_breach_propagates_correctly 
[gw1] [ 59%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_pooling_memory_efficiency 
tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_pooling_edge_cases 
tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_breach_event_propagation_to_agents 
[gw1] [ 59%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_pooling_edge_cases 
tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_reuse_without_reuse 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_breach_event_propagation_to_agents 
tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_parallel_step_handles_empty_branches 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_parallel_step_handles_empty_branches 
tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_parallel_step_handles_failing_branches 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_parallel_step_handles_failing_branches 
tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_parallel_step_concurrency_limits 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_parallel_step_concurrency_limits 
tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_parallel_step_context_isolation 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_robustness.py::TestParallelStepRobustness::test_parallel_step_context_isolation 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_basic_parallel_execution_no_merge 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_basic_parallel_execution_no_merge 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_with_context_include_keys 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_with_context_include_keys 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_no_context 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_no_context 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_with_usage_limits 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_with_usage_limits 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_cost_limit_breach 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_cost_limit_breach 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_token_limit_breach 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_token_limit_breach 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_branch_failure_propagate 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_branch_failure_propagate 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_branch_failure_ignore 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_branch_failure_ignore 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_overwrite 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_overwrite 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_scratchpad 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_scratchpad 
[gw0] [ 60%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_fallback_timestamp_naming 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_all_slots_undeletable_fallback 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_scratchpad_no_scratchpad 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_scratchpad_no_scratchpad 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_custom_merge_strategy 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_custom_merge_strategy 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_exception_handling 
[gw2] [ 60%] PASSED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_exception_handling 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_task_cancellation 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_task_cancellation 
tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_context_update 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_context_update 
tests/unit/test_parallel_step_strategies.py::test_parallel_usage_limit_enforced_atomically 
[gw2] [ 60%] FAILED tests/unit/test_parallel_step_strategies.py::test_parallel_usage_limit_enforced_atomically 
tests/unit/test_parameter_passing.py::test_agent_receives_context_parameter 
[gw2] [ 61%] PASSED tests/unit/test_parameter_passing.py::test_agent_receives_context_parameter 
tests/unit/test_parameter_passing.py::test_plugin_receives_context_parameter 
[gw2] [ 61%] PASSED tests/unit/test_parameter_passing.py::test_plugin_receives_context_parameter 
tests/unit/test_performance.py::TestScratchBufferManagement::test_get_scratch_buffer_creates_new_buffer 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestScratchBufferManagement::test_get_scratch_buffer_creates_new_buffer 
tests/unit/test_performance.py::TestScratchBufferManagement::test_get_scratch_buffer_reuses_existing_buffer 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestScratchBufferManagement::test_get_scratch_buffer_reuses_existing_buffer 
tests/unit/test_performance.py::TestScratchBufferManagement::test_clear_scratch_buffer_clears_contents 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestScratchBufferManagement::test_clear_scratch_buffer_clears_contents 
tests/unit/test_performance.py::TestScratchBufferManagement::test_clear_scratch_buffer_maintains_buffer_identity 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestScratchBufferManagement::test_clear_scratch_buffer_maintains_buffer_identity 
tests/unit/test_performance.py::TestScratchBufferManagement::test_clear_scratch_buffer_when_no_buffer_exists 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestScratchBufferManagement::test_clear_scratch_buffer_when_no_buffer_exists 
tests/unit/test_performance.py::TestScratchBufferManagement::test_buffer_isolation_between_tasks 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestScratchBufferManagement::test_buffer_isolation_between_tasks 
tests/unit/test_performance.py::TestBufferPooling::test_enable_buffer_pooling 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_enable_buffer_pooling 
tests/unit/test_performance.py::TestBufferPooling::test_disable_buffer_pooling 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_disable_buffer_pooling 
tests/unit/test_performance.py::TestBufferPooling::test_get_buffer_pool_stats_when_disabled 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_get_buffer_pool_stats_when_disabled 
tests/unit/test_performance.py::TestBufferPooling::test_get_buffer_pool_stats_when_enabled 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_get_buffer_pool_stats_when_enabled 
tests/unit/test_performance.py::TestBufferPooling::test_buffer_pooling_creates_new_buffers_when_pool_empty 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_buffer_pooling_creates_new_buffers_when_pool_empty 
tests/unit/test_performance.py::TestBufferPooling::test_release_scratch_buffer_when_pooling_disabled 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_release_scratch_buffer_when_pooling_disabled 
tests/unit/test_performance.py::TestBufferPooling::test_release_scratch_buffer_when_no_buffer_exists 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_release_scratch_buffer_when_no_buffer_exists 
tests/unit/test_performance.py::TestBufferPooling::test_release_scratch_buffer_returns_buffer_to_pool 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_release_scratch_buffer_returns_buffer_to_pool 
tests/unit/test_performance.py::TestBufferPooling::test_buffer_pooling_basic_functionality 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_buffer_pooling_basic_functionality 
tests/unit/test_performance.py::TestBufferPooling::test_buffer_pooling_handles_full_pool 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_buffer_pooling_handles_full_pool 
tests/unit/test_performance.py::TestBufferPooling::test_clear_scratch_buffer_consistency_with_pooling 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_clear_scratch_buffer_consistency_with_pooling 
tests/unit/test_performance.py::TestBufferPooling::test_buffer_identity_consistency_with_pooling 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestBufferPooling::test_buffer_identity_consistency_with_pooling 
tests/unit/test_performance.py::TestPerformanceOptimizations::test_time_perf_ns_returns_nanoseconds 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestPerformanceOptimizations::test_time_perf_ns_returns_nanoseconds 
tests/unit/test_performance.py::TestPerformanceOptimizations::test_time_perf_ns_to_seconds_conversion 
[gw2] [ 61%] PASSED tests/unit/test_performance.py::TestPerformanceOptimizations::test_time_perf_ns_to_seconds_conversion 
tests/unit/test_performance.py::TestPerformanceOptimizations::test_measure_time_decorator 
[gw2] [ 62%] PASSED tests/unit/test_performance.py::TestPerformanceOptimizations::test_measure_time_decorator 
tests/unit/test_performance.py::TestPerformanceOptimizations::test_measure_time_async_decorator 
[gw2] [ 62%] PASSED tests/unit/test_performance.py::TestPerformanceOptimizations::test_measure_time_async_decorator 
tests/unit/test_performance.py::TestBufferLeakPrevention::test_no_buffer_leak_with_pooling_enabled 
[gw2] [ 62%] PASSED tests/unit/test_performance.py::TestBufferLeakPrevention::test_no_buffer_leak_with_pooling_enabled 
tests/unit/test_performance.py::TestBufferLeakPrevention::test_consistent_behavior_with_pooling_toggle 
[gw2] [ 62%] PASSED tests/unit/test_performance.py::TestBufferLeakPrevention::test_consistent_behavior_with_pooling_toggle 
tests/unit/test_performance.py::TestBufferLeakPrevention::test_buffer_pool_overflow_handling 
[gw2] [ 62%] PASSED tests/unit/test_performance.py::TestBufferLeakPrevention::test_buffer_pool_overflow_handling 
tests/unit/test_performance_optimizations.py::TestModuleImportSafety::test_import_without_event_loop 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestModuleImportSafety::test_import_without_event_loop 
tests/unit/test_performance_optimizations.py::TestScratchBufferOptimizations::test_scratch_buffer_creation_and_reuse 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestScratchBufferOptimizations::test_scratch_buffer_creation_and_reuse 
tests/unit/test_performance_optimizations.py::TestScratchBufferOptimizations::test_scratch_buffer_isolation 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestScratchBufferOptimizations::test_scratch_buffer_isolation 
tests/unit/test_performance_optimizations.py::TestScratchBufferOptimizations::test_buffer_pooling_optimization 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestScratchBufferOptimizations::test_buffer_pooling_optimization 
tests/unit/test_performance_optimizations.py::TestScratchBufferOptimizations::test_clear_scratch_buffer_optimization 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestScratchBufferOptimizations::test_clear_scratch_buffer_optimization 
tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_with_callable 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_with_callable 
tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_with_direct_value 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_with_direct_value 
tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_with_none 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_with_none 
tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_with_complex_types 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_with_complex_types 
tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_type_safety 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_type_safety 
tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_performance 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestCallableResolutionOptimization::test_resolve_callable_performance 
tests/unit/test_performance_optimizations.py::TestIntegrationOptimizations::test_scratch_buffer_in_async_context 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestIntegrationOptimizations::test_scratch_buffer_in_async_context 
tests/unit/test_performance_optimizations.py::TestIntegrationOptimizations::test_callable_resolution_in_cost_context 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestIntegrationOptimizations::test_callable_resolution_in_cost_context 
tests/unit/test_performance_optimizations.py::TestIntegrationOptimizations::test_optimization_compatibility 
[gw2] [ 62%] PASSED tests/unit/test_performance_optimizations.py::TestIntegrationOptimizations::test_optimization_compatibility 
tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_frequency_optimization 
[gw2] [ 62%] FAILED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_frequency_optimization 
tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_on_step_failure 
[gw2] [ 62%] FAILED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_on_step_failure 
tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_large_context_serialization_performance 
[gw2] [ 62%] PASSED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_large_context_serialization_performance 
tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_serialization_error_handling 
[gw2] [ 63%] PASSED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_serialization_error_handling 
tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_concurrent_persistence_operations 
[gw2] [ 63%] PASSED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_concurrent_persistence_operations 
tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_with_none_context 
[gw2] [ 63%] PASSED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_with_none_context 
tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_with_complex_nested_objects 
[gw0] [ 63%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_all_slots_undeletable_fallback 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_stat_always_raises 
[gw2] [ 63%] PASSED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_with_complex_nested_objects 
tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_performance_under_load 
[gw2] [ 63%] PASSED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_performance_under_load 
tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_with_circular_references 
[gw2] [ 63%] PASSED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_with_circular_references 
tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_default_backend_performance_overhead 
[gw0] [ 63%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_stat_always_raises 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_glob_always_raises 
[gw2] [ 63%] FAILED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_default_backend_performance_overhead 
tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_persistence_overhead_with_large_context 
[gw1] [ 63%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_reuse_without_reuse 
tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_reuse_with_reuse 
[gw2] [ 63%] FAILED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_persistence_overhead_with_large_context 
tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_serialization_optimization_effectiveness 
[gw2] [ 63%] PASSED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_serialization_optimization_effectiveness 
tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_first_principles_caching_effectiveness 
[gw2] [ 63%] PASSED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_first_principles_caching_effectiveness 
tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_delta_detection_accuracy 
[gw2] [ 63%] PASSED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_delta_detection_accuracy 
tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_buffer_pooling_consistency_fix 
[gw2] [ 63%] PASSED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_buffer_pooling_consistency_fix 
tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_cache_consistency_data_loss_prevention 
[gw2] [ 63%] PASSED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_cache_consistency_data_loss_prevention 
tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_cache_eviction_logic_fixes 
[gw2] [ 63%] PASSED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_cache_eviction_logic_fixes 
tests/unit/test_pipeline_context.py::test_context_initialization_and_access 
[gw2] [ 63%] PASSED tests/unit/test_pipeline_context.py::test_context_initialization_and_access 
tests/unit/test_pipeline_context.py::test_context_initialization_failure 
[gw2] [ 63%] PASSED tests/unit/test_pipeline_context.py::test_context_initialization_failure 
tests/unit/test_pipeline_context.py::test_context_mutation_between_steps 
[gw2] [ 63%] PASSED tests/unit/test_pipeline_context.py::test_context_mutation_between_steps 
tests/unit/test_pipeline_context.py::test_context_isolated_per_run 
[gw2] [ 63%] PASSED tests/unit/test_pipeline_context.py::test_context_isolated_per_run 
tests/unit/test_pipeline_context.py::test_plugin_receives_context_and_strict_plugin_errors 
[gw2] [ 63%] FAILED tests/unit/test_pipeline_context.py::test_plugin_receives_context_and_strict_plugin_errors 
tests/unit/test_pipeline_context.py::test_step_updates_context_automatic_merge 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_context.py::test_step_updates_context_automatic_merge 
tests/unit/test_pipeline_dsl_deprecation.py::test_pipeline_dsl_deprecation_warning 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_dsl_deprecation.py::test_pipeline_dsl_deprecation_warning 
tests/unit/test_pipeline_registry.py::test_register_and_get 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_registry.py::test_register_and_get 
tests/unit/test_pipeline_registry.py::test_get_latest 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_registry.py::test_get_latest 
tests/unit/test_pipeline_registry.py::test_invalid_version 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_registry.py::test_invalid_version 
tests/unit/test_pipeline_validation.py::test_no_warning_for_context 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_validation.py::test_no_warning_for_context 
tests/unit/test_pipeline_validation.py::test_no_warning_for_modern_context 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_validation.py::test_no_warning_for_modern_context 
tests/unit/test_pipeline_visualization.py::test_simple_pipeline_visualization 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization.py::test_simple_pipeline_visualization 
tests/unit/test_pipeline_visualization.py::test_pipeline_with_validation_steps 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization.py::test_pipeline_with_validation_steps 
tests/unit/test_pipeline_visualization.py::test_pipeline_with_retry_config 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization.py::test_pipeline_with_retry_config 
tests/unit/test_pipeline_visualization.py::test_loop_step_visualization 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization.py::test_loop_step_visualization 
tests/unit/test_pipeline_visualization.py::test_conditional_step_visualization 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization.py::test_conditional_step_visualization 
tests/unit/test_pipeline_visualization.py::test_parallel_step_visualization 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization.py::test_parallel_step_visualization 
tests/unit/test_pipeline_visualization.py::test_human_in_the_loop_visualization 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization.py::test_human_in_the_loop_visualization 
tests/unit/test_pipeline_visualization.py::test_complex_nested_pipeline_visualization 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization.py::test_complex_nested_pipeline_visualization 
tests/unit/test_pipeline_visualization.py::test_pipeline_with_mixed_configurations 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization.py::test_pipeline_with_mixed_configurations 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_detail_level_validation 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_detail_level_validation 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_complexity_score_calculation 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_complexity_score_calculation 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_optimal_detail_level_detection 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_optimal_detail_level_detection 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_high_detail_mermaid_generation 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_high_detail_mermaid_generation 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_medium_detail_mermaid_generation 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_medium_detail_mermaid_generation 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_low_detail_mermaid_generation 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_low_detail_mermaid_generation 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_auto_detail_level_selection 
[gw2] [ 64%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_auto_detail_level_selection 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_default_to_mermaid_uses_auto 
[gw2] [ 65%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_default_to_mermaid_uses_auto 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_retry_annotations_in_different_levels 
[gw2] [ 65%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_retry_annotations_in_different_levels 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_validation_annotations_in_different_levels 
[gw2] [ 65%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_validation_annotations_in_different_levels 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_parallel_step_visualization 
[gw2] [ 65%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_parallel_step_visualization 
tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_human_in_the_loop_visualization 
[gw2] [ 65%] PASSED tests/unit/test_pipeline_visualization_detail_levels.py::TestPipelineVisualizationDetailLevels::test_human_in_the_loop_visualization 
tests/unit/test_plugins.py::test_plugin_protocol_instance 
[gw2] [ 65%] PASSED tests/unit/test_plugins.py::test_plugin_protocol_instance 
tests/unit/test_plugins.py::test_plugins 
[gw2] [ 65%] PASSED tests/unit/test_plugins.py::test_plugins 
tests/unit/test_prometheus_collector.py::test_prometheus_collector_yields_metrics 
[gw2] [ 65%] PASSED tests/unit/test_prometheus_collector.py::test_prometheus_collector_yields_metrics 
tests/unit/test_prompt_formatter.py::test_baseline_placeholder_and_json 
[gw2] [ 65%] PASSED tests/unit/test_prompt_formatter.py::test_baseline_placeholder_and_json 
tests/unit/test_prompt_formatter.py::test_if_block 
[gw2] [ 65%] PASSED tests/unit/test_prompt_formatter.py::test_if_block 
tests/unit/test_prompt_formatter.py::test_each_block 
[gw2] [ 65%] PASSED tests/unit/test_prompt_formatter.py::test_each_block 
tests/unit/test_prompt_formatter.py::test_nested_placeholders 
[gw2] [ 65%] PASSED tests/unit/test_prompt_formatter.py::test_nested_placeholders 
tests/unit/test_prompt_formatter.py::test_escaping 
[gw2] [ 65%] PASSED tests/unit/test_prompt_formatter.py::test_escaping 
tests/unit/test_prompt_formatter.py::test_prompt_robust_serialization 
[gw2] [ 65%] FAILED tests/unit/test_prompt_formatter.py::test_prompt_robust_serialization 
tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_basic 
[gw2] [ 65%] PASSED tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_basic 
tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_preserves_computed_fields 
[gw2] [ 65%] PASSED tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_preserves_computed_fields 
tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_triggers_validators 
[gw2] [ 65%] PASSED tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_triggers_validators 
tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_handles_invalid_values 
[gw2] [ 65%] PASSED tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_handles_invalid_values 
tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_handles_complex_equality 
[gw2] [ 65%] PASSED tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_handles_complex_equality 
tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_ignores_private_fields 
[gw2] [ 65%] PASSED tests/unit/test_pydantic_context_merging.py::test_safe_merge_context_updates_ignores_private_fields 
tests/unit/test_pydantic_context_merging.py::test_safe_context_field_update 
[gw2] [ 65%] PASSED tests/unit/test_pydantic_context_merging.py::test_safe_context_field_update 
tests/unit/test_pydantic_context_merging.py::test_safe_context_field_update_triggers_validation 
[gw2] [ 65%] PASSED tests/unit/test_pydantic_context_merging.py::test_safe_context_field_update_triggers_validation 
tests/unit/test_pydantic_context_merging.py::test_safe_context_field_update_handles_invalid_values 
[gw2] [ 66%] PASSED tests/unit/test_pydantic_context_merging.py::test_safe_context_field_update_handles_invalid_values 
tests/unit/test_pydantic_context_merging.py::test_get_context_field_safely 
[gw2] [ 66%] PASSED tests/unit/test_pydantic_context_merging.py::test_get_context_field_safely 
tests/unit/test_pydantic_context_merging.py::test_has_context_field 
[gw2] [ 66%] PASSED tests/unit/test_pydantic_context_merging.py::test_has_context_field 
tests/unit/test_pydantic_context_merging.py::test_merge_with_none_contexts 
[gw2] [ 66%] PASSED tests/unit/test_pydantic_context_merging.py::test_merge_with_none_contexts 
tests/unit/test_pydantic_context_merging.py::test_merge_with_different_types 
[gw2] [ 66%] PASSED tests/unit/test_pydantic_context_merging.py::test_merge_with_different_types 
tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_preserves_nested_models 
[gw2] [ 66%] PASSED tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_preserves_nested_models 
tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_string_encoded_lists 
[gw2] [ 66%] PASSED tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_string_encoded_lists 
tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_empty_structures 
[gw2] [ 66%] PASSED tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_empty_structures 
tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_none_values 
[gw2] [ 66%] PASSED tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_none_values 
tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_boolean_values 
[gw2] [ 66%] PASSED tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_boolean_values 
tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_complex_nested_structures 
[gw2] [ 66%] PASSED tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_complex_nested_structures 
tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_mixed_types 
[gw2] [ 66%] PASSED tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_handles_mixed_types 
tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_preserves_exact_types 
[gw2] [ 66%] PASSED tests/unit/test_reconstruction_logic.py::TestReconstructionLogic::test_reconstruction_preserves_exact_types 
tests/unit/test_safe_deserialize_roundtrip.py::test_default_behavior_returns_serialized_data 
[gw2] [ 66%] PASSED tests/unit/test_safe_deserialize_roundtrip.py::test_default_behavior_returns_serialized_data 
tests/unit/test_safe_deserialize_roundtrip.py::test_custom_type_roundtrip_with_registry 
[gw2] [ 66%] PASSED tests/unit/test_safe_deserialize_roundtrip.py::test_custom_type_roundtrip_with_registry 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_adds_missing_columns_to_runs_table 
[gw2] [ 66%] PASSED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_adds_missing_columns_to_runs_table 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_existing_data 
[gw2] [ 66%] PASSED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_existing_data 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_corrupted_database 
[gw0] [ 66%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_glob_always_raises 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_unlink_always_raises 
[gw2] [ 66%] FAILED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_corrupted_database 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_missing_tables 
[gw2] [ 66%] PASSED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_missing_tables 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_null_values_in_not_null_columns 
[gw2] [ 66%] PASSED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_null_values_in_not_null_columns 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_concurrent_access 
[gw2] [ 66%] PASSED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_concurrent_access 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_large_datasets 
[gw2] [ 67%] PASSED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_large_datasets 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_schema_version_changes 
[gw2] [ 67%] FAILED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_schema_version_changes 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_index_recreation 
[gw2] [ 67%] PASSED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_index_recreation 
tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_foreign_key_constraints 
[gw2] [ 67%] FAILED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_foreign_key_constraints 
tests/unit/test_scoring.py::test_ratio_score 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_ratio_score 
tests/unit/test_scoring.py::test_weighted_score 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_weighted_score 
tests/unit/test_scoring.py::test_reward_scorer_init_success 
[gw3] [ 67%] FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_lens_list_with_large_mixed_database 
tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_lens_list_with_various_filters 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_reward_scorer_init_success 
tests/unit/test_scoring.py::test_reward_scorer_init_failure 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_reward_scorer_init_failure 
tests/unit/test_scoring.py::test_reward_scorer_returns_float 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_reward_scorer_returns_float 
tests/unit/test_scoring.py::test_reward_scorer_disabled 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_reward_scorer_disabled 
tests/unit/test_scoring.py::test_weighted_score_empty_weights 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_weighted_score_empty_weights 
tests/unit/test_scoring.py::test_weighted_score_total_weight_zero 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_weighted_score_total_weight_zero 
tests/unit/test_scoring.py::test_redact_string_no_secret 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_redact_string_no_secret 
tests/unit/test_scoring.py::test_redact_string_secret_not_in_text 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_redact_string_secret_not_in_text 
tests/unit/test_scoring.py::test_redact_string_secret_in_text 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_redact_string_secret_in_text 
tests/unit/test_scoring.py::test_reward_scorer_score_no_output 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_reward_scorer_score_no_output 
tests/unit/test_scoring.py::test_ratio_score_all_passed 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_ratio_score_all_passed 
tests/unit/test_scoring.py::test_weighted_score_all_weights_present 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_weighted_score_all_weights_present 
tests/unit/test_scoring.py::test_weighted_score_invalid_weight_type 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_weighted_score_invalid_weight_type 
tests/unit/test_scoring.py::test_weighted_score_missing_keys 
[gw2] [ 67%] PASSED tests/unit/test_scoring.py::test_weighted_score_missing_keys 
tests/unit/test_self_improvement_agent.py::test_self_improve_sys_contains_examples 
[gw2] [ 67%] PASSED tests/unit/test_self_improvement_agent.py::test_self_improve_sys_contains_examples 
tests/unit/test_self_improvement_agent.py::test_self_improvement_agent_parses_json 
[gw2] [ 68%] PASSED tests/unit/test_self_improvement_agent.py::test_self_improvement_agent_parses_json 
tests/unit/test_self_improvement_agent.py::test_self_improvement_agent_handles_bad_json 
[gw2] [ 68%] PASSED tests/unit/test_self_improvement_agent.py::test_self_improvement_agent_handles_bad_json 
tests/unit/test_serialization_core.py::test_determinism_with_unordered_collections 
[gw2] [ 68%] PASSED tests/unit/test_serialization_core.py::test_determinism_with_unordered_collections 
tests/unit/test_serialization_core.py::test_circular_reference_handling 
[gw2] [ 68%] PASSED tests/unit/test_serialization_core.py::test_circular_reference_handling 
tests/unit/test_serialization_core.py::test_custom_serializer_is_honored 
[gw2] [ 68%] PASSED tests/unit/test_serialization_core.py::test_custom_serializer_is_honored 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_empty_structures 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_empty_structures 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_none_values 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_none_values 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_numeric_edge_cases 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_numeric_edge_cases 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_string_edge_cases 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_string_edge_cases 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_collection_edge_cases 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_collection_edge_cases 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_enum_and_union_edge_cases 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_enum_and_union_edge_cases 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_custom_types_edge_cases 
[gw2] [ 68%] FAILED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_custom_types_edge_cases 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_recursive_structures 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_recursive_structures 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_dataclass_serialization 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_dataclass_serialization 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_collections_edge_cases 
[gw2] [ 68%] FAILED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_collections_edge_cases 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_float_edge_cases 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_float_edge_cases 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_complex_nested_structures 
[gw0] [ 68%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_unlink_always_raises 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_complex_nested_structures 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_error_handling 
tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_permission_and_race_conditions 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_error_handling 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_circular_reference_handling 
[gw2] [ 68%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_circular_reference_handling 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_large_data_structures 
[gw1] [ 68%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestBufferReuse::test_buffer_reuse_with_reuse 
tests/benchmarks/test_performance_optimizations 2.py::TestEndToEndPerformance::test_end_to_end_performance 
[gw0] [ 68%] FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_permission_and_race_conditions 
tests/integration/test_stateful_hitl.py::test_stateful_hitl_resume 
[gw0] [ 69%] FAILED tests/integration/test_stateful_hitl.py::test_stateful_hitl_resume 
tests/integration/test_stateful_hitl.py::test_hitl_resume_with_pydantic_input 
[gw0] [ 69%] PASSED tests/integration/test_stateful_hitl.py::test_hitl_resume_with_pydantic_input 
tests/integration/test_stateful_hitl.py::test_hitl_as_final_step 
[gw0] [ 69%] PASSED tests/integration/test_stateful_hitl.py::test_hitl_as_final_step 
tests/integration/test_stateful_runner.py::test_runner_uses_state_backend 
[gw0] [ 69%] PASSED tests/integration/test_stateful_runner.py::test_runner_uses_state_backend 
tests/integration/test_stateful_runner.py::test_resume_from_saved_state 
[gw0] [ 69%] PASSED tests/integration/test_stateful_runner.py::test_resume_from_saved_state 
tests/integration/test_stateful_runner.py::test_delete_on_completion_removes_state 
[gw0] [ 69%] PASSED tests/integration/test_stateful_runner.py::test_delete_on_completion_removes_state 
tests/integration/test_stateful_runner.py::test_invalid_step_index_raises 
[gw0] [ 69%] PASSED tests/integration/test_stateful_runner.py::test_invalid_step_index_raises 
tests/integration/test_stateful_runner.py::test_cancelled_pipeline_state_saved 
[gw0] [ 69%] PASSED tests/integration/test_stateful_runner.py::test_cancelled_pipeline_state_saved 
tests/integration/test_streaming_pipeline.py::test_basic_streaming 
[gw0] [ 69%] FAILED tests/integration/test_streaming_pipeline.py::test_basic_streaming 
tests/integration/test_streaming_pipeline.py::test_non_streaming_pipeline 
[gw0] [ 69%] PASSED tests/integration/test_streaming_pipeline.py::test_non_streaming_pipeline 
tests/integration/test_streaming_pipeline.py::test_context_and_resources_in_stream 
[gw0] [ 69%] FAILED tests/integration/test_streaming_pipeline.py::test_context_and_resources_in_stream 
tests/integration/test_streaming_pipeline.py::test_pipeline_handles_streaming_agent_failure_gracefully 
[gw0] [ 69%] FAILED tests/integration/test_streaming_pipeline.py::test_pipeline_handles_streaming_agent_failure_gracefully 
tests/integration/test_strict_validation.py::test_non_strict_validation_pass_through 
[gw0] [ 69%] FAILED tests/integration/test_strict_validation.py::test_non_strict_validation_pass_through 
tests/integration/test_strict_validation.py::test_strict_validation_drops_output 
[gw0] [ 69%] FAILED tests/integration/test_strict_validation.py::test_strict_validation_drops_output 
tests/integration/test_strict_validation.py::test_regular_step_keeps_output_on_validation_failure 
[gw0] [ 69%] PASSED tests/integration/test_strict_validation.py::test_regular_step_keeps_output_on_validation_failure 
tests/integration/test_trace_complete_flow.py::TestTraceCompleteFlow::test_complete_trace_flow 
[gw0] [ 69%] PASSED tests/integration/test_trace_complete_flow.py::TestTraceCompleteFlow::test_complete_trace_flow 
tests/integration/test_trace_complete_flow.py::TestTraceCompleteFlow::test_trace_with_failed_step 
[gw0] [ 69%] PASSED tests/integration/test_trace_complete_flow.py::TestTraceCompleteFlow::test_trace_with_failed_step 
tests/integration/test_trace_complete_flow.py::TestTraceCompleteFlow::test_trace_without_backend 
[gw0] [ 69%] PASSED tests/integration/test_trace_complete_flow.py::TestTraceCompleteFlow::test_trace_without_backend 
tests/integration/test_unified_error_handling.py::test_simple_step_returns_stepresult_on_failure 
[gw0] [ 69%] PASSED tests/integration/test_unified_error_handling.py::test_simple_step_returns_stepresult_on_failure 
tests/integration/test_unified_error_handling.py::test_streaming_step_returns_stepresult_on_failure 
[gw0] [ 69%] FAILED tests/integration/test_unified_error_handling.py::test_streaming_step_returns_stepresult_on_failure 
tests/integration/test_unified_error_handling.py::test_complex_step_returns_stepresult_on_failure 
[gw0] [ 69%] PASSED tests/integration/test_unified_error_handling.py::test_complex_step_returns_stepresult_on_failure 
tests/integration/test_unified_error_handling.py::test_critical_exceptions_are_re_raised 
[gw0] [ 69%] FAILED tests/integration/test_unified_error_handling.py::test_critical_exceptions_are_re_raised 
tests/integration/test_unified_error_handling.py::test_consistent_api_contract 
[gw0] [ 70%] FAILED tests/integration/test_unified_error_handling.py::test_consistent_api_contract 
tests/integration/test_unified_error_handling.py::test_error_information_preservation 
[gw0] [ 70%] PASSED tests/integration/test_unified_error_handling.py::test_error_information_preservation 
tests/integration/test_unified_error_handling.py::test_pipeline_continuation_behavior 
[gw0] [ 70%] PASSED tests/integration/test_unified_error_handling.py::test_pipeline_continuation_behavior 
tests/integration/test_unified_error_handling.py::test_timing_preservation_for_failed_steps 
[gw0] [ 70%] PASSED tests/integration/test_unified_error_handling.py::test_timing_preservation_for_failed_steps 
tests/integration/test_usage_governor.py::test_governor_halts_on_cost_limit_breach 
[gw0] [ 70%] PASSED tests/integration/test_usage_governor.py::test_governor_halts_on_cost_limit_breach 
tests/integration/test_usage_governor.py::test_governor_halts_on_token_limit_breach 
[gw0] [ 70%] PASSED tests/integration/test_usage_governor.py::test_governor_halts_on_token_limit_breach 
tests/integration/test_usage_governor.py::test_governor_allows_completion_within_limits 
[gw0] [ 70%] PASSED tests/integration/test_usage_governor.py::test_governor_allows_completion_within_limits 
tests/integration/test_usage_governor.py::test_governor_inactive_when_no_limits_provided 
[gw0] [ 70%] PASSED tests/integration/test_usage_governor.py::test_governor_inactive_when_no_limits_provided 
tests/integration/test_usage_governor.py::test_governor_halts_immediately_on_zero_limit 
[gw0] [ 70%] PASSED tests/integration/test_usage_governor.py::test_governor_halts_immediately_on_zero_limit 
tests/integration/test_usage_governor.py::test_governor_with_loop_step 
[gw0] [ 70%] FAILED tests/integration/test_usage_governor.py::test_governor_with_loop_step 
tests/integration/test_usage_governor.py::test_governor_halts_loop_step_mid_iteration 
[gw0] [ 70%] FAILED tests/integration/test_usage_governor.py::test_governor_halts_loop_step_mid_iteration 
tests/integration/test_usage_governor.py::test_governor_parallel_step_limit 
[gw0] [ 70%] PASSED tests/integration/test_usage_governor.py::test_governor_parallel_step_limit 
tests/integration/test_usage_governor.py::test_governor_loop_with_nested_parallel_limit 
[gw0] [ 70%] FAILED tests/integration/test_usage_governor.py::test_governor_loop_with_nested_parallel_limit 
tests/integration/test_usage_governor.py::test_governor_parallel_limit_first_branch_exceeds 
[gw0] [ 70%] PASSED tests/integration/test_usage_governor.py::test_governor_parallel_limit_first_branch_exceeds 
tests/integration/test_usage_governor.py::test_governor_cumulative_cost_updates 
[gw0] [ 70%] PASSED tests/integration/test_usage_governor.py::test_governor_cumulative_cost_updates 
tests/integration/test_usage_governor.py::test_precise_breach_on_final_step 
[gw0] [ 70%] PASSED tests/integration/test_usage_governor.py::test_precise_breach_on_final_step 
tests/integration/test_usage_limits_enforcement.py::test_debug_usage_limits_passing 
[gw0] [ 70%] PASSED tests/integration/test_usage_limits_enforcement.py::test_debug_usage_limits_passing 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_simple_steps 
[gw0] [ 70%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_simple_steps 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_token_limits 
[gw0] [ 70%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_token_limits 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_loop_steps 
[gw0] [ 70%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_loop_steps 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_complex_steps 
[gw0] [ 70%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_complex_steps 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_no_enforcement_when_no_limits 
[gw0] [ 70%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_no_enforcement_when_no_limits 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_precise_timing 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_enforcement_precise_timing 
tests/integration/test_usage_limits_enforcement.py::test_debug_usage_limits_detailed 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_debug_usage_limits_detailed 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_zero_cost 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_zero_cost 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_zero_tokens 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_zero_tokens 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_very_small_limits 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_very_small_limits 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_very_large_limits 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_very_large_limits 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_mixed_limits 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_mixed_limits 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_conditional_steps 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_conditional_steps 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_parallel_steps 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_parallel_steps 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_nested_pipelines 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_nested_pipelines 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_retry_mechanism 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_retry_mechanism 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_fallback_mechanism 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_fallback_mechanism 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_state_persistence 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_state_persistence 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_concurrent_execution 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_concurrent_execution 
tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_error_handling 
[gw0] [ 71%] PASSED tests/integration/test_usage_limits_enforcement.py::test_usage_limits_edge_case_error_handling 
tests/integration/test_validation_persistence.py::test_persist_feedback_and_results 
[gw0] [ 71%] FAILED tests/integration/test_validation_persistence.py::test_persist_feedback_and_results 
tests/integration/test_validation_persistence.py::test_persist_results_on_success 
[gw0] [ 71%] FAILED tests/integration/test_validation_persistence.py::test_persist_results_on_success 
tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_basic 
[gw0] [ 71%] PASSED tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_basic 
tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_no_context 
[gw0] [ 71%] PASSED tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_no_context 
tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_empty_vars 
[gw0] [ 71%] PASSED tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_empty_vars 
tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_missing_vars 
[gw0] [ 71%] PASSED tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_missing_vars 
tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_complex_values 
[gw0] [ 71%] PASSED tests/processors/test_common.py::TestAddContextVariables::test_add_context_variables_complex_values 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_basic 
[gw0] [ 71%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_basic 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_with_language 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_with_language 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_no_fences 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_no_fences 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_partial_fences 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_partial_fences 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_nested_fences 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_nested_fences 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_multiple_blocks 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_multiple_blocks 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_empty_block 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_empty_block 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_whitespace_handling 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_whitespace_handling 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_special_characters 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_special_characters 
tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_non_string_input 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestStripMarkdownFences::test_strip_markdown_fences_non_string_input 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_dict_input 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_dict_input 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_list_input 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_list_input 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_string_json 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_string_json 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_string_list_json 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_string_list_json 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_invalid_json 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_invalid_json 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_plain_string 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_plain_string 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_none_input 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_none_input 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_empty_string 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_empty_string 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_whitespace_string 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_whitespace_string 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_complex_json 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_complex_json 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_numeric_input 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_numeric_input 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_boolean_input 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_boolean_input 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_json_with_unicode 
[gw0] [ 72%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_json_with_unicode 
tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_malformed_json_edge_cases 
[gw0] [ 73%] PASSED tests/processors/test_common.py::TestEnforceJsonResponse::test_enforce_json_response_malformed_json_edge_cases 
tests/processors/test_common.py::TestProcessorIntegration::test_processors_chain 
[gw0] [ 73%] PASSED tests/processors/test_common.py::TestProcessorIntegration::test_processors_chain 
tests/processors/test_common.py::TestProcessorIntegration::test_processors_with_context 
[gw0] [ 73%] PASSED tests/processors/test_common.py::TestProcessorIntegration::test_processors_with_context 
tests/processors/test_common.py::TestProcessorIntegration::test_processors_error_handling 
[gw0] [ 73%] PASSED tests/processors/test_common.py::TestProcessorIntegration::test_processors_error_handling 
tests/processors/test_common.py::TestEdgeCases::test_add_context_variables_missing_context_attribute 
[gw0] [ 73%] PASSED tests/processors/test_common.py::TestEdgeCases::test_add_context_variables_missing_context_attribute 
tests/processors/test_common.py::TestEdgeCases::test_strip_markdown_fences_edge_cases 
[gw0] [ 73%] PASSED tests/processors/test_common.py::TestEdgeCases::test_strip_markdown_fences_edge_cases 
tests/processors/test_common.py::TestEdgeCases::test_enforce_json_response_edge_cases 
[gw0] [ 73%] PASSED tests/processors/test_common.py::TestEdgeCases::test_enforce_json_response_edge_cases 
tests/processors/test_repair.py::test_literal_eval_size_guard 
[gw1] [ 73%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestEndToEndPerformance::test_end_to_end_performance 
tests/benchmarks/test_performance_optimizations.py::TestPerfCounterPrecision::test_perf_counter_standard 
[gw0] [ 73%] PASSED tests/processors/test_repair.py::test_literal_eval_size_guard 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_legacy_behavior_preserved 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_legacy_behavior_preserved 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_error_handling_regression 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_error_handling_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_metrics_accumulation_regression 
[gw0] [ 73%] FAILED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_metrics_accumulation_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_context_handling_regression 
[gw0] [ 73%] FAILED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_context_handling_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_branch_output_mapper_regression 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_branch_output_mapper_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_branch_input_mapper_regression 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_branch_input_mapper_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_default_branch_regression 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_default_branch_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_multiple_steps_regression 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_multiple_steps_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_attempts_field_regression 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_attempts_field_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_telemetry_regression 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_telemetry_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_null_parameters_regression 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_null_parameters_regression 
tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_empty_branches_regression 
[gw0] [ 73%] PASSED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_empty_branches_regression 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_basic_step_execution_preserved 
[gw0] [ 73%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_basic_step_execution_preserved 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_error_handling_preserved 
[gw0] [ 74%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_error_handling_preserved 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_context_passing_preserved 
[gw0] [ 74%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_context_passing_preserved 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_caching_behavior_preserved 
[gw0] [ 74%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_caching_behavior_preserved 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_usage_limits_preserved 
[gw0] [ 74%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationFunctionalityPreservation::test_usage_limits_preserved 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_legacy_constructor_compatibility 
[gw0] [ 74%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_legacy_constructor_compatibility 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_optimization_config_defaults 
[gw0] [ 74%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_optimization_config_defaults 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_optimization_config_validation 
[gw0] [ 74%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_optimization_config_validation 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_api_compatibility 
[gw0] [ 74%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_api_compatibility 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_configuration_serialization 
[gw0] [ 74%] FAILED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_configuration_serialization 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationErrorHandling::test_optimization_failure_fallback 
[gw0] [ 74%] PASSED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationErrorHandling::test_optimization_failure_fallback 
tests/regression/test_executor_core_optimization_regression.py::TestOptimizationErrorHandling::test_invalid_step_handling 
[gw0] [ 74%] FAILED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationErrorHandling::test_invalid_step_handling 
tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_list 
[gw0] [ 74%] PASSED tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_list 
tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_dict 
[gw0] [ 74%] PASSED tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_dict 
tests/unit/test_serialization_utilities.py::TestRobustSerialize::test_robust_serialize_basic_types 
[gw0] [ 74%] PASSED tests/unit/test_serialization_utilities.py::TestRobustSerialize::test_robust_serialize_basic_types 
tests/unit/test_serialization_utilities.py::TestRobustSerialize::test_robust_serialize_complex_objects 
[gw0] [ 74%] PASSED tests/unit/test_serialization_utilities.py::TestRobustSerialize::test_robust_serialize_complex_objects 
tests/unit/test_serialization_utilities.py::TestRobustSerialize::test_robust_serialize_with_fallback 
[gw0] [ 74%] PASSED tests/unit/test_serialization_utilities.py::TestRobustSerialize::test_robust_serialize_with_fallback 
tests/unit/test_serialization_utilities.py::TestSerializeToJson::test_serialize_to_json_basic 
[gw0] [ 74%] PASSED tests/unit/test_serialization_utilities.py::TestSerializeToJson::test_serialize_to_json_basic 
tests/unit/test_serialization_utilities.py::TestSerializeToJson::test_serialize_to_json_with_kwargs 
[gw0] [ 74%] PASSED tests/unit/test_serialization_utilities.py::TestSerializeToJson::test_serialize_to_json_with_kwargs 
tests/unit/test_serialization_utilities.py::TestSerializeToJson::test_serialize_to_json_robust 
[gw0] [ 74%] PASSED tests/unit/test_serialization_utilities.py::TestSerializeToJson::test_serialize_to_json_robust 
tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_text_roundtrip 
[gw2] [ 74%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_large_data_structures 
tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_type_preservation_edge_cases 
[gw2] [ 74%] PASSED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_type_preservation_edge_cases 
tests/unit/test_serialization_edge_cases.py::test_circular_reference_in_dict_keys 
[gw2] [ 74%] PASSED tests/unit/test_serialization_edge_cases.py::test_circular_reference_in_dict_keys 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_simple_model_roundtrip 
[gw0] [ 75%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_text_roundtrip 
tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_integer_roundtrip 
[gw0] [ 75%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_integer_roundtrip 
tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_float_roundtrip 
[gw0] [ 75%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_float_roundtrip 
tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_boolean_roundtrip 
[gw0] [ 75%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_boolean_roundtrip 
tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_list_roundtrip 
[gw0] [ 75%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_list_roundtrip 
tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_dict_roundtrip 
[gw2] [ 75%] PASSED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_simple_model_roundtrip 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_nested_model_roundtrip 
[gw0] [ 75%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_safe_serialize_dict_roundtrip 
tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_serialize_to_json_roundtrip 
[gw2] [ 75%] PASSED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_nested_model_roundtrip 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_complex_model_roundtrip 
[gw0] [ 75%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_serialize_to_json_roundtrip 
tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_serialize_to_json_robust_roundtrip 
[gw0] [ 75%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationProperties::test_serialize_to_json_robust_roundtrip 
tests/unit/test_settings.py::test_env_var_precedence 
[gw0] [ 75%] PASSED tests/unit/test_settings.py::test_env_var_precedence 
tests/unit/test_settings.py::test_defaults 
[gw0] [ 75%] PASSED tests/unit/test_settings.py::test_defaults 
tests/unit/test_settings.py::test_invalid_env_vars 
[gw0] [ 75%] PASSED tests/unit/test_settings.py::test_invalid_env_vars 
tests/unit/test_settings.py::test_logfire_legacy_alias 
[gw0] [ 75%] PASSED tests/unit/test_settings.py::test_logfire_legacy_alias 
tests/unit/test_settings.py::test_missing_api_key_allowed 
[gw0] [ 75%] PASSED tests/unit/test_settings.py::test_missing_api_key_allowed 
tests/unit/test_settings.py::test_settings_constructor_values 
[gw0] [ 75%] PASSED tests/unit/test_settings.py::test_settings_constructor_values 
tests/unit/test_settings.py::test_settings_initialization 
[gw0] [ 75%] PASSED tests/unit/test_settings.py::test_settings_initialization 
tests/unit/test_settings.py::test_test_settings 
[gw0] [ 75%] PASSED tests/unit/test_settings.py::test_test_settings 
tests/unit/test_signature_injection.py::test_context_injected 
[gw0] [ 75%] PASSED tests/unit/test_signature_injection.py::test_context_injected 
tests/unit/test_signature_injection.py::test_resources_injected 
[gw0] [ 75%] PASSED tests/unit/test_signature_injection.py::test_resources_injected 
tests/unit/test_signature_injection.py::test_legacy_context_works 
[gw0] [ 75%] PASSED tests/unit/test_signature_injection.py::test_legacy_context_works 
tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_get_failed_workflows_sql_injection_resistance 
[gw0] [ 75%] PASSED tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_get_failed_workflows_sql_injection_resistance 
tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_cleanup_old_workflows_sql_injection_resistance 
[gw0] [ 76%] PASSED tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_cleanup_old_workflows_sql_injection_resistance 
tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_list_workflows_sql_injection_resistance 
[gw0] [ 76%] PASSED tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_list_workflows_sql_injection_resistance 
tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_save_state_sql_injection_resistance 
[gw0] [ 76%] PASSED tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_save_state_sql_injection_resistance 
tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_load_state_sql_injection_resistance 
[gw0] [ 76%] PASSED tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_load_state_sql_injection_resistance 
tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_delete_state_sql_injection_resistance 
[gw0] [ 76%] PASSED tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_delete_state_sql_injection_resistance 
tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_parameterized_queries_work_correctly 
[gw0] [ 76%] PASSED tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_parameterized_queries_work_correctly 
tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_edge_cases_and_boundary_values 
[gw2] [ 76%] PASSED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_complex_model_roundtrip 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_unsupported_types_raise 
[gw0] [ 76%] PASSED tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_edge_cases_and_boundary_values 
[gw2] [ 76%] FAILED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_unsupported_types_raise 
tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_sql_injection_prevention_patterns 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_deeply_nested_model_roundtrip 
[gw0] [ 76%] PASSED tests/unit/test_sql_injection_security.py::TestSQLInjectionSecurity::test_sql_injection_prevention_patterns 
tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_save_run_start_missing_pipeline_id_handled_gracefully 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_save_run_start_missing_pipeline_id_handled_gracefully 
tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_save_run_start_partial_data_handled_gracefully 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_save_run_start_partial_data_handled_gracefully 
tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_schema_migration_handles_existing_runs_table 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_schema_migration_handles_existing_runs_table 
tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_schema_migration_handles_missing_columns 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_schema_migration_handles_missing_columns 
tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_schema_migration_handles_corrupted_database 
[gw1] [ 76%] PASSED tests/benchmarks/test_performance_optimizations.py::TestPerfCounterPrecision::test_perf_counter_standard 
tests/benchmarks/test_performance_optimizations.py::TestPerfCounterPrecision::test_perf_counter_ns_precision 
[gw0] [ 76%] FAILED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_schema_migration_handles_corrupted_database 
tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_concurrent_access_handled_gracefully 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_concurrent_access_handled_gracefully 
tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_database_locking_handled_properly 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_database_locking_handled_properly 
tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_error_propagation_maintains_context 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_error_propagation_maintains_context 
tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_retry_logic_handles_transient_errors 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_retry_logic_handles_transient_errors 
tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_save_and_get_trace 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_save_and_get_trace 
tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_get_spans_with_filtering 
[gw0] [ 76%] PASSED tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_get_spans_with_filtering 
tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_span_statistics 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_span_statistics 
tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_cascade_deletion 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_cascade_deletion 
tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_complex_nested_trace 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_complex_nested_trace 
tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_span_attributes_preservation 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_span_attributes_preservation 
tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_empty_trace_handling 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_empty_trace_handling 
tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_concurrent_access 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_backend_traces.py::TestNormalizedTraceStorage::test_concurrent_access 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendDeleteState::test_delete_state_removes_workflow_from_database 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendDeleteState::test_delete_state_removes_workflow_from_database 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendDeleteState::test_delete_state_nonexistent_workflow 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendDeleteState::test_delete_state_nonexistent_workflow 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendDeleteState::test_delete_state_multiple_workflows 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendDeleteState::test_delete_state_multiple_workflows 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_special_characters_in_filename 
[gw2] [ 77%] PASSED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_deeply_nested_model_roundtrip 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_mixed_data_roundtrip 
[gw0] [ 77%] FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_special_characters_in_filename 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_long_filename 
[gw0] [ 77%] FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_long_filename 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_no_write_permissions 
[gw2] [ 77%] PASSED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_mixed_data_roundtrip 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_complex_model_field_preservation 
[gw0] [ 77%] FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_no_write_permissions 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_disk_full_error 
[gw3] [ 77%] FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_lens_list_with_various_filters 
tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_lens_show_with_various_run_ids 
[gw0] [ 77%] FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_disk_full_error 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendPerformanceThresholds::test_performance_threshold_environment_variable 
[gw2] [ 77%] PASSED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_complex_model_field_preservation 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_edge_cases_roundtrip 
[gw2] [ 77%] PASSED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_edge_cases_roundtrip 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_large_data_roundtrip 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendPerformanceThresholds::test_performance_threshold_environment_variable 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendPerformanceThresholds::test_performance_threshold_default_behavior 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendPerformanceThresholds::test_performance_threshold_default_behavior 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendLoggerContextManagement::test_logger_context_manager_cleanup 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendLoggerContextManagement::test_logger_context_manager_cleanup 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendLoggerContextManagement::test_logger_context_manager_exception_handling 
[gw0] [ 77%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendLoggerContextManagement::test_logger_context_manager_exception_handling 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendTypeSafety::test_retry_mechanism_type_safety 
[gw0] [ 78%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendTypeSafety::test_retry_mechanism_type_safety 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendTypeSafety::test_serialization_type_safety 
[gw0] [ 78%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendTypeSafety::test_serialization_type_safety 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendConcurrencyEdgeCases::test_concurrent_delete_and_save 
[gw0] [ 78%] PASSED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendConcurrencyEdgeCases::test_concurrent_delete_and_save 
tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendConcurrencyEdgeCases::test_concurrent_backup_operations 
[gw2] [ 78%] PASSED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_large_data_roundtrip 
tests/unit/test_serialization_properties.py::TestSerializationProperties::test_special_characters_roundtrip 
[gw2] [ 78%] PASSED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_special_characters_roundtrip 
tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_register_custom_serializer 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_register_custom_serializer 
tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_register_custom_deserializer 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_register_custom_deserializer 
tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_lookup_custom_serializer_not_found 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_lookup_custom_serializer_not_found 
tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_lookup_custom_deserializer_not_found 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_lookup_custom_deserializer_not_found 
tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_reset_custom_serializer_registry 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSerializationRegistry::test_reset_custom_serializer_registry 
tests/unit/test_serialization_utilities.py::TestSerializerFactories::test_create_serializer_for_type 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSerializerFactories::test_create_serializer_for_type 
tests/unit/test_serialization_utilities.py::TestSerializerFactories::test_create_field_serializer 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSerializerFactories::test_create_field_serializer 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_basic_types 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_basic_types 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_datetime_objects 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_datetime_objects 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_enum 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_enum 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_pydantic_model 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_pydantic_model 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_list 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_list 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_dict 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_dict 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_set 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_set 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_custom_object 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_custom_object 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_with_custom_serializer 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_with_custom_serializer 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_circular_reference 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_circular_reference 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_with_default_serializer 
[gw2] [ 78%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_with_default_serializer 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_nested_structures 
[gw2] [ 79%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_nested_structures 
tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_deep_circular_reference 
[gw2] [ 79%] PASSED tests/unit/test_serialization_utilities.py::TestSafeSerialize::test_safe_serialize_deep_circular_reference 
tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_basic_types 
[gw2] [ 79%] PASSED tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_basic_types 
tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_datetime_strings 
[gw2] [ 79%] PASSED tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_datetime_strings 
tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_enum 
[gw2] [ 79%] PASSED tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_enum 
tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_pydantic_model 
[gw2] [ 79%] PASSED tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_pydantic_model 
tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_with_custom_deserializer 
[gw2] [ 79%] PASSED tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_with_custom_deserializer 
tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_with_default_deserializer 
[gw2] [ 79%] PASSED tests/unit/test_serialization_utilities.py::TestSafeDeserialize::test_safe_deserialize_with_default_deserializer 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_parse_cache_key_edge_cases 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_parse_cache_key_edge_cases 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_legacy_cache_key_handling 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_legacy_cache_key_handling 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_with_pipe_in_run_id 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_with_pipe_in_run_id 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_with_multiple_pipes_in_run_id 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_with_multiple_pipes_in_run_id 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_with_special_characters 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_with_special_characters 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_performance 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_performance 
tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_fallback_serialization_includes_all_essential_fields 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_fallback_serialization_includes_all_essential_fields 
tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_fallback_serialization_with_missing_fields 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_fallback_serialization_with_missing_fields 
tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_error_fallback_serialization 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_error_fallback_serialization 
tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_fallback_serialization_preserves_data_integrity 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_fallback_serialization_preserves_data_integrity 
[gw0] [ 79%] FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendConcurrencyEdgeCases::test_concurrent_backup_operations 
tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_fallback_serialization_performance 
tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_initialization_events 
[gw0] [ 79%] PASSED tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_initialization_events 
tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_save_operations 
[gw0] [ 79%] PASSED tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_save_operations 
[gw2] [ 79%] PASSED tests/unit/test_state_manager_fallback_serialization.py::TestStateManagerFallbackSerialization::test_fallback_serialization_performance 
tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_error_conditions 
tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_text_streaming_agent_protocol 
[gw1] [ 80%] PASSED tests/benchmarks/test_performance_optimizations.py::TestPerfCounterPrecision::test_perf_counter_ns_precision 
tests/benchmarks/test_performance_optimizations.py::TestSerializationPerformance::test_json_serialization 
[gw2] [ 80%] FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_text_streaming_agent_protocol 
tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_binary_streaming_agent_protocol 
[gw2] [ 80%] FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_binary_streaming_agent_protocol 
tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_legacy_streaming_agent_fallback 
[gw2] [ 80%] FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_legacy_streaming_agent_fallback 
tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_legacy_binary_streaming_agent_fallback 
[gw2] [ 80%] FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_legacy_binary_streaming_agent_fallback 
tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_mixed_stream_types_handled_gracefully 
[gw2] [ 80%] FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_mixed_stream_types_handled_gracefully 
tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_empty_stream_handled_correctly 
[gw2] [ 80%] FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_empty_stream_handled_correctly 
tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_single_bytes_chunk 
[gw2] [ 80%] FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_single_bytes_chunk 
tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_large_bytes_stream 
[gw2] [ 80%] FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_large_bytes_stream 
tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_protocol_type_safety 
[gw2] [ 80%] PASSED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_protocol_type_safety 
tests/unit/test_streaming_protocol.py::test_streaming_protocol_import 
[gw2] [ 80%] PASSED tests/unit/test_streaming_protocol.py::test_streaming_protocol_import 
tests/unit/test_streaming_protocol.py::test_streaming_protocol_implementation 
[gw2] [ 80%] PASSED tests/unit/test_streaming_protocol.py::test_streaming_protocol_implementation 
tests/unit/test_streaming_protocol.py::test_streaming_agent_functionality 
[gw2] [ 80%] PASSED tests/unit/test_streaming_protocol.py::test_streaming_agent_functionality 
tests/unit/test_streaming_protocol.py::test_streaming_agent_with_kwargs 
[gw2] [ 80%] PASSED tests/unit/test_streaming_protocol.py::test_streaming_agent_with_kwargs 
tests/unit/test_streaming_protocol.py::test_streaming_agent_default_kwargs 
[gw2] [ 80%] PASSED tests/unit/test_streaming_protocol.py::test_streaming_agent_default_kwargs 
tests/unit/test_telemetry.py::test_init_telemetry[False-None] 
[gw2] [ 80%] PASSED tests/unit/test_telemetry.py::test_init_telemetry[False-None] 
tests/unit/test_telemetry.py::test_init_telemetry[True-None] 
[gw2] [ 80%] PASSED tests/unit/test_telemetry.py::test_init_telemetry[True-None] 
tests/unit/test_telemetry.py::test_init_telemetry[True-https://otlp.example.com] 
[gw2] [ 80%] PASSED tests/unit/test_telemetry.py::test_init_telemetry[True-https://otlp.example.com] 
tests/unit/test_telemetry.py::test_init_telemetry_telemetry_disabled 
[gw2] [ 80%] PASSED tests/unit/test_telemetry.py::test_init_telemetry_telemetry_disabled 
tests/unit/test_telemetry.py::test_init_telemetry_otlp_with_endpoint 
[gw2] [ 80%] PASSED tests/unit/test_telemetry.py::test_init_telemetry_otlp_with_endpoint 
tests/unit/test_telemetry.py::test_init_telemetry_otlp_no_endpoint 
[gw2] [ 80%] PASSED tests/unit/test_telemetry.py::test_init_telemetry_otlp_no_endpoint 
tests/unit/test_telemetry.py::test_telemetry_initialization 
[gw2] [ 80%] PASSED tests/unit/test_telemetry.py::test_telemetry_initialization 
tests/unit/test_telemetry.py::test_telemetry_export 
[gw2] [ 81%] PASSED tests/unit/test_telemetry.py::test_telemetry_export 
tests/unit/test_telemetry.py::test_telemetry_export_disabled 
[gw2] [ 81%] PASSED tests/unit/test_telemetry.py::test_telemetry_export_disabled 
tests/unit/test_telemetry.py::test_telemetry_export_error 
[gw2] [ 81%] PASSED tests/unit/test_telemetry.py::test_telemetry_export_error 
tests/unit/test_telemetry.py::test_telemetry_export_with_span_tree 
[gw2] [ 81%] PASSED tests/unit/test_telemetry.py::test_telemetry_export_with_span_tree 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_handles_closed_file_error 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_handles_closed_file_error 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_handles_other_closed_errors 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_handles_other_closed_errors 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_raises_unexpected_errors 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_raises_unexpected_errors 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_with_real_logger_and_multiple_handlers 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_with_real_logger_and_multiple_handlers 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_with_binary_stream_handler 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_log_with_binary_stream_handler 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_mock_logfire_handles_closed_file_error 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_mock_logfire_handles_closed_file_error 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_logfire_wrapper_handles_closed_file_error 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_logfire_wrapper_handles_closed_file_error 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_logfire_wrapper_raises_unexpected_errors 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_safe_logfire_wrapper_raises_unexpected_errors 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_telemetry_during_cancellation 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_telemetry_during_cancellation 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_telemetry_initialization_robustness 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_telemetry_initialization_robustness 
tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_logging_handler_cleanup 
[gw2] [ 81%] PASSED tests/unit/test_telemetry_robustness.py::TestTelemetryRobustness::test_logging_handler_cleanup 
tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_within_schedule 
[gw2] [ 81%] PASSED tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_within_schedule 
tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_beyond_schedule 
[gw2] [ 81%] PASSED tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_beyond_schedule 
tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_single_value_schedule 
[gw2] [ 81%] PASSED tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_single_value_schedule 
tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_empty_schedule 
[gw2] [ 81%] PASSED tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_empty_schedule 
tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_negative_round 
[gw2] [ 81%] PASSED tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_negative_round 
tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_float_temperatures 
[gw2] [ 81%] PASSED tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_float_temperatures 
tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_integer_temperatures 
[gw2] [ 81%] PASSED tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_integer_temperatures 
tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_mixed_temperatures 
[gw2] [ 82%] PASSED tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_mixed_temperatures 
tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_large_schedule 
[gw2] [ 82%] PASSED tests/unit/test_temperature.py::TestTempForRound::test_temp_for_round_large_schedule 
tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_basic 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_basic 
tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_call_count 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_call_count 
tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_exhaustion 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_exhaustion 
tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_empty_outputs 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_empty_outputs 
tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_single_output 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_single_output 
tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_multiple_outputs 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_multiple_outputs 
tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_with_context 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_with_context 
tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_input_ignored 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestStubAgent::test_stub_agent_input_ignored 
tests/unit/test_testing_utils.py::TestOverrideAgent::test_override_agent_basic 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestOverrideAgent::test_override_agent_basic 
tests/unit/test_testing_utils.py::TestOverrideAgent::test_override_agent_exception_handling 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestOverrideAgent::test_override_agent_exception_handling 
tests/unit/test_testing_utils.py::TestOverrideAgent::test_override_agent_nested 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestOverrideAgent::test_override_agent_nested 
tests/unit/test_testing_utils.py::TestOverrideAgent::test_override_agent_multiple_calls 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestOverrideAgent::test_override_agent_multiple_calls 
tests/unit/test_testing_utils.py::TestGatherResult::test_gather_result_basic 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestGatherResult::test_gather_result_basic 
tests/unit/test_testing_utils.py::TestGatherResult::test_gather_result_with_context 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestGatherResult::test_gather_result_with_context 
tests/unit/test_testing_utils.py::TestGatherResult::test_gather_result_with_resources 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestGatherResult::test_gather_result_with_resources 
tests/unit/test_testing_utils.py::TestAssertPipelineResult::test_assert_pipeline_result_success 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestAssertPipelineResult::test_assert_pipeline_result_success 
tests/unit/test_testing_utils.py::TestAssertPipelineResult::test_assert_pipeline_result_failure 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestAssertPipelineResult::test_assert_pipeline_result_failure 
tests/unit/test_testing_utils.py::TestAssertPipelineResult::test_assert_pipeline_result_wrong_output 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestAssertPipelineResult::test_assert_pipeline_result_wrong_output 
tests/unit/test_testing_utils.py::TestAssertPipelineResult::test_assert_pipeline_result_no_output_check 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestAssertPipelineResult::test_assert_pipeline_result_no_output_check 
tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_failure 
[gw2] [ 82%] PASSED tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_failure 
tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_success 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_success 
tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_return_value 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_return_value 
tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_none_return 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_none_return 
tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_test_validator_failed_sync_with_complex_data 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_test_validator_failed_sync_with_complex_data 
tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_async_validator 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestAssertValidatorFailed::test_assert_validator_failed_with_async_validator 
tests/unit/test_testing_utils.py::TestEdgeCases::test_stub_agent_with_none_outputs 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestEdgeCases::test_stub_agent_with_none_outputs 
tests/unit/test_testing_utils.py::TestEdgeCases::test_stub_agent_with_non_list_outputs 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestEdgeCases::test_stub_agent_with_non_list_outputs 
tests/unit/test_testing_utils.py::TestEdgeCases::test_override_agent_with_none_agent 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestEdgeCases::test_override_agent_with_none_agent 
tests/unit/test_testing_utils.py::TestEdgeCases::test_override_agent_with_same_agent 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestEdgeCases::test_override_agent_with_same_agent 
tests/unit/test_testing_utils.py::TestEdgeCases::test_override_agent_context_manager_protocol 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestEdgeCases::test_override_agent_context_manager_protocol 
tests/unit/test_testing_utils.py::TestEdgeCases::test_assert_validator_failed_with_async_validator 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestEdgeCases::test_assert_validator_failed_with_async_validator 
tests/unit/test_testing_utils.py::TestIntegration::test_stub_agent_with_override_agent 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestIntegration::test_stub_agent_with_override_agent 
tests/unit/test_testing_utils.py::TestIntegration::test_multiple_stub_agents_with_override 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::TestIntegration::test_multiple_stub_agents_with_override 
tests/unit/test_testing_utils.py::test_assert_validator_failed_with_stub_agent_sync 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::test_assert_validator_failed_with_stub_agent_sync 
tests/unit/test_testing_utils.py::test_assert_validator_failed_with_stub_agent_async 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils.py::test_assert_validator_failed_with_stub_agent_async 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_initialization 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_initialization 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_execute_step 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_execute_step 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_execute_step_with_context 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_execute_step_with_context 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_execute_step_with_resources 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_execute_step_with_resources 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_execute_step_agent_without_run 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_execute_step_agent_without_run 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_store_and_retrieve 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_store_and_retrieve 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_clear 
[gw2] [ 83%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_clear 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_get_call_count 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_get_call_count 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_get_storage_keys 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_get_storage_keys 
tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_get_storage_size 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_simple_dummy_remote_backend_get_storage_size 
tests/unit/test_testing_utils_extended.py::test_override_agent_context_manager 
[gw0] [ 84%] PASSED tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_error_conditions 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_override_agent_context_manager 
tests/unit/test_testing_utils_extended.py::test_override_agent_direct_context_manager 
tests/unit/test_sqlite_observability.py::test_sqlite_backend_metrics_correctness 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_override_agent_direct_context_manager 
tests/unit/test_testing_utils_extended.py::test_override_agent_direct_with_none_agents 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_override_agent_direct_with_none_agents 
tests/unit/test_testing_utils_extended.py::test_override_agent_direct_with_same_agent 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_override_agent_direct_with_same_agent 
tests/unit/test_testing_utils_extended.py::test_dummy_remote_backend_initialization 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_dummy_remote_backend_initialization 
tests/unit/test_testing_utils_extended.py::test_dummy_remote_backend_execute_step 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_dummy_remote_backend_execute_step 
tests/unit/test_testing_utils_extended.py::test_dummy_remote_backend_execute_step_with_agent_registry 
[gw2] [ 84%] PASSED tests/unit/test_testing_utils_extended.py::test_dummy_remote_backend_execute_step_with_agent_registry 
tests/unit/test_trace_integration.py::test_trace_saving_integration 
[gw0] [ 84%] PASSED tests/unit/test_sqlite_observability.py::test_sqlite_backend_metrics_correctness 
tests/unit/test_sqlite_observability.py::test_sqlite_backend_error_reporting_detail 
[gw0] [ 84%] PASSED tests/unit/test_sqlite_observability.py::test_sqlite_backend_error_reporting_detail 
tests/unit/test_sqlite_observability.py::test_sqlite_backend_performance_metrics 
[gw2] [ 84%] PASSED tests/unit/test_trace_integration.py::test_trace_saving_integration 
tests/unit/test_trace_integration.py::test_trace_saving_without_trace_tree 
[gw2] [ 84%] PASSED tests/unit/test_trace_integration.py::test_trace_saving_without_trace_tree 
tests/unit/test_trace_integration.py::test_trace_saving_error_handling 
[gw0] [ 84%] PASSED tests/unit/test_sqlite_observability.py::test_sqlite_backend_performance_metrics 
tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_cleanup_operations 
[gw2] [ 84%] PASSED tests/unit/test_trace_integration.py::test_trace_saving_error_handling 
tests/unit/test_tracing.py::test_console_tracer_hook_prints 
[gw2] [ 84%] PASSED tests/unit/test_tracing.py::test_console_tracer_hook_prints 
tests/unit/test_tracing.py::test_console_tracer_config 
[gw2] [ 84%] PASSED tests/unit/test_tracing.py::test_console_tracer_config 
tests/unit/test_tracing_manager.py::TestTraceManager::test_trace_manager_initialization 
[gw2] [ 84%] PASSED tests/unit/test_tracing_manager.py::TestTraceManager::test_trace_manager_initialization 
tests/unit/test_tracing_manager.py::TestTraceManager::test_trace_manager_builds_nested_tree 
[gw0] [ 84%] PASSED tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_cleanup_operations 
[gw2] [ 84%] PASSED tests/unit/test_tracing_manager.py::TestTraceManager::test_trace_manager_builds_nested_tree 
tests/unit/test_sqlite_observability.py::test_sqlite_backend_error_context_preservation 
tests/unit/test_tracing_manager.py::TestTraceManager::test_trace_manager_integration 
[gw2] [ 85%] PASSED tests/unit/test_tracing_manager.py::TestTraceManager::test_trace_manager_integration 
tests/unit/test_tracing_manager.py::TestTraceManager::test_step_failure_marks_span_as_failed 
[gw2] [ 85%] PASSED tests/unit/test_tracing_manager.py::TestTraceManager::test_step_failure_marks_span_as_failed 
tests/unit/test_tracing_manager.py::TestTraceManager::test_nested_spans_are_correctly_structured 
[gw2] [ 85%] PASSED tests/unit/test_tracing_manager.py::TestTraceManager::test_nested_spans_are_correctly_structured 
tests/unit/test_tracing_manager.py::TestTraceManager::test_span_dataclass_attributes 
[gw2] [ 85%] PASSED tests/unit/test_tracing_manager.py::TestTraceManager::test_span_dataclass_attributes 
tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_set_get 
[gw2] [ 85%] PASSED tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_set_get 
tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_eviction 
[gw2] [ 85%] PASSED tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_eviction 
tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_ttl 
[gw2] [ 85%] PASSED tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_ttl 
[gw0] [ 85%] PASSED tests/unit/test_sqlite_observability.py::test_sqlite_backend_error_context_preservation 
tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_ttl_never_expire 
[gw2] [ 85%] PASSED tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_ttl_never_expire 
tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_concurrent_access 
tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_ttl_with_expiration 
[gw0] [ 85%] PASSED tests/unit/test_sqlite_observability.py::test_sqlite_backend_logs_concurrent_access 
tests/unit/test_sqlite_trace_persistence.py::test_save_and_get_trace_roundtrip 
[gw0] [ 85%] PASSED tests/unit/test_sqlite_trace_persistence.py::test_save_and_get_trace_roundtrip 
tests/unit/test_sqlite_trace_persistence.py::test_get_trace_nonexistent_run 
[gw0] [ 85%] PASSED tests/unit/test_sqlite_trace_persistence.py::test_get_trace_nonexistent_run 
tests/unit/test_sqlite_trace_persistence.py::test_save_trace_overwrites_existing 
[gw0] [ 85%] PASSED tests/unit/test_sqlite_trace_persistence.py::test_save_trace_overwrites_existing 
tests/unit/test_sqlite_trace_persistence.py::test_save_trace_complex_nested_structure 
[gw0] [ 85%] PASSED tests/unit/test_sqlite_trace_persistence.py::test_save_trace_complex_nested_structure 
tests/unit/test_sqlite_trace_persistence.py::test_save_trace_with_special_json_types 
[gw0] [ 85%] PASSED tests/unit/test_sqlite_trace_persistence.py::test_save_trace_with_special_json_types 
tests/unit/test_sqlite_trace_persistence.py::test_trace_persistence_with_run_deletion 
[gw0] [ 85%] PASSED tests/unit/test_sqlite_trace_persistence.py::test_trace_persistence_with_run_deletion 
tests/unit/test_sqlite_trace_persistence.py::test_concurrent_trace_operations 
[gw0] [ 85%] PASSED tests/unit/test_sqlite_trace_persistence.py::test_concurrent_trace_operations 
tests/unit/test_state_backend.py::test_inmemory_backend_roundtrip 
[gw0] [ 85%] PASSED tests/unit/test_state_backend.py::test_inmemory_backend_roundtrip 
tests/unit/test_state_backend.py::test_inmemory_backend_handles_special_types 
[gw0] [ 85%] PASSED tests/unit/test_state_backend.py::test_inmemory_backend_handles_special_types 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_create_cache_key_with_underscores_in_run_id 
[gw0] [ 85%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_create_cache_key_with_underscores_in_run_id 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_parse_cache_key_with_underscores_in_run_id 
[gw0] [ 85%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_parse_cache_key_with_underscores_in_run_id 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_roundtrip_with_complex_run_id 
[gw0] [ 85%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_cache_key_roundtrip_with_complex_run_id 
tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_parse_cache_key_invalid_format 
[gw0] [ 85%] PASSED tests/unit/test_state_manager_cache_key_parsing.py::TestStateManagerCacheKeyParsing::test_parse_cache_key_invalid_format 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_failed_validation 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_failed_validation 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_usage_limits 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_usage_limits 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_streaming_execution 
[gw0] [ 86%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_streaming_execution 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_context_and_resources_passing 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_context_and_resources_passing 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_temperature_passing 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_temperature_passing 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_cache_key_generation 
[gw0] [ 86%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_cache_key_generation 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_hash_obj 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_hash_obj 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_hash_obj_bytes_fix 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_hash_obj_bytes_fix 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_stable_cache_keys 
[gw2] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_ttl_with_expiration 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_stable_cache_keys 
tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_monotonic_time 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_caching_functionality 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_caching_functionality 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_caching_with_context 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_caching_with_context 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_caching_disabled 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_caching_disabled 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_cache_key_stability 
[gw0] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_cache_key_stability 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_agent_identification_stability 
[gw0] [ 86%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_agent_identification_stability 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_concurrency_limiting 
[gw2] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_monotonic_time 
tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_lru_promotion 
[gw2] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestLRUCache::test_cache_lru_promotion 
tests/unit/test_ultra_executor.py::TestUsageTracker::test_usage_tracking 
[gw2] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUsageTracker::test_usage_tracking 
tests/unit/test_ultra_executor.py::TestUsageTracker::test_concurrent_usage_tracking 
[gw2] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUsageTracker::test_concurrent_usage_tracking 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_simple_step_execution 
[gw2] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_simple_step_execution 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_cached_step_execution 
[gw2] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_cached_step_execution 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_retries 
[gw2] [ 86%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_retries 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_validation 
[gw2] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_validation 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_mock_detection_safety 
[gw2] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_mock_detection_safety 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_mock_detection_blocks_mocks 
[gw2] [ 87%] SKIPPED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_mock_detection_blocks_mocks 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_integration_works 
[gw2] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_integration_works 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_disabled_works 
[gw2] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_disabled_works 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_stability 
[gw0] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_concurrency_limiting 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_cache_property 
[gw0] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_cache_property 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_clear_cache 
[gw0] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_clear_cache 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_plugins_validators_fallbacks 
[gw2] [ 87%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_stability 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_bytes_hashing_correct 
[gw2] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_bytes_hashing_correct 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_with_complex_steps 
[gw2] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_with_complex_steps 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_with_resources 
[gw0] [ 87%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_plugins_validators_fallbacks 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_validators 
[gw0] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_validators 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_fallback 
[gw2] [ 87%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_with_resources 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_includes_all_components 
[gw0] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_fallback 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_agent_with_kwargs_signature 
[gw0] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_agent_with_kwargs_signature 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_module_level_dataclasses_used 
[gw0] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_module_level_dataclasses_used 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_agent_identification_includes_module 
[gw2] [ 87%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_includes_all_components 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_persistence_across_executor_instances 
[gw2] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_persistence_across_executor_instances 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_handles_edge_cases 
[gw2] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_handles_edge_cases 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_metadata_correct 
[gw2] [ 87%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_metadata_correct 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_performance_with_module_dataclasses 
[gw0] [ 87%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_agent_identification_includes_module 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_consistent_agent_config_hashing 
[gw2] [ 88%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_performance_with_module_dataclasses 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_agent_identification_handles_edge_cases 
[gw0] [ 88%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_consistent_agent_config_hashing 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_stability_across_python_runs 
[gw0] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_stability_across_python_runs 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_copy_behavior 
[gw0] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_copy_behavior 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_always_defined 
[gw0] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_always_defined 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_critical_exceptions_not_cached 
[gw0] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_critical_exceptions_not_cached 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_unified_error_handling_contract 
[gw0] [ 88%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_unified_error_handling_contract 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_critical_exceptions_are_re_raised 
[gw0] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_critical_exceptions_are_re_raised 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_timing_preservation_for_failed_steps 
[gw2] [ 88%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_agent_identification_handles_edge_cases 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_mutation_does_not_corrupt_cached_data 
[gw2] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_mutation_does_not_corrupt_cached_data 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_redundant_retry_logic_removal 
[gw2] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_redundant_retry_logic_removal 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_input_validation_ultra_executor 
[gw2] [ 88%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_input_validation_ultra_executor 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_input_validation_lru_cache 
[gw2] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_input_validation_lru_cache 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_ttl_logic_fix 
[gw1] [ 88%] PASSED tests/benchmarks/test_performance_optimizations.py::TestSerializationPerformance::test_json_serialization 
tests/benchmarks/test_performance_optimizations.py::TestSerializationPerformance::test_orjson_serialization 
[gw2] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_ttl_logic_fix 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_monotonic_time_usage 
[gw2] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_monotonic_time_usage 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_independent_latency_measurement 
[gw2] [ 88%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_independent_latency_measurement 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_no_duplicate_trace_functions 
[gw2] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_no_duplicate_trace_functions 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_no_undefined_imports 
[gw2] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_no_undefined_imports 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_constructor_validation_preserved 
[gw0] [ 88%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_timing_preservation_for_failed_steps 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_retry_latency_measurement 
[gw2] [ 88%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_constructor_validation_preserved 
tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_cumulative_tracking 
[gw2] [ 88%] FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_cumulative_tracking 
tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_limit_checking 
[gw2] [ 89%] FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_limit_checking 
tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_thread_safety 
[gw2] [ 89%] FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_thread_safety 
tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_legacy_guard_method_compatibility 
[gw2] [ 89%] FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_legacy_guard_method_compatibility 
tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_multiple_limit_checks 
[gw2] [ 89%] FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_multiple_limit_checks 
tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_zero_limits 
[gw2] [ 89%] FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_zero_limits 
tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_precision_handling 
[gw2] [ 89%] FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_precision_handling 
tests/unit/test_utils_properties.py::test_property_summary_length 
[gw2] [ 89%] PASSED tests/unit/test_utils_properties.py::test_property_summary_length 
tests/unit/test_utils_properties.py::test_property_redaction_hides_secrets 
[gw2] [ 89%] PASSED tests/unit/test_utils_properties.py::test_property_redaction_hides_secrets 
tests/unit/test_validation.py::test_base_validator_initialization 
[gw2] [ 89%] PASSED tests/unit/test_validation.py::test_base_validator_initialization 
tests/unit/test_validation.py::test_base_validator_validate_valid 
[gw2] [ 89%] PASSED tests/unit/test_validation.py::test_base_validator_validate_valid 
tests/unit/test_validation.py::test_base_validator_validate_invalid 
[gw2] [ 89%] PASSED tests/unit/test_validation.py::test_base_validator_validate_invalid 
tests/unit/test_validation.py::test_base_validator_validate_with_context 
[gw2] [ 89%] PASSED tests/unit/test_validation.py::test_base_validator_validate_with_context 
tests/unit/test_validation.py::test_validator_decorator_valid 
[gw2] [ 89%] PASSED tests/unit/test_validation.py::test_validator_decorator_valid 
tests/unit/test_validation.py::test_validator_decorator_invalid 
[gw2] [ 89%] PASSED tests/unit/test_validation.py::test_validator_decorator_invalid 
tests/unit/test_validation.py::test_validator_decorator_exception 
[gw2] [ 89%] PASSED tests/unit/test_validation.py::test_validator_decorator_exception 
tests/unit/test_validation.py::test_validator_decorator_with_context 
[gw2] [ 89%] PASSED tests/unit/test_validation.py::test_validator_decorator_with_context 
tests/unit/test_validation.py::test_validator_decorator_creates_functional_validator 
[gw2] [ 89%] PASSED tests/unit/test_validation.py::test_validator_decorator_creates_functional_validator 
[gw0] [ 89%] FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_retry_latency_measurement 
tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_dead_code_removal 
[gw0] [ 89%] PASSED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_dead_code_removal 
[gw1] [ 89%] PASSED tests/benchmarks/test_performance_optimizations.py::TestSerializationPerformance::test_orjson_serialization 
tests/benchmarks/test_performance_optimizations.py::TestHashingPerformance::test_hashlib_hashing 
[gw1] [ 89%] PASSED tests/benchmarks/test_performance_optimizations.py::TestHashingPerformance::test_hashlib_hashing 
tests/benchmarks/test_performance_optimizations.py::TestHashingPerformance::test_blake3_hashing 
[gw1] [ 89%] PASSED tests/benchmarks/test_performance_optimizations.py::TestHashingPerformance::test_blake3_hashing 
tests/benchmarks/test_performance_optimizations.py::TestAsyncPerformance::test_async_performance_with_uvloop 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestAsyncPerformance::test_async_performance_with_uvloop 
tests/benchmarks/test_performance_optimizations.py::TestMeasureTimeDecorators::test_measure_time_decorator 
[gw3] [ 90%] PASSED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_lens_show_with_various_run_ids 
tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_cli_performance_with_concurrent_access 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestMeasureTimeDecorators::test_measure_time_decorator 
tests/benchmarks/test_performance_optimizations.py::TestMeasureTimeDecorators::test_measure_time_async_decorator 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestMeasureTimeDecorators::test_measure_time_async_decorator 
tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_scratch_buffer_reuse 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_scratch_buffer_reuse 
tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_pooling_functionality 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_pooling_functionality 
tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_pooling_concurrent_access 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_pooling_concurrent_access 
tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_pooling_memory_efficiency 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_pooling_memory_efficiency 
tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_pooling_edge_cases 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_pooling_edge_cases 
tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_reuse_without_reuse 
[gw3] [ 90%] FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_cli_performance_with_concurrent_access 
tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_cli_performance_with_nonexistent_data 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_reuse_without_reuse 
tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_reuse_with_reuse 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestBufferReuse::test_buffer_reuse_with_reuse 
tests/benchmarks/test_performance_optimizations.py::TestEndToEndPerformance::test_end_to_end_performance 
[gw1] [ 90%] PASSED tests/benchmarks/test_performance_optimizations.py::TestEndToEndPerformance::test_end_to_end_performance 
tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_small_model_performance 
[gw1] [ 90%] PASSED tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_small_model_performance 
tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_medium_model_performance 
[gw1] [ 90%] PASSED tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_medium_model_performance 
tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_large_model_performance 
[gw3] [ 90%] PASSED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_cli_performance_with_nonexistent_data 
tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_index_optimization 
[gw1] [ 90%] PASSED tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_large_model_performance 
tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_nested_model_performance 
[gw1] [ 90%] PASSED tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_nested_model_performance 
tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_memory_usage 
[gw3] [ 90%] FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_index_optimization 
tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_concurrent_writes 
[gw1] [ 90%] PASSED tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_memory_usage 
tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_concurrent_serialization 
[gw1] [ 90%] PASSED tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_concurrent_serialization 
tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_serialization_throughput 
[gw3] [ 90%] PASSED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_concurrent_writes 
tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_memory_usage 
[gw3] [ 91%] FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_memory_usage 
tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_corruption_recovery 
[gw3] [ 91%] PASSED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_corruption_recovery 
tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_fails_with_unwritable_path_unix 
[gw1] [ 91%] PASSED tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_serialization_throughput 
tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_memory_efficiency 
[gw3] [ 91%] PASSED tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_fails_with_unwritable_path_unix 
tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_skipped_on_windows 
[gw3] [ 91%] SKIPPED tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_skipped_on_windows 
tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_skipped_as_root 
[gw3] [ 91%] PASSED tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_skipped_as_root 
tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_with_malformed_environment_variable 
[gw3] [ 91%] PASSED tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_with_malformed_environment_variable 
tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_with_missing_environment_variable 
[gw1] [ 91%] PASSED tests/benchmarks/test_serialization_performance.py::TestSerializationPerformance::test_memory_efficiency 
tests/benchmarks/test_tracing_performance.py::TestTracingPerformance::test_tracing_memory_overhead 
[gw1] [ 91%] PASSED tests/benchmarks/test_tracing_performance.py::TestTracingPerformance::test_tracing_memory_overhead 
tests/benchmarks/test_tracing_performance.py::TestTracingPerformance::test_trace_tree_size_limits 
[gw1] [ 91%] PASSED tests/benchmarks/test_tracing_performance.py::TestTracingPerformance::test_trace_tree_size_limits 
tests/benchmarks/test_tracing_performance.py::TestTracingPerformance::test_tracing_performance_regression 
[gw3] [ 91%] PASSED tests/unit/test_cli_performance_edge_cases.py::TestCLIErrorHandling::test_cli_with_missing_environment_variable 
tests/unit/test_conditional_step.py::test_conditional_step_init_validation 
[gw3] [ 91%] PASSED tests/unit/test_conditional_step.py::test_conditional_step_init_validation 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_only_cost 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_only_cost 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_cost_and_tokens 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_cost_and_tokens 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_zero_cost 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_zero_cost 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_negative_cost 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_negative_cost 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_none_cost 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_none_cost 
tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_none_tokens 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestExtractUsageMetrics::test_explicit_cost_reporter_protocol_with_none_tokens 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_calculate_cost_with_pricing 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_calculate_cost_with_pricing 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_calculate_cost_no_pricing 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_calculate_cost_no_pricing 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_calculate_cost_with_provider_inference 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_calculate_cost_with_provider_inference 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_calculate_cost_unknown_provider_returns_zero 
[gw3] [ 91%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_calculate_cost_unknown_provider_returns_zero 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_infer_provider_from_model 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_infer_provider_from_model 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_infer_provider_from_model_edge_cases 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_infer_provider_from_model_edge_cases 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_cost_calculation_edge_cases 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_cost_calculation_edge_cases 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_provider_inference_comprehensive 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_provider_inference_comprehensive 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_cost_calculation_precision 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_cost_calculation_precision 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_ci_environment_fallback_with_existing_config 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_ci_environment_fallback_with_existing_config 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_ci_environment_fallback_for_failing_models 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_ci_environment_fallback_for_failing_models 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_ci_environment_fallback_with_strict_pricing 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_ci_environment_fallback_with_strict_pricing 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_cost_calculation_for_embedding_models 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_cost_calculation_for_embedding_models 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_error_handling_robustness 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_error_handling_robustness 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_integration_with_real_pricing_config 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_integration_with_real_pricing_config 
tests/unit/test_cost_tracking.py::TestCostCalculator::test_backward_compatibility 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostCalculator::test_backward_compatibility 
tests/unit/test_cost_tracking.py::TestProviderPricing::test_provider_pricing_creation 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestProviderPricing::test_provider_pricing_creation 
tests/unit/test_cost_tracking.py::TestProviderPricing::test_provider_pricing_validation 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestProviderPricing::test_provider_pricing_validation 
tests/unit/test_cost_tracking.py::TestCostConfig::test_get_provider_pricing_with_valid_data 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostConfig::test_get_provider_pricing_with_valid_data 
tests/unit/test_cost_tracking.py::TestCostConfig::test_get_provider_pricing_with_default_pricing 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestCostConfig::test_get_provider_pricing_with_default_pricing 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_off_default_behavior 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_off_default_behavior 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_off_with_default_pricing 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_off_with_default_pricing 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_with_user_config 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_with_user_config 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_without_user_config_raises_error 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_without_user_config_raises_error 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_with_unknown_model_raises_error 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_with_unknown_model_raises_error 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_with_unknown_provider_raises_error 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_with_unknown_provider_raises_error 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_with_none_provider_raises_error 
[gw3] [ 92%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_on_with_none_provider_raises_error 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_does_not_fallback_to_hardcoded_defaults 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_strict_mode_does_not_fallback_to_hardcoded_defaults 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_cost_config_strict_field_default 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_cost_config_strict_field_default 
tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_cost_config_strict_field_explicit 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestStrictPricingMode::test_cost_config_strict_field_explicit 
tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_1_silent_failure_fixed 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_1_silent_failure_fixed 
tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_2_model_id_extraction_improved 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_2_model_id_extraction_improved 
tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_3_redundant_metric_calculation_removed 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_3_redundant_metric_calculation_removed 
tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_4_code_duplication_addressed 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_4_code_duplication_addressed 
tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_5_hardcoded_prices_warning 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestBugFixes::test_bug_5_hardcoded_prices_warning 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_valid_pricing 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_valid_pricing 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_hd_quality 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_hd_quality 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_missing_pricing 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_missing_pricing 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_no_images 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_no_images 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_no_usage_details 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_no_usage_details 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_no_usage 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_no_usage 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_different_sizes 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessor::test_image_cost_post_processor_with_different_sizes 
tests/unit/test_cost_tracking.py::TestImageModelDetection::test_is_image_generation_model_with_dall_e 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageModelDetection::test_is_image_generation_model_with_dall_e 
tests/unit/test_cost_tracking.py::TestImageModelDetection::test_is_image_generation_model_with_other_image_models 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageModelDetection::test_is_image_generation_model_with_other_image_models 
tests/unit/test_cost_tracking.py::TestImageModelDetection::test_is_image_generation_model_with_chat_models 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageModelDetection::test_is_image_generation_model_with_chat_models 
tests/unit/test_cost_tracking.py::TestImageModelDetection::test_is_image_generation_model_with_edge_cases 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageModelDetection::test_is_image_generation_model_with_edge_cases 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_attach_image_cost_post_processor_with_valid_pricing 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_attach_image_cost_post_processor_with_valid_pricing 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_attach_image_cost_post_processor_with_missing_pricing 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_attach_image_cost_post_processor_with_missing_pricing 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_attach_image_cost_post_processor_with_no_image_pricing 
[gw3] [ 93%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_attach_image_cost_post_processor_with_no_image_pricing 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_attach_image_cost_post_processor_with_invalid_provider 
[gw3] [ 94%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_attach_image_cost_post_processor_with_invalid_provider 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_make_agent_async_with_image_model 
[gw3] [ 94%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_make_agent_async_with_image_model 
tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_make_agent_async_with_chat_model 
[gw3] [ 94%] PASSED tests/unit/test_cost_tracking.py::TestImageCostPostProcessorAttachment::test_make_agent_async_with_chat_model 
tests/unit/test_cost_tracking_bug_fixes.py::TestBug1MetricsLostDuringOutputProcessing::test_output_processor_preserves_usage_info 
[gw3] [ 94%] PASSED tests/unit/test_cost_tracking_bug_fixes.py::TestBug1MetricsLostDuringOutputProcessing::test_output_processor_preserves_usage_info 
[gw1] [ 94%] PASSED tests/benchmarks/test_tracing_performance.py::TestTracingPerformance::test_tracing_performance_regression 
tests/benchmarks/test_ultra_executor_performance.py::test_performance_threshold_detection 
[gw1] [ 94%] PASSED tests/benchmarks/test_ultra_executor_performance.py::test_performance_threshold_detection 
tests/e2e/test_e2e_pipelines.py::test_sql_pipeline_with_real_validator 
[gw1] [ 94%] PASSED tests/e2e/test_e2e_pipelines.py::test_sql_pipeline_with_real_validator 
tests/e2e/test_e2e_self_improvement.py::test_e2e_self_improvement_workflow 
[gw1] [ 94%] PASSED tests/e2e/test_e2e_self_improvement.py::test_e2e_self_improvement_workflow 
tests/e2e/test_golden_transcript_agentic_loop.py::test_golden_transcript_agentic_loop 
[gw1] [ 94%] FAILED tests/e2e/test_golden_transcript_agentic_loop.py::test_golden_transcript_agentic_loop 
tests/e2e/test_golden_transcript_agentic_loop.py::test_golden_transcript_agentic_loop_resume 
[gw1] [ 94%] PASSED tests/e2e/test_golden_transcript_agentic_loop.py::test_golden_transcript_agentic_loop_resume 
tests/e2e/test_golden_transcript_core.py::test_golden_transcript_core 
[gw1] [ 94%] PASSED tests/e2e/test_golden_transcript_core.py::test_golden_transcript_core 
tests/e2e/test_golden_transcript_core.py::test_golden_transcript_core_branch_b 
[gw1] [ 94%] PASSED tests/e2e/test_golden_transcript_core.py::test_golden_transcript_core_branch_b 
tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel 
[gw1] [ 94%] FAILED tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel 
tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel_selective 
[gw1] [ 94%] FAILED tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel_selective 
tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel_empty 
[gw1] [ 94%] PASSED tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel_empty 
tests/e2e/test_golden_transcript_refine.py::test_golden_transcript_refine 
[gw1] [ 94%] PASSED tests/e2e/test_golden_transcript_refine.py::test_golden_transcript_refine 
tests/e2e/test_golden_transcript_refine.py::test_golden_transcript_refine_max_iterations 
[gw1] [ 94%] FAILED tests/e2e/test_golden_transcript_refine.py::test_golden_transcript_refine_max_iterations 
tests/e2e/test_golden_transcript_refine.py::test_golden_transcript_refine_feedback_flow 
[gw1] [ 94%] PASSED tests/e2e/test_golden_transcript_refine.py::test_golden_transcript_refine_feedback_flow 
tests/e2e/test_realistic_code_review_pipeline.py::test_realistic_code_review_pipeline_success 
[gw1] [ 94%] PASSED tests/e2e/test_realistic_code_review_pipeline.py::test_realistic_code_review_pipeline_success 
tests/e2e/test_realistic_code_review_pipeline.py::test_realistic_code_review_pipeline_poor_quality 
[gw1] [ 94%] PASSED tests/e2e/test_realistic_code_review_pipeline.py::test_realistic_code_review_pipeline_poor_quality 
tests/e2e/test_realistic_code_review_pipeline.py::test_realistic_code_review_pipeline_with_failure_recovery 
[gw1] [ 94%] PASSED tests/e2e/test_realistic_code_review_pipeline.py::test_realistic_code_review_pipeline_with_failure_recovery 
tests/e2e/test_realistic_code_review_pipeline.py::test_realistic_code_review_pipeline_e2e 
[gw1] [ 94%] PASSED tests/e2e/test_realistic_code_review_pipeline.py::test_realistic_code_review_pipeline_e2e 
tests/integration/test_FSD11_bug_fix.py::test_fsd11_stateless_agent_make_agent_async 
[gw1] [ 95%] PASSED tests/integration/test_FSD11_bug_fix.py::test_fsd11_stateless_agent_make_agent_async 
tests/integration/test_FSD11_bug_fix.py::test_fsd11_context_aware_agent_explicit 
[gw1] [ 95%] PASSED tests/integration/test_FSD11_bug_fix.py::test_fsd11_context_aware_agent_explicit 
tests/integration/test_FSD11_bug_fix.py::test_fsd11_context_aware_agent_kwargs 
[gw1] [ 95%] PASSED tests/integration/test_FSD11_bug_fix.py::test_fsd11_context_aware_agent_kwargs 
tests/integration/test_FSD11_bug_fix.py::test_fsd11_error_propagation 
[gw1] [ 95%] PASSED tests/integration/test_FSD11_bug_fix.py::test_fsd11_error_propagation 
tests/integration/test_FSD11_bug_fix.py::test_fsd11_no_context_passed_to_stateless 
[gw1] [ 95%] PASSED tests/integration/test_FSD11_bug_fix.py::test_fsd11_no_context_passed_to_stateless 
tests/integration/test_FSD11_bug_fix.py::test_fsd11_context_required_but_none_provided 
[gw1] [ 95%] PASSED tests/integration/test_FSD11_bug_fix.py::test_fsd11_context_required_but_none_provided 
tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_hierarchical_structure 
[gw1] [ 95%] PASSED tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_hierarchical_structure 
tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_metadata_capture 
[gw1] [ 95%] PASSED tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_metadata_capture 
tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_persistence_recovery 
[gw1] [ 95%] PASSED tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_persistence_recovery 
tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_error_handling 
[gw1] [ 95%] PASSED tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_error_handling 
tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_large_pipeline 
[gw1] [ 95%] PASSED tests/integration/test_fsd_12_tracing_complete.py::TestFSD12TracingComplete::test_trace_large_pipeline 
tests/integration/test_hitl_pipeline.py::test_static_approval_pause_and_resume 
[gw1] [ 95%] PASSED tests/integration/test_hitl_pipeline.py::test_static_approval_pause_and_resume 
tests/integration/test_hitl_pipeline.py::test_dynamic_clarification_pause_and_resume 
[gw1] [ 95%] PASSED tests/integration/test_hitl_pipeline.py::test_dynamic_clarification_pause_and_resume 
tests/integration/test_hitl_pipeline.py::test_resume_with_structured_input_validation 
[gw1] [ 95%] PASSED tests/integration/test_hitl_pipeline.py::test_resume_with_structured_input_validation 
tests/integration/test_hitl_pipeline.py::test_resume_with_invalid_structured_input 
[gw1] [ 95%] PASSED tests/integration/test_hitl_pipeline.py::test_resume_with_invalid_structured_input 
tests/integration/test_hitl_pipeline.py::test_multi_turn_correction_loop 
[gw1] [ 95%] PASSED tests/integration/test_hitl_pipeline.py::test_multi_turn_correction_loop 
tests/integration/test_hitl_pipeline.py::test_resume_preserves_metrics 
[gw1] [ 95%] PASSED tests/integration/test_hitl_pipeline.py::test_resume_preserves_metrics 
tests/integration/test_hitl_pipeline.py::test_cannot_resume_non_paused_pipeline 
[gw1] [ 95%] PASSED tests/integration/test_hitl_pipeline.py::test_cannot_resume_non_paused_pipeline 
tests/integration/test_hitl_pipeline.py::test_paused_hitl_pipeline_can_be_serialized_and_resumed 
[gw1] [ 95%] PASSED tests/integration/test_hitl_pipeline.py::test_paused_hitl_pipeline_can_be_serialized_and_resumed 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_complex_pipeline_integration 
[gw1] [ 95%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_complex_pipeline_integration 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_with_different_contexts 
[gw1] [ 95%] FAILED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_with_different_contexts 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_with_telemetry 
[gw1] [ 95%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_with_telemetry 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_with_usage_limits 
[gw1] [ 96%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_with_usage_limits 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_legacy_compatibility 
[gw1] [ 96%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_legacy_compatibility 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_backward_compatibility 
[gw1] [ 96%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_backward_compatibility 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_performance_comparison 
[gw1] [ 96%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_performance_comparison 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_with_executor_core_integration 
[gw1] [ 96%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_with_executor_core_integration 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_error_handling_integration 
[gw1] [ 96%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_error_handling_integration 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_context_serialization_integration 
[gw1] [ 96%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_context_serialization_integration 
tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_memory_efficiency_integration 
[gw1] [ 96%] PASSED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_memory_efficiency_integration 
tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_basic 
[gw1] [ 96%] PASSED tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_basic 
tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_error_handling 
[gw1] [ 96%] PASSED tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_error_handling 
tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_context_dependent 
[gw1] [ 96%] PASSED tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_context_dependent 
tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_state_isolation 
[gw1] [ 96%] PASSED tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_state_isolation 
tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_complex_interaction 
[gw1] [ 96%] PASSED tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_complex_interaction 
tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_metadata_conflicts 
[gw1] [ 96%] PASSED tests/integration/test_hitl_with_context_updates.py::test_hitl_with_context_updates_metadata_conflicts 
tests/integration/test_hybrid_validation.py::test_successful_hybrid_validation 
[gw1] [ 96%] PASSED tests/integration/test_hybrid_validation.py::test_successful_hybrid_validation 
tests/integration/test_hybrid_validation.py::test_programmatic_check_failure 
[gw1] [ 96%] FAILED tests/integration/test_hybrid_validation.py::test_programmatic_check_failure 
tests/integration/test_hybrid_validation.py::test_context_aware_validator 
[gw1] [ 96%] PASSED tests/integration/test_hybrid_validation.py::test_context_aware_validator 
tests/integration/test_hybrid_validation.py::test_aggregated_feedback 
[gw1] [ 96%] FAILED tests/integration/test_hybrid_validation.py::test_aggregated_feedback 
tests/integration/test_hybrid_validation.py::test_backward_compatibility_no_validators 
[gw1] [ 96%] PASSED tests/integration/test_hybrid_validation.py::test_backward_compatibility_no_validators 
tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_end_to_end_success 
[gw1] [ 96%] PASSED tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_end_to_end_success 
tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_hd_quality 
[gw1] [ 96%] PASSED tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_hd_quality 
tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_large_size 
[gw1] [ 96%] PASSED tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_large_size 
tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_usage_limits 
[gw1] [ 97%] PASSED tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_usage_limits 
tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_multiple_steps 
[gw1] [ 97%] PASSED tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_multiple_steps 
tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_missing_pricing 
[gw1] [ 97%] PASSED tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_with_missing_pricing 
tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_regression_with_chat_models 
[gw1] [ 97%] PASSED tests/integration/test_image_cost_integration.py::TestImageCostIntegration::test_image_cost_tracking_regression_with_chat_models 
tests/integration/test_local_tracer.py::test_default_local_tracer_added 
[gw1] [ 97%] PASSED tests/integration/test_local_tracer.py::test_default_local_tracer_added 
tests/integration/test_local_tracer.py::test_custom_console_tracer_instance 
[gw1] [ 97%] PASSED tests/integration/test_local_tracer.py::test_custom_console_tracer_instance 
tests/integration/test_local_tracer.py::test_tracer_outputs_info_level 
[gw1] [ 97%] PASSED tests/integration/test_local_tracer.py::test_tracer_outputs_info_level 
tests/integration/test_local_tracer.py::test_tracer_outputs_debug_level 
[gw1] [ 97%] PASSED tests/integration/test_local_tracer.py::test_tracer_outputs_debug_level 
tests/integration/test_loop_context_update_regression.py::test_regression_first_principles_guarantee 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_first_principles_guarantee 
tests/integration/test_loop_context_update_regression.py::test_regression_assertion_catches_merge_failures 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_assertion_catches_merge_failures 
tests/integration/test_loop_context_update_regression.py::test_regression_parallel_step_context_updates 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_parallel_step_context_updates 
tests/integration/test_loop_context_update_regression.py::test_regression_conditional_step_context_updates 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_conditional_step_context_updates 
tests/integration/test_loop_context_update_regression.py::test_regression_edge_case_deep_copy_isolation 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_edge_case_deep_copy_isolation 
tests/integration/test_loop_context_update_regression.py::test_regression_serialization_edge_cases 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_serialization_edge_cases 
tests/integration/test_loop_context_update_regression.py::test_regression_performance_under_load 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_performance_under_load 
tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_basic 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_basic 
tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_multiple_iterations 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_multiple_iterations 
tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_max_loops 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_max_loops 
tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_complex_state 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_complex_state 
tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_error_handling 
[gw1] [ 97%] PASSED tests/integration/test_loop_context_updates_fix.py::test_loop_context_updates_error_handling 
tests/integration/test_loop_step_execution.py::test_basic_loop_until_condition_met 
[gw1] [ 97%] PASSED tests/integration/test_loop_step_execution.py::test_basic_loop_until_condition_met 
tests/integration/test_loop_step_execution.py::test_loop_max_loops_reached 
[gw1] [ 97%] PASSED tests/integration/test_loop_step_execution.py::test_loop_max_loops_reached 
tests/integration/test_loop_step_execution.py::test_iteration_mapper_not_called_on_max_loops 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_iteration_mapper_not_called_on_max_loops 
tests/integration/test_loop_step_execution.py::test_loop_with_context_modification 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_with_context_modification 
tests/integration/test_loop_step_execution.py::test_loop_iteration_context_isolated 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_iteration_context_isolated 
tests/integration/test_loop_step_execution.py::test_loop_step_error_in_exit_condition_callable 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_error_in_exit_condition_callable 
tests/integration/test_loop_step_execution.py::test_loop_step_error_in_initial_input_mapper 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_error_in_initial_input_mapper 
tests/integration/test_loop_step_execution.py::test_loop_step_error_in_iteration_input_mapper 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_error_in_iteration_input_mapper 
tests/integration/test_loop_step_execution.py::test_loop_step_error_in_loop_output_mapper 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_error_in_loop_output_mapper 
tests/integration/test_loop_step_execution.py::test_loop_step_body_failure_with_robust_exit_condition 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_body_failure_with_robust_exit_condition 
tests/integration/test_loop_step_execution.py::test_loop_step_body_failure_causing_exit_condition_error 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_body_failure_causing_exit_condition_error 
tests/integration/test_loop_step_execution.py::test_loop_step_initial_input_mapper_flow 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_initial_input_mapper_flow 
tests/integration/test_loop_step_execution.py::test_loop_step_iteration_input_mapper_flow 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_iteration_input_mapper_flow 
tests/integration/test_loop_step_execution.py::test_loop_step_loop_output_mapper_flow 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_loop_output_mapper_flow 
tests/integration/test_loop_step_execution.py::test_loop_step_mappers_with_context_modification 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_mappers_with_context_modification 
tests/integration/test_loop_step_execution.py::test_loop_step_default_mapper_behavior 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_default_mapper_behavior 
tests/integration/test_loop_step_execution.py::test_loop_step_overall_span 
[gw1] [ 98%] PASSED tests/integration/test_loop_step_execution.py::test_loop_step_overall_span 
tests/integration/test_loop_step_execution.py::test_loop_step_iteration_spans_and_logging 
[gw1] [ 98%] FAILED tests/integration/test_loop_step_execution.py::test_loop_step_iteration_spans_and_logging 
tests/integration/test_loop_step_execution.py::test_loop_step_error_logging_in_callables 
[gw1] [ 98%] FAILED tests/integration/test_loop_step_execution.py::test_loop_step_error_logging_in_callables 
tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_basic 
[gw1] [ 98%] PASSED tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_basic 
tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_complex 
[gw1] [ 98%] PASSED tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_complex 
tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_mapper_conflicts 
[gw1] [ 98%] PASSED tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_mapper_conflicts 
tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_max_loops 
[gw1] [ 98%] PASSED tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_max_loops 
tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_error_handling 
[gw1] [ 98%] FAILED tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_error_handling 
tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_state_isolation 
[gw1] [ 99%] PASSED tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_state_isolation 
tests/integration/test_map_over_step.py::test_map_over_sequential 
[gw1] [ 99%] FAILED tests/integration/test_map_over_step.py::test_map_over_sequential 
tests/integration/test_map_over_step.py::test_map_over_parallel 
[gw1] [ 99%] FAILED tests/integration/test_map_over_step.py::test_map_over_parallel 
tests/integration/test_map_over_step.py::test_map_over_empty 
[gw1] [ 99%] PASSED tests/integration/test_map_over_step.py::test_map_over_empty 
tests/integration/test_map_over_step.py::test_map_over_invalid_input 
[gw1] [ 99%] PASSED tests/integration/test_map_over_step.py::test_map_over_invalid_input 
tests/integration/test_map_over_step.py::test_map_over_reusable_after_empty 
[gw1] [ 99%] FAILED tests/integration/test_map_over_step.py::test_map_over_reusable_after_empty 
tests/integration/test_map_over_step.py::test_map_over_concurrent_runs 
[gw1] [ 99%] FAILED tests/integration/test_map_over_step.py::test_map_over_concurrent_runs 
tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_basic 
[gw1] [ 99%] FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_basic 
tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_error_handling 
[gw1] [ 99%] FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_error_handling 
tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_context_dependent 
[gw1] [ 99%] FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_context_dependent 
tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_nested_context 
[gw1] [ 99%] FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_nested_context 
tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_state_isolation 
[gw1] [ 99%] FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_state_isolation 
tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_complex_aggregation 
[gw1] [ 99%] FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_complex_aggregation 
tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_metadata_conflicts 
[gw1] [ 99%] FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_metadata_conflicts 
tests/integration/test_mock_output_handling.py::test_concrete_value_passes 
[gw1] [ 99%] PASSED tests/integration/test_mock_output_handling.py::test_concrete_value_passes 
tests/integration/test_mock_output_handling.py::test_mock_output_raises_type_error 
[gw1] [ 99%] PASSED tests/integration/test_mock_output_handling.py::test_mock_output_raises_type_error 
tests/integration/test_mock_output_handling.py::test_nested_mock_not_caught 
[gw1] [ 99%] PASSED tests/integration/test_mock_output_handling.py::test_nested_mock_not_caught 
tests/integration/test_mock_output_handling.py::test_pipeline_stops_on_mock 
[gw1] [ 99%] PASSED tests/integration/test_mock_output_handling.py::test_pipeline_stops_on_mock 
tests/integration/test_nested_dsl_constructs.py::test_nested_loop_in_loop 
[gw1] [ 99%] PASSED tests/integration/test_nested_dsl_constructs.py::test_nested_loop_in_loop 
tests/integration/test_nested_dsl_constructs.py::test_nested_conditional_in_loop 
[gw1] [ 99%] PASSED tests/integration/test_nested_dsl_constructs.py::test_nested_conditional_in_loop 
tests/integration/test_nested_dsl_constructs.py::test_nested_loop_in_conditional_branch 
[gw1] [ 99%] PASSED tests/integration/test_nested_dsl_constructs.py::test_nested_loop_in_conditional_branch 
tests/integration/test_nested_dsl_constructs.py::test_nested_conditional_in_conditional_branch 
[gw1] [ 99%] PASSED tests/integration/test_nested_dsl_constructs.py::test_nested_conditional_in_conditional_branch 
tests/integration/test_nested_dsl_constructs.py::test_deeply_nested_context_modification_and_access 
[gw1] [100%] PASSED tests/integration/test_nested_dsl_constructs.py::test_deeply_nested_context_modification_and_access 

==================================== ERRORS ====================================
____ ERROR collecting tests/application/core/test_step_logic_accounting.py _____
ImportError while importing test module '/Users/alvaro/Documents/Code/flujo/tests/application/core/test_step_logic_accounting.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/application/core/test_step_logic_accounting.py:11: in <module>
    from flujo.application.core.step_logic import _run_step_logic
E   ModuleNotFoundError: No module named 'flujo.application.core.step_logic'
_____ ERROR collecting tests/benchmarks/test_legacy_cleanup_performance.py _____
ImportError while importing test module '/Users/alvaro/Documents/Code/flujo/tests/benchmarks/test_legacy_cleanup_performance.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/benchmarks/test_legacy_cleanup_performance.py:15: in <module>
    from flujo.application.core.step_logic import (
E   ModuleNotFoundError: No module named 'flujo.application.core.step_logic'
_____ ERROR collecting tests/integration/test_legacy_cleanup_validation.py _____
ImportError while importing test module '/Users/alvaro/Documents/Code/flujo/tests/integration/test_legacy_cleanup_validation.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/integration/test_legacy_cleanup_validation.py:12: in <module>
    from flujo.application.core.step_logic import (
E   ModuleNotFoundError: No module named 'flujo.application.core.step_logic'
_______ ERROR collecting tests/regression/test_legacy_cleanup_impact.py ________
ImportError while importing test module '/Users/alvaro/Documents/Code/flujo/tests/regression/test_legacy_cleanup_impact.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/regression/test_legacy_cleanup_impact.py:13: in <module>
    from flujo.application.core.step_logic import (
E   ModuleNotFoundError: No module named 'flujo.application.core.step_logic'
_________ ERROR collecting tests/unit/test_fallback_loop_detection.py __________
ImportError while importing test module '/Users/alvaro/Documents/Code/flujo/tests/unit/test_fallback_loop_detection.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/unit/test_fallback_loop_detection.py:14: in <module>
    from flujo.application.core.step_logic import _detect_fallback_loop
E   ModuleNotFoundError: No module named 'flujo.application.core.step_logic'
____________ ERROR collecting tests/unit/test_ultra_executor_v2.py _____________
ImportError while importing test module '/Users/alvaro/Documents/Code/flujo/tests/unit/test_ultra_executor_v2.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py:126: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/unit/test_ultra_executor_v2.py:19: in <module>
    from flujo.application.core.ultra_executor import (
E   ImportError: cannot import name 'CacheKeyGenerator' from 'flujo.application.core.ultra_executor' (/Users/alvaro/Documents/Code/flujo/flujo/application/core/ultra_executor.py)
______________ ERROR collecting tests/utils/test_serialization.py ______________
import file mismatch:
imported module 'test_serialization' has this __file__ attribute:
  /Users/alvaro/Documents/Code/flujo/tests/benchmarks/test_serialization.py
which is not the same as the test file we want to collect:
  /Users/alvaro/Documents/Code/flujo/tests/utils/test_serialization.py
HINT: remove __pycache__ / .pyc files and/or use a unique basename for your test file modules
=================================== FAILURES ===================================
_ TestExecutorCoreHITLStepMigration.test_handle_hitl_step_context_preservation _
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_hitl_step_migration.py:244: in test_handle_hitl_step_context_preservation
    assert mock_context.scratchpad["existing_key"] == "existing_value"
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'existing_key'
____________ test_dynamic_router_multiple_branches_context_updates _____________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_dynamic_parallel_router_with_context_updates.py:184: in test_dynamic_router_multiple_branches_context_updates
    assert "support" in result.final_pipeline_context.branch_results
E   AssertionError: assert 'support' in {'billing': 'billing:Need both billing and support'}
E    +  where {'billing': 'billing:Need both billing and support'} = DynamicRouterContext(run_id='run_7389d41a989b4f06804b7cdeb0cd706b', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:Need both billing and support'}, execution_count=1, router_called=True, context_updates=['billing_processed']).branch_results
E    +    where DynamicRouterContext(run_id='run_7389d41a989b4f06804b7cdeb0cd706b', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:Need both billing and support'}, execution_count=1, router_called=True, context_updates=['billing_processed']) = PipelineResult(step_history=[StepResult(name='dynamic_router', output={'billing': {'billing_result': 'billing:Need both billing and support'}}, success=True, attempts=1, latency_s=0.0006684580002911389, token_counts=0, cost_usd=0.0, feedback='All 1 branches executed successfully', branch_context=DynamicRouterContext(run_id='run_7389d41a989b4f06804b7cdeb0cd706b', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:Need both billing and support'}, execution_count=1, router_called=True, context_updates=['billing_processed']), metadata_={}, step_history=[])], total_cost_usd=0.0, total_tokens=0, final_pipeline_context=DynamicRouterContext(run_id='run_7389d41a989b4f06804b7cdeb0cd706b', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:Need both billing and support'}, execution_count=1, router_called=True, context_updates=['billing_processed']), trace_tree=Span(span_id='89148e75-490e-4f3b-a205-335ecff185ea', name='pipeline_root', start_time=286532.097636166, end_time=286532.099230125, parent_span_id=None, attributes={'initial_input': 'Need both billing and support'}, children=[Span(span_id='cdba873c-0e46-45e8-a3e4-9685a1268016', name='dynamic_router', start_time=286532.097698333, end_time=286532.098686833, parent_span_id='89148e75-490e-4f3b-a205-335ecff185ea', attributes={'step_type': 'DynamicParallelRouterStep[~ContextModelT]', 'step_input': 'Need both billing and support', 'success': True, 'attempts': 1, 'latency_s': 0.0006684580002911389, 'cost_usd': 0.0, 'token_counts': 0}, children=[], status='completed')], status='completed')).final_pipeline_context
----------------------------- Captured stdout call -----------------------------
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_dynamic_parallel_router_with_context_updates.DynamicRouterContext'>
[DEBUG] source_context type: <class 'tests.integration.test_dynamic_parallel_router_with_context_updates.DynamicRouterContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_7389d41a989b4f06804b7cdeb0cd706b', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'router_selection': [], 'branch_results': {'billing': 'billing:Need both billing and support'}, 'execution_count': 1, 'router_called': True, 'context_updates': ['billing_processed']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_7389d41a989b4f06804b7cdeb0cd706b
[DEBUG] actual_source_value: run_7389d41a989b4f06804b7cdeb0cd706b
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: router_selection
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: router_selection
[DEBUG] No new items to add to list field: router_selection
[DEBUG] Processing field: branch_results
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {'billing': 'billing:Need both billing and support'}
[DEBUG] Merging dictionaries for field: branch_results
[DEBUG] Updated dict field: branch_results
[DEBUG] Processing field: execution_count
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: execution_count
[DEBUG] Processing field: router_called
[DEBUG] current_value: True
[DEBUG] actual_source_value: True
[DEBUG] Field unchanged: router_called
[DEBUG] Processing field: context_updates
[DEBUG] current_value: []
[DEBUG] actual_source_value: ['billing_processed']
[DEBUG] Merging lists for field: context_updates
[DEBUG] Updated list field: context_updates with new items: ['billing_processed']
[DEBUG] Total fields updated: 2
[DEBUG] Validation errors: []
__ TestExecutorCoreHITLStepMigration.test_handle_hitl_step_context_isolation ___
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_hitl_step_migration.py:313: in test_handle_hitl_step_context_isolation
    assert mock_context.scratchpad[key] == value
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'key1'
___________ test_dynamic_router_branch_failure_context_preservation ____________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_dynamic_parallel_router_with_context_updates.py:268: in test_dynamic_router_branch_failure_context_preservation
    assert "support_failed" in result.final_pipeline_context.context_updates
E   AssertionError: assert 'support_failed' in ['router_called', 'billing_processed']
E    +  where ['router_called', 'billing_processed'] = DynamicRouterContext(run_id='run_0c5afacfea6542d696920db4ba519008', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:test'}, execution_count=0, router_called=True, context_updates=['router_called', 'billing_processed']).context_updates
E    +    where DynamicRouterContext(run_id='run_0c5afacfea6542d696920db4ba519008', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:test'}, execution_count=0, router_called=True, context_updates=['router_called', 'billing_processed']) = PipelineResult(step_history=[StepResult(name='dynamic_router', output={'billing': {'billing_result': 'billing:test'}}, success=True, attempts=1, latency_s=0.0012312500039115548, token_counts=0, cost_usd=0.0, feedback='All 1 branches executed successfully', branch_context=DynamicRouterContext(run_id='run_0c5afacfea6542d696920db4ba519008', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:test'}, execution_count=0, router_called=True, context_updates=['router_called', 'billing_processed']), metadata_={}, step_history=[])], total_cost_usd=0.0, total_tokens=0, final_pipeline_context=DynamicRouterContext(run_id='run_0c5afacfea6542d696920db4ba519008', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:test'}, execution_count=0, router_called=True, context_updates=['router_called', 'billing_processed']), trace_tree=Span(span_id='adcf6105-50a1-4822-8c6b-b1a06018213e', name='pipeline_root', start_time=286532.110641958, end_time=286532.113114833, parent_span_id=None, attributes={'initial_input': 'test'}, children=[Span(span_id='cb64a7e8-5b76-46c5-80dd-10f11d16b351', name='dynamic_router', start_time=286532.110696333, end_time=286532.112554125, parent_span_id='adcf6105-50a1-4822-8c6b-b1a06018213e', attributes={'step_type': 'DynamicParallelRouterStep[~ContextModelT]', 'step_input': 'test', 'success': True, 'attempts': 1, 'latency_s': 0.0012312500039115548, 'cost_usd': 0.0, 'token_counts': 0}, children=[], status='completed')], status='completed')).final_pipeline_context
----------------------------- Captured stdout call -----------------------------
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_dynamic_parallel_router_with_context_updates.DynamicRouterContext'>
[DEBUG] source_context type: <class 'tests.integration.test_dynamic_parallel_router_with_context_updates.DynamicRouterContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_0c5afacfea6542d696920db4ba519008', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'router_selection': [], 'branch_results': {'billing': 'billing:test'}, 'execution_count': 0, 'router_called': True, 'context_updates': ['router_called', 'billing_processed']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_0c5afacfea6542d696920db4ba519008
[DEBUG] actual_source_value: run_0c5afacfea6542d696920db4ba519008
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: router_selection
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: router_selection
[DEBUG] No new items to add to list field: router_selection
[DEBUG] Processing field: branch_results
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {'billing': 'billing:test'}
[DEBUG] Merging dictionaries for field: branch_results
[DEBUG] Updated dict field: branch_results
[DEBUG] Processing field: execution_count
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: execution_count
[DEBUG] Processing field: router_called
[DEBUG] current_value: True
[DEBUG] actual_source_value: True
[DEBUG] Field unchanged: router_called
[DEBUG] Processing field: context_updates
[DEBUG] current_value: ['router_called']
[DEBUG] actual_source_value: ['router_called', 'billing_processed']
[DEBUG] Merging lists for field: context_updates
[DEBUG] Updated list field: context_updates with new items: ['billing_processed']
[DEBUG] Total fields updated: 2
[DEBUG] Validation errors: []
______________ test_dynamic_router_multiple_branches_context_fix _______________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_dynamic_router_bug_fix.py:118: in test_dynamic_router_multiple_branches_context_fix
    assert "support" in result.final_pipeline_context.branch_results
E   AssertionError: assert 'support' in {'billing': 'billing:Need both billing and support'}
E    +  where {'billing': 'billing:Need both billing and support'} = DynamicRouterTestContext(run_id='run_01e31851fa2b4e6ca2309c9eec759c4c', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_called=True, branch_results={'billing': 'billing:Need both billing and support'}, context_updates=['router_executed', 'billing_processed']).branch_results
E    +    where DynamicRouterTestContext(run_id='run_01e31851fa2b4e6ca2309c9eec759c4c', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_called=True, branch_results={'billing': 'billing:Need both billing and support'}, context_updates=['router_executed', 'billing_processed']) = PipelineResult(step_history=[StepResult(name='dynamic_router', output={'billing': {'billing_result': 'billing:Need both billing and support'}}, success=True, attempts=1, latency_s=0.0005972090293653309, token_counts=0, cost_usd=0.0, feedback='All 1 branches executed successfully', branch_context=DynamicRouterTestContext(run_id='run_01e31851fa2b4e6ca2309c9eec759c4c', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_called=True, branch_results={'billing': 'billing:Need both billing and support'}, context_updates=['router_executed', 'billing_processed']), metadata_={}, step_history=[])], total_cost_usd=0.0, total_tokens=0, final_pipeline_context=DynamicRouterTestContext(run_id='run_01e31851fa2b4e6ca2309c9eec759c4c', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_called=True, branch_results={'billing': 'billing:Need both billing and support'}, context_updates=['router_executed', 'billing_processed']), trace_tree=Span(span_id='030c1bc2-ff80-4231-b07c-2ebb8b372399', name='pipeline_root', start_time=286532.157646416, end_time=286532.159075625, parent_span_id=None, attributes={'initial_input': 'Need both billing and support'}, children=[Span(span_id='4a041c20-c41e-4f6b-a545-26618ca39fe9', name='dynamic_router', start_time=286532.157711166, end_time=286532.15858775, parent_span_id='030c1bc2-ff80-4231-b07c-2ebb8b372399', attributes={'step_type': 'DynamicParallelRouterStep[~ContextModelT]', 'step_input': 'Need both billing and support', 'success': True, 'attempts': 1, 'latency_s': 0.0005972090293653309, 'cost_usd': 0.0, 'token_counts': 0}, children=[], status='completed')], status='completed')).final_pipeline_context
----------------------------- Captured stdout call -----------------------------
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_dynamic_router_bug_fix.DynamicRouterTestContext'>
[DEBUG] source_context type: <class 'tests.integration.test_dynamic_router_bug_fix.DynamicRouterTestContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_01e31851fa2b4e6ca2309c9eec759c4c', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'router_called': True, 'branch_results': {'billing': 'billing:Need both billing and support'}, 'context_updates': ['router_executed', 'billing_processed']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_01e31851fa2b4e6ca2309c9eec759c4c
[DEBUG] actual_source_value: run_01e31851fa2b4e6ca2309c9eec759c4c
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: router_called
[DEBUG] current_value: True
[DEBUG] actual_source_value: True
[DEBUG] Field unchanged: router_called
[DEBUG] Processing field: branch_results
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {'billing': 'billing:Need both billing and support'}
[DEBUG] Merging dictionaries for field: branch_results
[DEBUG] Updated dict field: branch_results
[DEBUG] Processing field: context_updates
[DEBUG] current_value: ['router_executed']
[DEBUG] actual_source_value: ['router_executed', 'billing_processed']
[DEBUG] Merging lists for field: context_updates
[DEBUG] Updated list field: context_updates with new items: ['billing_processed']
[DEBUG] Total fields updated: 2
[DEBUG] Validation errors: []
__________ TestExecutorCoreSimpleStep.test_successful_run_no_retries ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:124: in test_successful_run_no_retries
    executor_core._processor_pipeline.apply_prompt.assert_called_once()
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:902: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'apply_prompt' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,271 - flujo - INFO - Counting string output as 1 token for step 'test_step': 'raw output'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'test_step': 'raw output'
___________ test_dynamic_router_with_context_updates_error_handling ____________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_dynamic_router_with_context_updates.py:348: in test_dynamic_router_with_context_updates_error_handling
    assert "branch 'failing_branch' failed" in result.step_history[-1].feedback.lower()
E   assert "branch 'failing_branch' failed" in "parallel step failed: branch 'failing_branch': agent execution failed with runtimeerror: intentional router branch failure"
E    +  where "parallel step failed: branch 'failing_branch': agent execution failed with runtimeerror: intentional router branch failure" = <built-in method lower of str object at 0x112cf6c30>()
E    +    where <built-in method lower of str object at 0x112cf6c30> = "Parallel step failed: Branch 'failing_branch': Agent execution failed with RuntimeError: Intentional router branch failure".lower
E    +      where "Parallel step failed: Branch 'failing_branch': Agent execution failed with RuntimeError: Intentional router branch failure" = StepResult(name='error_router', output={'failing_branch': StepResult(name='error_router_failing_branch', output=None, success=False, attempts=1, latency_s=0.0003892080276273191, token_counts=0, cost_usd=0.0, feedback='Agent execution failed with RuntimeError: Intentional router branch failure', branch_context=RouterContext(run_id='run_2f102aaa3d234437b947a514a023cd95', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], router_state='executed_failing', branch_executed='failing_branch', branch_count=2, total_updates=0, router_metadata={}, branch_results={}), metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.000594375014770776, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'failing_branch': Agent execution failed with RuntimeError: Intentional router branch failure", branch_context=RouterContext(run_id='run_2f102aaa3d234437b947a514a023cd95', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], router_state='', branch_executed='', branch_count=0, total_updates=0, router_metadata={}, branch_results={}), metadata_={}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,364 - flujo - WARNING - Step 'failing_branch' agent execution attempt 1 failed: Intentional router branch failure
2025-08-04 22:44:59,364 - flujo - WARNING - Step 'failing_branch' agent execution attempt 2 failed: Intentional router branch failure
2025-08-04 22:44:59,364 - flujo - WARNING - Step 'error_router' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,364 - flujo - ERROR - Step 'failing_branch' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'failing_branch' agent execution attempt 1 failed: Intentional router branch failure
WARNING  flujo:telemetry.py:54 Step 'failing_branch' agent execution attempt 2 failed: Intentional router branch failure
ERROR    flujo:telemetry.py:54 Step 'failing_branch' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'error_router' failed. Halting pipeline execution.
_ TestEmbeddingCostIntegration.test_embedding_cost_tracking_strict_mode_failure _
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_embedding_cost_integration.py:210: in test_embedding_cost_tracking_strict_mode_failure
    with pytest.raises(PricingNotConfiguredError) as exc_info:
E   Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,394 - flujo - INFO - Extracted tokens for step 'EmbedDocument': prompt=100, completion=0
2025-08-04 22:44:59,394 - flujo - INFO - Extracted model ID for step 'EmbedDocument' from 'model_id': openai:text-embedding-3-large
2025-08-04 22:44:59,394 - flujo - WARNING - Step 'EmbedDocument' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='text-embedding-3-large' in flujo.toml.
2025-08-04 22:44:59,394 - flujo - INFO - Extracted tokens for step 'EmbedDocument': prompt=100, completion=0
2025-08-04 22:44:59,394 - flujo - WARNING - Step 'EmbedDocument' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='text-embedding-3-large' in flujo.toml.
2025-08-04 22:44:59,395 - flujo - WARNING - Step 'EmbedDocument' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,394 - flujo - ERROR - Step 'EmbedDocument' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Extracted tokens for step 'EmbedDocument': prompt=100, completion=0
INFO     flujo:telemetry.py:54 Extracted model ID for step 'EmbedDocument' from 'model_id': openai:text-embedding-3-large
WARNING  flujo:telemetry.py:54 Step 'EmbedDocument' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='text-embedding-3-large' in flujo.toml.
INFO     flujo:telemetry.py:54 Extracted tokens for step 'EmbedDocument': prompt=100, completion=0
WARNING  flujo:telemetry.py:54 Step 'EmbedDocument' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='text-embedding-3-large' in flujo.toml.
ERROR    flujo:telemetry.py:54 Step 'EmbedDocument' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'EmbedDocument' failed. Halting pipeline execution.
_______ TestExecutorCoreSimpleStep.test_validator_failure_triggers_retry _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:251: in test_validator_failure_triggers_retry
    assert result.attempts == 4  # max_retries (1 initial + 3 retries)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert 1 == 4
E    +  where 1 = StepResult(name='test_step', output='processed output', success=False, attempts=1, latency_s=0.00034433399559929967, token_counts=1, cost_usd=0.0, feedback='Validation failed: Validation failed', branch_context=None, metadata_={}, step_history=[]).attempts
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,389 - flujo - INFO - Counting string output as 1 token for step 'test_step': 'raw output'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,389 - flujo - ERROR - Step 'test_step' validation failed: Validation failed
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'test_step': 'raw output'
ERROR    flujo:telemetry.py:54 Step 'test_step' validation failed: Validation failed
_ TestExecutorCoreLoopStepDispatch.test_execute_complex_step_routes_loopstep_to_new_handler _
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_loop_step_dispatch.py:35: in test_execute_complex_step_routes_loopstep_to_new_handler
    result = await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
_ TestExecutorCoreLoopStepDispatch.test_execute_complex_step_loopstep_parameter_passing _
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_loop_step_dispatch.py:72: in test_execute_complex_step_loopstep_parameter_passing
    await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
_ TestExecutorCoreLoopStepDispatch.test_execute_complex_step_loopstep_legacy_import_removed _
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_loop_step_dispatch.py:108: in test_execute_complex_step_loopstep_legacy_import_removed
    result = await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
____ TestExecutorCoreSimpleStep.test_usage_limit_exceeded_error_propagates _____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:276: in test_usage_limit_exceeded_error_propagates
    with pytest.raises(UsageLimitExceededError, match="Cost limit exceeded"):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,408 - flujo - INFO - Counting string output as 1 token for step 'test_step': 'raw output'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'test_step': 'raw output'
_ TestExecutorCoreLoopStepDispatch.test_execute_complex_step_loopstep_error_propagation _
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_loop_step_dispatch.py:135: in test_execute_complex_step_loopstep_error_propagation
    await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'

During handling of the above exception, another exception occurred:
tests/application/core/test_executor_core_loop_step_dispatch.py:134: in test_execute_complex_step_loopstep_error_propagation
    with pytest.raises(Exception, match="Test error"):
E   AssertionError: Regex pattern did not match.
E    Regex: 'Test error'
E    Input: "ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'"
_ TestExecutorCoreLoopStepDispatch.test_execute_complex_step_loopstep_telemetry_logging _
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_loop_step_dispatch.py:159: in test_execute_complex_step_loopstep_telemetry_logging
    await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
_________ TestEmbeddingRegression.test_strict_mode_behavior_unchanged __________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_embedding_regression.py:302: in test_strict_mode_behavior_unchanged
    with pytest.raises(PricingNotConfiguredError) as exc_info:
E   Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,425 - flujo - INFO - Extracted tokens for step 'ChatResponse': prompt=50, completion=25
2025-08-04 22:44:59,425 - flujo - INFO - Extracted model ID for step 'ChatResponse' from 'model_id': openai:unknown-model
2025-08-04 22:44:59,425 - flujo - WARNING - Step 'ChatResponse' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='unknown-model' in flujo.toml.
2025-08-04 22:44:59,425 - flujo - INFO - Extracted tokens for step 'ChatResponse': prompt=50, completion=25
2025-08-04 22:44:59,426 - flujo - WARNING - Step 'ChatResponse' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='unknown-model' in flujo.toml.
2025-08-04 22:44:59,426 - flujo - WARNING - Step 'ChatResponse' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,426 - flujo - ERROR - Step 'ChatResponse' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Extracted tokens for step 'ChatResponse': prompt=50, completion=25
INFO     flujo:telemetry.py:54 Extracted model ID for step 'ChatResponse' from 'model_id': openai:unknown-model
WARNING  flujo:telemetry.py:54 Step 'ChatResponse' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='unknown-model' in flujo.toml.
INFO     flujo:telemetry.py:54 Extracted tokens for step 'ChatResponse': prompt=50, completion=25
WARNING  flujo:telemetry.py:54 Step 'ChatResponse' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='unknown-model' in flujo.toml.
ERROR    flujo:telemetry.py:54 Step 'ChatResponse' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'ChatResponse' failed. Halting pipeline execution.
_____________ TestExecutorCoreSimpleStep.test_missing_agent_error ______________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:304: in test_missing_agent_error
    with pytest.raises(MissingAgentError, match="Step 'test_step' has no agent configured"):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.MissingAgentError'>
__________ TestLoopStepMigration.test_handle_loop_step_max_iterations __________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_loop_step_migration.py:152: in test_handle_loop_step_max_iterations
    assert result.attempts == 3  # max_retries (1 initial + 2 retries)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert 2 == 3
E    +  where 2 = StepResult(name='test_loop', output=None, success=False, attempts=2, latency_s=0.0010872079874388874, token_counts=10, cost_usd=0.1, feedback='Loop terminated after reaching max_loops (2), but last iteration body failed: Loop body failed: Agent execution failed with IndexError: No more outputs available', branch_context=LoopTestContext(counter=0, messages=[], data={}, original_value=None), metadata_={'iterations': 2, 'exit_reason': 'max_iterations_with_body_failure'}, step_history=[]).attempts
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === test_loop
2025-08-04 22:44:59,432 - flujo - INFO - LoopStep 'test_loop': Starting Iteration 1/2
2025-08-04 22:44:59,433 - flujo - INFO - Using explicit cost from 'CostlyOutput' for step 'test_step': cost=$0.1, tokens=10
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] source_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'counter': 0, 'messages': [], 'data': {}, 'original_value': None}
[DEBUG] Processing field: counter
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: counter
[DEBUG] Processing field: messages
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: messages
[DEBUG] No new items to add to list field: messages
[DEBUG] Processing field: data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: data
[DEBUG] Processing field: original_value
[DEBUG] current_value: None
[DEBUG] actual_source_value: None
[DEBUG] Field unchanged: original_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] source_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'counter': 0, 'messages': [], 'data': {}, 'original_value': None}
[DEBUG] Processing field: counter
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: counter
[DEBUG] Processing field: messages
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: messages
[DEBUG] No new items to add to list field: messages
[DEBUG] Processing field: data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: data
[DEBUG] Processing field: original_value
[DEBUG] current_value: None
[DEBUG] actual_source_value: None
[DEBUG] Field unchanged: original_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 2
2025-08-04 22:44:59,433 - flujo - INFO - LoopStep 'test_loop': Starting Iteration 2/2
2025-08-04 22:44:59,433 - flujo - WARNING - Step 'test_step' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:44:59,433 - flujo - WARNING - Step 'test_step' agent execution attempt 2 failed: No more outputs available
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] source_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'counter': 0, 'messages': [], 'data': {}, 'original_value': None}
[DEBUG] Processing field: counter
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: counter
[DEBUG] Processing field: messages
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: messages
[DEBUG] No new items to add to list field: messages
[DEBUG] Processing field: data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: data
[DEBUG] Processing field: original_value
[DEBUG] current_value: None
[DEBUG] actual_source_value: None
[DEBUG] Field unchanged: original_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] source_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'counter': 0, 'messages': [], 'data': {}, 'original_value': None}
[DEBUG] Processing field: counter
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: counter
[DEBUG] Processing field: messages
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: messages
[DEBUG] No new items to add to list field: messages
[DEBUG] Processing field: data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: data
[DEBUG] Processing field: original_value
[DEBUG] current_value: None
[DEBUG] actual_source_value: None
[DEBUG] Field unchanged: original_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 2: Checking condition - limits: False, iteration_count: 2, max_iterations: 2
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,433 - flujo - ERROR - Step 'test_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'test_loop': Starting Iteration 1/2
INFO     flujo:telemetry.py:54 Using explicit cost from 'CostlyOutput' for step 'test_step': cost=$0.1, tokens=10
INFO     flujo:telemetry.py:54 LoopStep 'test_loop': Starting Iteration 2/2
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 1 failed: No more outputs available
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'test_step' agent failed after 2 attempts
____________ TestExecutorCoreSimpleStep.test_mock_object_detection _____________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:334: in test_mock_object_detection
    with pytest.raises(TypeError, match="returned a Mock object"):
E   Failed: DID NOT RAISE <class 'TypeError'>
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,445 - flujo - ERROR - Step 'test_step' failed with critical error: Step 'test_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
------------------------------ Captured log call -------------------------------
ERROR    flujo:telemetry.py:54 Step 'test_step' failed with critical error: Step 'test_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
___________ TestLoopStepMigration.test_handle_loop_step_token_limits ___________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_loop_step_migration.py:426: in test_handle_loop_step_token_limits
    with pytest.raises(UsageLimitExceededError):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === test_loop
2025-08-04 22:44:59,464 - flujo - INFO - LoopStep 'test_loop': Starting Iteration 1/3
2025-08-04 22:44:59,464 - flujo - INFO - Using explicit cost from 'CostlyOutput' for step 'token_heavy_step': cost=$0.1, tokens=60
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] source_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'counter': 0, 'messages': [], 'data': {}, 'original_value': None}
[DEBUG] Processing field: counter
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: counter
[DEBUG] Processing field: messages
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: messages
[DEBUG] No new items to add to list field: messages
[DEBUG] Processing field: data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: data
[DEBUG] Processing field: original_value
[DEBUG] current_value: None
[DEBUG] actual_source_value: None
[DEBUG] Field unchanged: original_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] source_context type: <class 'test_executor_core_loop_step_migration.LoopTestContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'counter': 0, 'messages': [], 'data': {}, 'original_value': None}
[DEBUG] Processing field: counter
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: counter
[DEBUG] Processing field: messages
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: messages
[DEBUG] No new items to add to list field: messages
[DEBUG] Processing field: data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: data
[DEBUG] Processing field: original_value
[DEBUG] current_value: None
[DEBUG] actual_source_value: None
[DEBUG] Field unchanged: original_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,464 - flujo - ERROR - Error in LoopStep 'test_loop': '>' not supported between instances of 'float' and 'NoneType'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'test_loop': Starting Iteration 1/3
INFO     flujo:telemetry.py:54 Using explicit cost from 'CostlyOutput' for step 'token_heavy_step': cost=$0.1, tokens=60
ERROR    flujo:telemetry.py:54 Error in LoopStep 'test_loop': '>' not supported between instances of 'float' and 'NoneType'
___ TestExecutorCoreSimpleStep.test_plugin_validation_failure_with_feedback ____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:383: in test_plugin_validation_failure_with_feedback
    assert result.attempts == 3  # max_retries (1 initial + 2 retries)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert 1 == 3
E    +  where 1 = StepResult(name='test_step', output='processed output', success=False, attempts=1, latency_s=0.0004451249842531979, token_counts=1, cost_usd=0.0, feedback='Plugin failed: Plugin validation failed: Invalid format', branch_context=None, metadata_={}, step_history=[]).attempts
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,464 - flujo - INFO - Counting string output as 1 token for step 'test_step': 'raw output'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,464 - flujo - ERROR - Step 'test_step' plugin failed: Plugin validation failed: Invalid format
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'test_step': 'raw output'
ERROR    flujo:telemetry.py:54 Step 'test_step' plugin failed: Plugin validation failed: Invalid format
__________ TestExecutorCoreSimpleStep.test_plugin_failure_propagates ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:428: in test_plugin_failure_propagates
    assert "Plugin validation failed: Plugin execution error" in result.feedback
E   AssertionError: assert 'Plugin validation failed: Plugin execution error' in 'Plugin failed: Plugin execution error'
E    +  where 'Plugin failed: Plugin execution error' = StepResult(name='test_step', output='processed output', success=False, attempts=1, latency_s=0.0003417499829083681, token_counts=1, cost_usd=0.0, feedback='Plugin failed: Plugin execution error', branch_context=None, metadata_={}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,483 - flujo - INFO - Counting string output as 1 token for step 'test_step': 'raw output'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,483 - flujo - ERROR - Step 'test_step' plugin failed: Plugin failed: Plugin execution error
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'test_step': 'raw output'
ERROR    flujo:telemetry.py:54 Step 'test_step' plugin failed: Plugin failed: Plugin execution error
________ TestComponentIntegration.test_component_interface_optimization ________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_executor_core_architecture_validation.py:149: in test_component_interface_optimization
    assert serializer.serialize_calls > 0, "Serializer should be called"
E   AssertionError: Serializer should be called
E   assert 0 > 0
E    +  where 0 = <tests.integration.test_executor_core_architecture_validation.MockSerializer object at 0x11533af50>.serialize_calls
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,560 - flujo - INFO - Counting string output as 1 token for step 'test_step': 'interface_test'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'test_step': 'interface_test'
________________ TestExecutorCoreSimpleStep.test_usage_tracking ________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:561: in test_usage_tracking
    executor_core._usage_meter.guard.assert_called_once()
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:902: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'guard' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,578 - flujo - INFO - Counting string output as 1 token for step 'test_step': 'raw output'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'test_step': 'raw output'
___ TestExecutorCoreSimpleStep.test_pricing_not_configured_error_propagates ____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:657: in test_pricing_not_configured_error_propagates
    with pytest.raises(PricingNotConfiguredError, match="Pricing not configured"):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,628 - flujo - WARNING - Step 'test_step' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='Pricing not configured', model='test_model' in flujo.toml.
2025-08-04 22:44:59,629 - flujo - WARNING - Step 'test_step' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='Pricing not configured', model='test_model' in flujo.toml.
2025-08-04 22:44:59,629 - flujo - WARNING - Step 'test_step' agent execution attempt 3 failed: Strict pricing is enabled, but no configuration was found for provider='Pricing not configured', model='test_model' in flujo.toml.
2025-08-04 22:44:59,629 - flujo - WARNING - Step 'test_step' agent execution attempt 4 failed: Strict pricing is enabled, but no configuration was found for provider='Pricing not configured', model='test_model' in flujo.toml.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,629 - flujo - ERROR - Step 'test_step' agent failed after 4 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='Pricing not configured', model='test_model' in flujo.toml.
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='Pricing not configured', model='test_model' in flujo.toml.
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 3 failed: Strict pricing is enabled, but no configuration was found for provider='Pricing not configured', model='test_model' in flujo.toml.
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 4 failed: Strict pricing is enabled, but no configuration was found for provider='Pricing not configured', model='test_model' in flujo.toml.
ERROR    flujo:telemetry.py:54 Step 'test_step' agent failed after 4 attempts
_______ TestExecutorCoreFallbackLogic.test_successful_fallback_execution _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:1008: in test_successful_fallback_execution
    result = await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:761: in _execute_simple_step
    _detect_mock_objects(fallback_output)
flujo/application/core/ultra_executor.py:689: in _detect_mock_objects
    raise MockDetectionError(f"Step '{step.name}' returned a Mock object. This is usually due to an unconfigured mock in a test.")
E   flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,703 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:44:59,703 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,704 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
____ TestExecutorCoreFallbackIntegration.test_real_fallback_with_streaming _____
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_executor_core_fallback_integration.py:325: in test_real_fallback_with_streaming
    assert result.output == "fallback stream"
E   AssertionError: assert <async_generator object TestExecutorCoreFallbackIntegration.test_real_fallback_with_streaming.<locals>.StreamingAgent.stream at 0x114afca00> == 'fallback stream'
E    +  where <async_generator object TestExecutorCoreFallbackIntegration.test_real_fallback_with_streaming.<locals>.StreamingAgent.stream at 0x114afca00> = StepResult(name='stream_primary', output=<async_generator object TestExecutorCoreFallbackIntegration.test_real_fallback_with_streaming.<locals>.StreamingAgent.stream at 0x114afca00>, success=True, attempts=3, latency_s=0.0002477919915691018, token_counts=0, cost_usd=0.0, feedback='', branch_context=None, metadata_={'fallback_triggered': True, 'original_error': 'Agent execution failed with Exception: Primary failed'}, step_history=[]).output
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,814 - flujo - WARNING - Step 'stream_primary' agent execution attempt 1 failed: Primary failed
2025-08-04 22:44:59,814 - flujo - WARNING - Step 'stream_primary' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,814 - flujo - ERROR - Step 'stream_primary' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'stream_primary' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'stream_primary' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'stream_primary' agent failed after 2 attempts
________ TestExecutorCoreFallbackLogic.test_fallback_metric_accounting _________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:1185: in test_fallback_metric_accounting
    assert result.token_counts == 38  # Should be sum: (10+5) + (15+8) = 38
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert 24 == 38
E    +  where 24 = StepResult(name='primary_step', output='fallback success', success=True, attempts=2, latency_s=0.10039637496229262, token_counts=24, cost_usd=0.2, feedback='', branch_context=None, metadata_={'fallback_triggered': True, 'original_error': 'Validation failed: Validation failed'}, step_history=[]).token_counts
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,815 - flujo - INFO - Counting string output as 1 token for step 'primary_step': 'primary success'
Expected token counts: 38, Actual: 24
Primary token counts from mock: 15, Fallback token counts: 23
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,815 - flujo - ERROR - Step 'primary_step' validation failed: Validation failed
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'primary_step': 'primary success'
ERROR    flujo:telemetry.py:54 Step 'primary_step' validation failed: Validation failed
___ TestExecutorCoreFallbackLogic.test_fallback_execution_exception_handling ___
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:1369: in test_fallback_execution_exception_handling
    result = await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:755: in _execute_simple_step
    fallback_result = await self.execute(fallback_frame)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:2195: in _execute_mock_call
    raise effect
E   Exception: Fallback execution failed
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:44:59,845 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:44:59,845 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Fallback execution failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:44:59,845 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Fallback execution failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
_ TestExecutorCoreFunctionalEquivalence.test_functional_equivalence_plugin_steps _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:1882: in test_functional_equivalence_plugin_steps
    assert old_result == new_result == True, (
E   AssertionError: Plugin step classification mismatch: old=True, new=False
E   assert True == False
_ TestExecutorCoreFunctionalEquivalence.test_functional_equivalence_comprehensive_coverage _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core.py:2011: in test_functional_equivalence_comprehensive_coverage
    assert old_result == new_result, (
E   AssertionError: Basic test failed for <Mock name='plugin_step.name' id='4462333584'> (Mock): old=True, new=False
E   assert True == False
____ TestExecutorCoreConditionalStep.test_handle_conditional_step_signature ____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step.py:54: in test_handle_conditional_step_signature
    assert params == expected_params
E   AssertionError: assert ['step', 'dat..._setter', ...] == ['conditional...ntext_setter']
E     
E     At index 0 diff: 'step' != 'conditional_step'
E     Left contains one more item: 'fallback_depth'
E     
E     Full diff:
E       [
E     -     'conditional_step',
E     +     'step',
E           'data',
E           'context',
E           'resources',
E           'limits',
E           'context_setter',
E     +     'fallback_depth',
E       ]
_ TestExecutorCoreConditionalStep.test_handle_conditional_step_parameter_passing _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step.py:159: in test_handle_conditional_step_parameter_passing
    assert call_args[0][0] == mock_step  # step
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert ExecutionFrame(step=<Mock spec='Step' id='4474855376'>, data='test_data', context=<Mock id='4436917968'>, resources=<Mock id='4474858768'>, limits=UsageLimits(total_cost_usd_limit=10.0, total_tokens_limit=None), stream=False, on_chunk=None, breach_event=None, context_setter=<Mock id='4474849104'>, result=None, _fallback_depth=0) == <Mock spec='Step' id='4474855376'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,096 - flujo - INFO - Condition evaluated to branch key 'branch_a'
2025-08-04 22:45:00,096 - flujo - INFO - Executing branch for key 'branch_a'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'unittest.mock.Mock'>
[DEBUG] source_context type: <class 'unittest.mock.Mock'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: <Mock name='mock.model_dump()' id='4470368272'>
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,097 - flujo.utils.context - ERROR - Failed to merge context updates: 'Mock' object is not iterable
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Condition evaluated to branch key 'branch_a'
INFO     flujo:telemetry.py:54 Executing branch for key 'branch_a'
ERROR    flujo.utils.context:context.py:276 Failed to merge context updates: 'Mock' object is not iterable
_ TestExecutorCoreConditionalStep.test_handle_conditional_step_with_limits_and_context_setter _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step.py:216: in test_handle_conditional_step_with_limits_and_context_setter
    test_context_setter.assert_called_once()
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:902: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'mock' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,108 - flujo - INFO - Condition evaluated to branch key 'branch_a'
2025-08-04 22:45:00,108 - flujo - INFO - Executing branch for key 'branch_a'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'dict'>
[DEBUG] source_context type: <class 'dict'>
[DEBUG] excluded_fields: {'command_log'}
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,108 - flujo.utils.context - ERROR - Failed to merge context updates: 'dict' object has no attribute '__dict__'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Condition evaluated to branch key 'branch_a'
INFO     flujo:telemetry.py:54 Executing branch for key 'branch_a'
ERROR    flujo.utils.context:context.py:276 Failed to merge context updates: 'dict' object has no attribute '__dict__'
_ TestExecutorCoreConditionalStep.test_handle_conditional_step_integration_with_execute_complex_step _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step.py:248: in test_handle_conditional_step_integration_with_execute_complex_step
    result = await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
_ TestExecutorCoreConditionalStepDispatch.test_execute_complex_step_routes_conditionalstep_to_new_handler _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_dispatch.py:35: in test_execute_complex_step_routes_conditionalstep_to_new_handler
    result = await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
_ TestExecutorCoreConditionalStepDispatch.test_execute_complex_step_conditionalstep_parameter_passing _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_dispatch.py:72: in test_execute_complex_step_conditionalstep_parameter_passing
    await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
_ TestExecutorCoreConditionalStepDispatch.test_execute_complex_step_conditionalstep_no_legacy_import _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_dispatch.py:103: in test_execute_complex_step_conditionalstep_no_legacy_import
    result = await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
_ TestExecutorCoreConditionalStepDispatch.test_execute_complex_step_conditionalstep_error_propagation _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_dispatch.py:133: in test_execute_complex_step_conditionalstep_error_propagation
    await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'

During handling of the above exception, another exception occurred:
tests/application/core/test_executor_core_conditional_step_dispatch.py:132: in test_execute_complex_step_conditionalstep_error_propagation
    with pytest.raises(Exception, match="Test error"):
E   AssertionError: Regex pattern did not match.
E    Regex: 'Test error'
E    Input: "ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'"
_ TestExecutorCoreConditionalStepDispatch.test_execute_complex_step_conditionalstep_telemetry_logging _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_dispatch.py:157: in test_execute_complex_step_conditionalstep_telemetry_logging
    await executor_core._execute_complex_step(
E   TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
____ TestExecutorCoreConditionalStepLogic.test_branch_not_found_no_default _____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_logic.py:96: in test_branch_not_found_no_default
    assert "No branch found for key 'nonexistent_branch'" in result.feedback
E   assert "No branch found for key 'nonexistent_branch'" in "No branch matches condition 'nonexistent_branch' and no default branch provided"
E    +  where "No branch matches condition 'nonexistent_branch' and no default branch provided" = StepResult(name='test_conditional', output='test_data', success=False, attempts=1, latency_s=9.487499482929707e-05, token_counts=0, cost_usd=0.0, feedback="No branch matches condition 'nonexistent_branch' and no default branch provided", branch_context=None, metadata_={}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,167 - flujo - WARNING - No branch matches condition 'nonexistent_branch' and no default branch provided
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 No branch matches condition 'nonexistent_branch' and no default branch provided
___ TestExecutorCoreConditionalStepLogic.test_branch_output_mapper_exception ___
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_logic.py:237: in test_branch_output_mapper_exception
    assert "Branch output mapper raised an exception" in result.feedback
E   AssertionError: assert 'Branch output mapper raised an exception' in 'Error executing conditional logic or branch: Mapper failed'
E    +  where 'Error executing conditional logic or branch: Mapper failed' = StepResult(name='test_conditional', output=None, success=False, attempts=1, latency_s=0.0002668750239536166, token_counts=0, cost_usd=0.0, feedback='Error executing conditional logic or branch: Mapper failed', branch_context=None, metadata_={'executed_branch_key': 'branch_a'}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,189 - flujo - INFO - Condition evaluated to branch key 'branch_a'
2025-08-04 22:45:00,189 - flujo - INFO - Executing branch for key 'branch_a'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,189 - flujo - ERROR - Error in conditional step 'test_conditional': Mapper failed
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Condition evaluated to branch key 'branch_a'
INFO     flujo:telemetry.py:54 Executing branch for key 'branch_a'
ERROR    flujo:telemetry.py:54 Error in conditional step 'test_conditional': Mapper failed
________ TestExecutorCoreConditionalStepLogic.test_metrics_accumulation ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_logic.py:264: in test_metrics_accumulation
    assert result.latency_s == 1.5
E   assert 0.0005757910548709333 == 1.5
E    +  where 0.0005757910548709333 = StepResult(name='test_conditional', output='test_output', success=True, attempts=1, latency_s=0.0005757910548709333, token_counts=100, cost_usd=0.01, feedback="Branch 'branch_a' executed successfully", branch_context=PipelineContext(run_id='run_c4053cedd0ab4695b0a75bcaf7daf933', initial_prompt='test_data', scratchpad={}, hitl_history=[], command_log=[]), metadata_={'executed_branch_key': 'branch_a'}, step_history=[]).latency_s
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,196 - flujo - INFO - Condition evaluated to branch key 'branch_a'
2025-08-04 22:45:00,196 - flujo - INFO - Executing branch for key 'branch_a'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Condition evaluated to branch key 'branch_a'
INFO     flujo:telemetry.py:54 Executing branch for key 'branch_a'
__ TestExecutorCoreConditionalStepLogic.test_context_setter_called_on_success __
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_logic.py:309: in test_context_setter_called_on_success
    mock_context_setter.assert_called_once()
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:902: in assert_called_once
    raise AssertionError(msg)
E   AssertionError: Expected 'mock' to have been called once. Called 0 times.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,206 - flujo - INFO - Condition evaluated to branch key 'branch_a'
2025-08-04 22:45:00,206 - flujo - INFO - Executing branch for key 'branch_a'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'dict'>
[DEBUG] source_context type: <class 'dict'>
[DEBUG] excluded_fields: {'command_log'}
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,206 - flujo.utils.context - ERROR - Failed to merge context updates: 'dict' object has no attribute '__dict__'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Condition evaluated to branch key 'branch_a'
INFO     flujo:telemetry.py:54 Executing branch for key 'branch_a'
ERROR    flujo.utils.context:context.py:276 Failed to merge context updates: 'dict' object has no attribute '__dict__'
__ TestExecutorCoreConditionalStepLogic.test_branch_execution_with_resources ___
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_logic.py:415: in test_branch_execution_with_resources
    assert call_args[1]["resources"] == resources
           ^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'resources'
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,286 - flujo - INFO - Condition evaluated to branch key 'branch_a'
2025-08-04 22:45:00,286 - flujo - INFO - Executing branch for key 'branch_a'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Condition evaluated to branch key 'branch_a'
INFO     flujo:telemetry.py:54 Executing branch for key 'branch_a'
_________ TestErrorHandlingIntegration.test_error_recovery_integration _________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_executor_core_optimization_integration.py:480: in test_error_recovery_integration
    result = await error_handling_executor.execute(failing_step, test_data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/core/ultra_executor.py:564: in execute
    return await self._execute_simple_step(
flujo/application/core/ultra_executor.py:718: in _execute_simple_step
    primary_result = StepResult(
E   pydantic_core._pydantic_core.ValidationError: 1 validation error for StepResult
E   name
E     Input should be a valid string [type=string_type, input_value=<Mock name='mock.step.name' id='4614630864'>, input_type=Mock]
E       For further information visit https://errors.pydantic.dev/2.11/v/string_type
____ TestExecutorCoreConditionalStepLogic.test_branch_execution_with_limits ____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_conditional_step_logic.py:441: in test_branch_execution_with_limits
    assert call_args[1]["limits"] == limits
           ^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'limits'
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,293 - flujo - INFO - Condition evaluated to branch key 'branch_a'
2025-08-04 22:45:00,293 - flujo - INFO - Executing branch for key 'branch_a'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Condition evaluated to branch key 'branch_a'
INFO     flujo:telemetry.py:54 Executing branch for key 'branch_a'
________________________ test_pause_and_resume_in_loop _________________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_agentic_loop_recipe.py:101: in test_pause_and_resume_in_loop
    assert ctx.scratchpad["status"] == "paused"
E   AssertionError: assert 'failed' == 'paused'
E     
E     - paused
E     + failed
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === AgenticExplorationLoop
2025-08-04 22:45:00,313 - flujo - INFO - LoopStep 'AgenticExplorationLoop': Starting Iteration 1/10
2025-08-04 22:45:00,314 - flujo - WARNING - Step 'AgenticExplorationLoop' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,314 - flujo - ERROR - Step 'command_executor_step' failed with critical error: Need input
2025-08-04 22:45:00,314 - flujo - ERROR - Error in LoopStep 'AgenticExplorationLoop': Need input
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'AgenticExplorationLoop': Starting Iteration 1/10
ERROR    flujo:telemetry.py:54 Step 'command_executor_step' failed with critical error: Need input
ERROR    flujo:telemetry.py:54 Error in LoopStep 'AgenticExplorationLoop': Need input
WARNING  flujo:telemetry.py:54 Step 'AgenticExplorationLoop' failed. Halting pipeline execution.
________ TestErrorHandlingIntegration.test_circuit_breaker_integration _________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_executor_core_optimization_integration.py:520: in test_circuit_breaker_integration
    result = await error_handling_executor.execute(always_failing_step, test_data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/core/ultra_executor.py:564: in execute
    return await self._execute_simple_step(
flujo/application/core/ultra_executor.py:718: in _execute_simple_step
    primary_result = StepResult(
E   pydantic_core._pydantic_core.ValidationError: 1 validation error for StepResult
E   name
E     Input should be a valid string [type=string_type, input_value=<Mock name='mock.step.name' id='4652068304'>, input_type=Mock]
E       For further information visit https://errors.pydantic.dev/2.11/v/string_type
_______________________________ test_sync_resume _______________________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_agentic_loop_recipe.py:129: in test_sync_resume
    resumed = asyncio.run(run_agentic_loop_pipeline(pipeline, "goal", resume_from=paused))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:190: in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:118: in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:650: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
flujo/recipes/factories.py:433: in run_agentic_loop_pipeline
    result = await runner.resume_async(resume_from, human_input)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/runner.py:770: in resume_async
    raise OrchestratorError("Pipeline is not paused")
E   flujo.exceptions.OrchestratorError: Pipeline is not paused
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === AgenticExplorationLoop
2025-08-04 22:45:00,324 - flujo - INFO - LoopStep 'AgenticExplorationLoop': Starting Iteration 1/10
2025-08-04 22:45:00,324 - flujo - WARNING - Step 'AgenticExplorationLoop' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,324 - flujo - ERROR - Step 'command_executor_step' failed with critical error: Need input
2025-08-04 22:45:00,324 - flujo - ERROR - Error in LoopStep 'AgenticExplorationLoop': Need input
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'AgenticExplorationLoop': Starting Iteration 1/10
ERROR    flujo:telemetry.py:54 Step 'command_executor_step' failed with critical error: Need input
ERROR    flujo:telemetry.py:54 Error in LoopStep 'AgenticExplorationLoop': Need input
WARNING  flujo:telemetry.py:54 Step 'AgenticExplorationLoop' failed. Halting pipeline execution.
__________ TestExecutorCoreFallback.test_fallback_with_none_feedback ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:395: in test_fallback_with_none_feedback
    result = await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:761: in _execute_simple_step
    _detect_mock_objects(fallback_output)
flujo/application/core/ultra_executor.py:689: in _detect_mock_objects
    raise MockDetectionError(f"Step '{step.name}' returned a Mock object. This is usually due to an unconfigured mock in a test.")
E   flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,346 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,346 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,346 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
_____ TestExecutorCoreFallback.test_fallback_execution_exception_handling ______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:435: in test_fallback_execution_exception_handling
    result = await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:755: in _execute_simple_step
    fallback_result = await self.execute(fallback_frame)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/unittest/mock.py:2195: in _execute_mock_call
    raise effect
E   Exception: Fallback execution failed
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,396 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,396 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,396 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
___________ TestExecutorCoreFallback.test_fallback_with_usage_limits ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:476: in test_fallback_with_usage_limits
    result = await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:761: in _execute_simple_step
    _detect_mock_objects(fallback_output)
flujo/application/core/ultra_executor.py:689: in _detect_mock_objects
    raise MockDetectionError(f"Step '{step.name}' returned a Mock object. This is usually due to an unconfigured mock in a test.")
E   flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,501 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,501 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,501 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
_________ TestExecutorCoreFallback.test_fallback_metadata_preservation _________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:613: in test_fallback_metadata_preservation
    result = await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:761: in _execute_simple_step
    _detect_mock_objects(fallback_output)
flujo/application/core/ultra_executor.py:689: in _detect_mock_objects
    raise MockDetectionError(f"Step '{step.name}' returned a Mock object. This is usually due to an unconfigured mock in a test.")
E   flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,557 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,557 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,557 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
__________________ test_parallel_overwrite_multi_branch_order __________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_step.py:155: in test_parallel_overwrite_multi_branch_order
    assert result.final_pipeline_context.scratchpad["v"] == 2
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'v'
_______ TestExecutorCoreFallback.test_fallback_with_critical_exceptions ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:678: in test_fallback_with_critical_exceptions
    await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:761: in _execute_simple_step
    _detect_mock_objects(fallback_output)
flujo/application/core/ultra_executor.py:689: in _detect_mock_objects
    raise MockDetectionError(f"Step '{step.name}' returned a Mock object. This is usually due to an unconfigured mock in a test.")
E   flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,656 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Cost limit exceeded
2025-08-04 22:45:00,656 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Cost limit exceeded
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,656 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Cost limit exceeded
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Cost limit exceeded
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
______ TestExecutorCoreFallback.test_fallback_with_pricing_not_configured ______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:705: in test_fallback_with_pricing_not_configured
    await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:761: in _execute_simple_step
    _detect_mock_objects(fallback_output)
flujo/application/core/ultra_executor.py:689: in _detect_mock_objects
    raise MockDetectionError(f"Step '{step.name}' returned a Mock object. This is usually due to an unconfigured mock in a test.")
E   flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,701 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='gpt-4' in flujo.toml.
2025-08-04 22:45:00,702 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='gpt-4' in flujo.toml.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,702 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='gpt-4' in flujo.toml.
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='gpt-4' in flujo.toml.
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
_______ TestExecutorCoreFallback.test_fallback_with_missing_agent_error ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:730: in test_fallback_with_missing_agent_error
    await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:761: in _execute_simple_step
    _detect_mock_objects(fallback_output)
flujo/application/core/ultra_executor.py:689: in _detect_mock_objects
    raise MockDetectionError(f"Step '{step.name}' returned a Mock object. This is usually due to an unconfigured mock in a test.")
E   flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
_______ TestExecutorCoreFallback.test_fallback_with_usage_meter_tracking _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:1153: in test_fallback_with_usage_meter_tracking
    assert executor_core._usage_meter.add.call_count == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = <AsyncMock name='mock.add' id='4473468240'>.call_count
E    +    where <AsyncMock name='mock.add' id='4473468240'> = <AsyncMock id='4474084496'>.add
E    +      where <AsyncMock id='4474084496'> = <flujo.application.core.ultra_executor.ExecutorCore object at 0x10aa9b850>._usage_meter
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,883 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
________ TestExecutorCoreFallback.test_fallback_with_processor_pipeline ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:1224: in test_fallback_with_processor_pipeline
    assert result.success is True
E   AssertionError: assert False is True
E    +  where False = StepResult(name='primary_step', output=None, success=False, attempts=4, latency_s=0.00036854203790426254, token_counts=0, cost_usd=0.0, feedback='Original error: Agent execution failed with Exception: Primary failed; Fallback error: Agent execution failed with Exception: Primary failed', branch_context=None, metadata_={'fallback_triggered': True}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,896 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,896 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Primary failed
2025-08-04 22:45:00,896 - flujo - WARNING - Step 'fallback_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,896 - flujo - WARNING - Step 'fallback_step' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,896 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
2025-08-04 22:45:00,896 - flujo - ERROR - Step 'fallback_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'fallback_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'fallback_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'fallback_step' agent failed after 2 attempts
__________ TestExecutorCoreFallback.test_fallback_with_plugin_runner ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:1291: in test_fallback_with_plugin_runner
    assert result.success is True
E   AssertionError: assert False is True
E    +  where False = StepResult(name='primary_step', output=None, success=False, attempts=4, latency_s=0.0003055409761145711, token_counts=0, cost_usd=0.0, feedback='Original error: Agent execution failed with Exception: Primary failed; Fallback error: Agent execution failed with Exception: Primary failed', branch_context=None, metadata_={'fallback_triggered': True}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,909 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,909 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Primary failed
2025-08-04 22:45:00,910 - flujo - WARNING - Step 'fallback_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,910 - flujo - WARNING - Step 'fallback_step' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,909 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
2025-08-04 22:45:00,910 - flujo - ERROR - Step 'fallback_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'fallback_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'fallback_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'fallback_step' agent failed after 2 attempts
________________ test_as_step_state_persistence_and_resumption _________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_as_step_state_persistence.py:62: in test_as_step_state_persistence_and_resumption
    assert saved["current_step_index"] == 1
E   assert 0 == 1
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,723 - flujo - INFO - Logfire telemetry is disabled or failed to initialize. Using standard Python logging.
2025-08-04 22:45:00,925 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_as_step_state_persistence0/state.db
2025-08-04 22:45:00,929 - flujo - INFO - Saved state for run_id=inner_run
2025-08-04 22:45:00,939 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_as_step_state_persistence0/state.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_as_step_state_persistence0/state.db
__________ TestExecutorCoreFallback.test_fallback_with_cache_backend ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:1319: in test_fallback_with_cache_backend
    result = await executor_core._execute_simple_step(
flujo/application/core/ultra_executor.py:755: in _execute_simple_step
    fallback_result = await self.execute(fallback_frame)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/core/ultra_executor.py:564: in execute
    return await self._execute_simple_step(
flujo/application/core/ultra_executor.py:755: in _execute_simple_step
    fallback_result = await self.execute(fallback_frame)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/core/ultra_executor.py:564: in execute
    return await self._execute_simple_step(
flujo/application/core/ultra_executor.py:718: in _execute_simple_step
    primary_result = StepResult(
E   pydantic_core._pydantic_core.ValidationError: 1 validation error for StepResult
E   name
E     Input should be a valid string [type=string_type, input_value=<Mock name='mock.fallback...p.name' id='4474187152'>, input_type=Mock]
E       For further information visit https://errors.pydantic.dev/2.11/v/string_type
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:00,924 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,924 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Primary failed
2025-08-04 22:45:00,925 - flujo - WARNING - Step 'fallback_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:00,925 - flujo - WARNING - Step 'fallback_step' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:00,924 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
2025-08-04 22:45:00,925 - flujo - ERROR - Step 'fallback_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'fallback_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'fallback_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'fallback_step' agent failed after 2 attempts
_____________________ test_caching_pipeline_speed_and_hits _____________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_caching_and_fallbacks.py:61: in test_caching_pipeline_speed_and_hits
    assert first_meta is None or "cache_hit" not in first_meta
E   AssertionError: assert ({'cache_hit': True} is None or 'cache_hit' not in {'cache_hit': True})
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,039 - flujo - INFO - Counting string output as 1 token for step 'solution': 'ok'
2025-08-04 22:45:01,039 - flujo - INFO - Counting string output as 1 token for step 'passthrough': 'ok'
2025-08-04 22:45:01,041 - flujo - INFO - Counting string output as 1 token for step 'passthrough': 'ok'
2025-08-04 22:45:01,041 - flujo - INFO - Counting string output as 1 token for step 'solution': 'ok'
2025-08-04 22:45:01,042 - flujo - INFO - Counting string output as 1 token for step 'passthrough': 'ok'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'solution': 'ok'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'passthrough': 'ok'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'passthrough': 'ok'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'solution': 'ok'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'passthrough': 'ok'
____________ TestExecutorCoreFallback.test_fallback_with_telemetry _____________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:1397: in test_fallback_with_telemetry
    assert result.success is True
E   AssertionError: assert False is True
E    +  where False = StepResult(name='primary_step', output=None, success=False, attempts=4, latency_s=0.00025141704827547073, token_counts=0, cost_usd=0.0, feedback='Original error: Agent execution failed with Exception: Primary failed; Fallback error: Agent execution failed with Exception: Primary failed', branch_context=None, metadata_={'fallback_triggered': True}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,044 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:01,044 - flujo - WARNING - Step 'primary_step' agent execution attempt 2 failed: Primary failed
2025-08-04 22:45:01,044 - flujo - WARNING - Step 'fallback_step' agent execution attempt 1 failed: Primary failed
2025-08-04 22:45:01,044 - flujo - WARNING - Step 'fallback_step' agent execution attempt 2 failed: Primary failed
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:01,044 - flujo - ERROR - Step 'primary_step' agent failed after 2 attempts
2025-08-04 22:45:01,044 - flujo - ERROR - Step 'fallback_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'fallback_step' agent execution attempt 1 failed: Primary failed
WARNING  flujo:telemetry.py:54 Step 'fallback_step' agent execution attempt 2 failed: Primary failed
ERROR    flujo:telemetry.py:54 Step 'fallback_step' agent failed after 2 attempts
_______ TestExecutorCoreFallback.test_fallback_integration_real_pipeline _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:1448: in test_fallback_integration_real_pipeline
    assert result.output == "fallback success"
E   AssertionError: assert 'primary success' == 'fallback success'
E     
E     - fallback success
E     + primary success
----------------------------- Captured stdout call -----------------------------
ðŸ” PrimaryAgent.run called with data: test data
ðŸ” PrimaryAgent.run returning: primary success
2025-08-04 22:45:01,055 - flujo - INFO - Counting string output as 1 token for step 'primary_step': 'primary success'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'primary_step': 'primary success'
___________ TestExecutorCoreFallback.test_fallback_on_plugin_failure ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:1520: in test_fallback_on_plugin_failure
    print(f"ðŸ” Result.metadata: {result.metadata}")
                                ^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/pydantic/main.py:991: in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E   AttributeError: 'StepResult' object has no attribute 'metadata'
----------------------------- Captured stdout call -----------------------------
ðŸ” primary_step.agent: <test_executor_core_fallback.TestExecutorCoreFallback.test_fallback_on_plugin_failure.<locals>.PrimaryAgent object at 0x10a6cb050>
ðŸ” primary_step.agent type: <class 'test_executor_core_fallback.TestExecutorCoreFallback.test_fallback_on_plugin_failure.<locals>.PrimaryAgent'>
ðŸ” hasattr(primary_step.agent, 'run'): True
ðŸ” primary_step.agent.run: <bound method TestExecutorCoreFallback.test_fallback_on_plugin_failure.<locals>.PrimaryAgent.run of <test_executor_core_fallback.TestExecutorCoreFallback.test_fallback_on_plugin_failure.<locals>.PrimaryAgent object at 0x10a6cb050>>
ðŸ” fallback_step.agent: <test_executor_core_fallback.TestExecutorCoreFallback.test_fallback_on_plugin_failure.<locals>.FallbackAgent object at 0x10a6cb4d0>
ðŸ” fallback_step.agent type: <class 'test_executor_core_fallback.TestExecutorCoreFallback.test_fallback_on_plugin_failure.<locals>.FallbackAgent'>
ðŸ” hasattr(fallback_step.agent, 'run'): True
ðŸ” fallback_step.agent.run: <bound method TestExecutorCoreFallback.test_fallback_on_plugin_failure.<locals>.FallbackAgent.run of <test_executor_core_fallback.TestExecutorCoreFallback.test_fallback_on_plugin_failure.<locals>.FallbackAgent object at 0x10a6cb4d0>>
2025-08-04 22:45:01,064 - flujo - INFO - Counting string output as 1 token for step 'primary_step': 'primary success'
ðŸ” Result: name='primary_step' output='primary success' success=True attempts=1 latency_s=0.0001716670230962336 token_counts=1 cost_usd=0.0 feedback=None branch_context=None metadata_={} step_history=[]
ðŸ” Result.success: True
ðŸ” Result.output: primary success
ðŸ” Result.feedback: None
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'primary_step': 'primary success'
_________ TestExecutorCoreFallback.test_fallback_on_validator_failure __________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:1578: in test_fallback_on_validator_failure
    assert result.output == "fallback success"
E   AssertionError: assert 'processed: primary success' == 'fallback success'
E     
E     - fallback success
E     + processed: primary success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,083 - flujo - INFO - Counting string output as 1 token for step 'primary_step': 'primary success'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'primary_step': 'primary success'
_______ TestExecutorCoreFallback.test_fallback_on_complex_failure_chain ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/application/core/test_executor_core_fallback.py:1635: in test_fallback_on_complex_failure_chain
    assert "Primary agent failed" in result.feedback
E   AssertionError: assert 'Primary agent failed' in ''
E    +  where '' = StepResult(name='primary_step', output='fallback success', success=True, attempts=2, latency_s=0.0002532079815864563, token_counts=1, cost_usd=0.0, feedback='', branch_context=None, metadata_={'fallback_triggered': True, 'original_error': 'Agent execution failed with Exception: Primary agent failed'}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,093 - flujo - WARNING - Step 'primary_step' agent execution attempt 1 failed: Primary agent failed
2025-08-04 22:45:01,093 - flujo - INFO - Counting string output as 1 token for step 'fallback_step': 'fallback success'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:01,093 - flujo - ERROR - Step 'primary_step' agent failed after 1 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'primary_step' agent execution attempt 1 failed: Primary agent failed
ERROR    flujo:telemetry.py:54 Step 'primary_step' agent failed after 1 attempts
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'fallback_step': 'fallback success'
______________ test_redirect_loop_detected_with_unhashable_agents ______________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_redirect_loop_unhashable.py:41: in test_redirect_loop_detected_with_unhashable_agents
    with pytest.raises(InfiniteRedirectError):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.InfiniteRedirectError'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,106 - flujo - INFO - Counting string output as 1 token for step 'loop': 'a1'
2025-08-04 22:45:01,106 - flujo - INFO - Step 'loop' redirecting to agent: <tests.integration.test_redirect_loop_unhashable.UnhashableAgent object at 0x10ab14fd0>
2025-08-04 22:45:01,106 - flujo - INFO - Counting string output as 1 token for step 'loop': 'a2'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'loop': 'a1'
INFO     flujo:telemetry.py:54 Step 'loop' redirecting to agent: <tests.integration.test_redirect_loop_unhashable.UnhashableAgent object at 0x10ab14fd0>
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'loop': 'a2'
____________ test_refine_until_with_context_updates_error_handling _____________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_refine_until_with_context_updates.py:146: in test_refine_until_with_context_updates_error_handling
    assert (
E   AssertionError: assert 'loop exited by condition, but last iteration body failed' in 'loop terminated after reaching max_loops (5)'
E    +  where 'loop terminated after reaching max_loops (5)' = <built-in method lower of str object at 0x10aaa60d0>()
E    +    where <built-in method lower of str object at 0x10aaa60d0> = 'Loop terminated after reaching max_loops (5)'.lower
E    +      where 'Loop terminated after reaching max_loops (5)' = StepResult(name='error_refine', output='refined_content_6', success=False, attempts=5, latency_s=0.007084375014528632, token_counts=10, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (5)', branch_context=RefineContext(run_id='run_e3e13301ae15417582ba3eaf2baf03cf', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], refinement_count=6, total_iterations=11, refinement_history=['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5', 'refinement_6'], current_quality=0.16999999999999998, best_quality=0.16999999999999998, refinement_data={}), metadata_={'iterations': 5, 'exit_reason': 'max_iterations'}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === error_refine
2025-08-04 22:45:01,134 - flujo - INFO - LoopStep 'error_refine': Starting Iteration 1/5
2025-08-04 22:45:01,134 - flujo - INFO - Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_1'
2025-08-04 22:45:01,135 - flujo - INFO - Counting string output as 1 token for step '_capture_artifact': 'refined_content_1'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 1, 'total_iterations': 2, 'refinement_history': ['refinement_1'], 'current_quality': 0.07, 'best_quality': 0.07, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 2
[DEBUG] actual_source_value: 2
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1']
[DEBUG] actual_source_value: ['refinement_1']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.07
[DEBUG] actual_source_value: 0.07
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.07
[DEBUG] actual_source_value: 0.07
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 1, 'total_iterations': 2, 'refinement_history': ['refinement_1'], 'current_quality': 0.07, 'best_quality': 0.07, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 2
[DEBUG] actual_source_value: 2
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1']
[DEBUG] actual_source_value: ['refinement_1']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.07
[DEBUG] actual_source_value: 0.07
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.07
[DEBUG] actual_source_value: 0.07
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 5
2025-08-04 22:45:01,135 - flujo - INFO - LoopStep 'error_refine': Starting Iteration 2/5
2025-08-04 22:45:01,136 - flujo - WARNING - Step 'refine_with_error_step' agent execution attempt 1 failed: Intentional failure in refinement 2
2025-08-04 22:45:01,136 - flujo - INFO - Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_3'
2025-08-04 22:45:01,136 - flujo - INFO - Counting string output as 1 token for step '_capture_artifact': 'refined_content_3'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 3, 'total_iterations': 5, 'refinement_history': ['refinement_1', 'refinement_2', 'refinement_3'], 'current_quality': 0.11, 'best_quality': 0.11, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 3
[DEBUG] actual_source_value: 3
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 5
[DEBUG] actual_source_value: 5
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1', 'refinement_2', 'refinement_3']
[DEBUG] actual_source_value: ['refinement_1', 'refinement_2', 'refinement_3']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.11
[DEBUG] actual_source_value: 0.11
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.11
[DEBUG] actual_source_value: 0.11
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 3, 'total_iterations': 5, 'refinement_history': ['refinement_1', 'refinement_2', 'refinement_3'], 'current_quality': 0.11, 'best_quality': 0.11, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 3
[DEBUG] actual_source_value: 3
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 5
[DEBUG] actual_source_value: 5
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1', 'refinement_2', 'refinement_3']
[DEBUG] actual_source_value: ['refinement_1', 'refinement_2', 'refinement_3']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.11
[DEBUG] actual_source_value: 0.11
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.11
[DEBUG] actual_source_value: 0.11
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 2: Checking condition - limits: False, iteration_count: 2, max_iterations: 5
2025-08-04 22:45:01,137 - flujo - INFO - LoopStep 'error_refine': Starting Iteration 3/5
2025-08-04 22:45:01,137 - flujo - INFO - Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_4'
2025-08-04 22:45:01,137 - flujo - INFO - Counting string output as 1 token for step '_capture_artifact': 'refined_content_4'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 4, 'total_iterations': 7, 'refinement_history': ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4'], 'current_quality': 0.13, 'best_quality': 0.13, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 4
[DEBUG] actual_source_value: 4
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 7
[DEBUG] actual_source_value: 7
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4']
[DEBUG] actual_source_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.13
[DEBUG] actual_source_value: 0.13
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.13
[DEBUG] actual_source_value: 0.13
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 4, 'total_iterations': 7, 'refinement_history': ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4'], 'current_quality': 0.13, 'best_quality': 0.13, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 4
[DEBUG] actual_source_value: 4
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 7
[DEBUG] actual_source_value: 7
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4']
[DEBUG] actual_source_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.13
[DEBUG] actual_source_value: 0.13
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.13
[DEBUG] actual_source_value: 0.13
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 3: Checking condition - limits: False, iteration_count: 3, max_iterations: 5
2025-08-04 22:45:01,138 - flujo - INFO - LoopStep 'error_refine': Starting Iteration 4/5
2025-08-04 22:45:01,139 - flujo - INFO - Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_5'
2025-08-04 22:45:01,139 - flujo - INFO - Counting string output as 1 token for step '_capture_artifact': 'refined_content_5'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 5, 'total_iterations': 9, 'refinement_history': ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5'], 'current_quality': 0.15000000000000002, 'best_quality': 0.15000000000000002, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 5
[DEBUG] actual_source_value: 5
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 9
[DEBUG] actual_source_value: 9
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5']
[DEBUG] actual_source_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.15000000000000002
[DEBUG] actual_source_value: 0.15000000000000002
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.15000000000000002
[DEBUG] actual_source_value: 0.15000000000000002
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 5, 'total_iterations': 9, 'refinement_history': ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5'], 'current_quality': 0.15000000000000002, 'best_quality': 0.15000000000000002, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 5
[DEBUG] actual_source_value: 5
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 9
[DEBUG] actual_source_value: 9
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5']
[DEBUG] actual_source_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.15000000000000002
[DEBUG] actual_source_value: 0.15000000000000002
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.15000000000000002
[DEBUG] actual_source_value: 0.15000000000000002
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 4: Checking condition - limits: False, iteration_count: 4, max_iterations: 5
2025-08-04 22:45:01,140 - flujo - INFO - LoopStep 'error_refine': Starting Iteration 5/5
2025-08-04 22:45:01,140 - flujo - INFO - Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_6'
2025-08-04 22:45:01,140 - flujo - INFO - Counting string output as 1 token for step '_capture_artifact': 'refined_content_6'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 6, 'total_iterations': 11, 'refinement_history': ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5', 'refinement_6'], 'current_quality': 0.16999999999999998, 'best_quality': 0.16999999999999998, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 6
[DEBUG] actual_source_value: 6
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 11
[DEBUG] actual_source_value: 11
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5', 'refinement_6']
[DEBUG] actual_source_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5', 'refinement_6']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.16999999999999998
[DEBUG] actual_source_value: 0.16999999999999998
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.16999999999999998
[DEBUG] actual_source_value: 0.16999999999999998
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_e3e13301ae15417582ba3eaf2baf03cf', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 6, 'total_iterations': 11, 'refinement_history': ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5', 'refinement_6'], 'current_quality': 0.16999999999999998, 'best_quality': 0.16999999999999998, 'refinement_data': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] actual_source_value: run_e3e13301ae15417582ba3eaf2baf03cf
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 6
[DEBUG] actual_source_value: 6
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 11
[DEBUG] actual_source_value: 11
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5', 'refinement_6']
[DEBUG] actual_source_value: ['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5', 'refinement_6']
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.16999999999999998
[DEBUG] actual_source_value: 0.16999999999999998
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.16999999999999998
[DEBUG] actual_source_value: 0.16999999999999998
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 5: Checking condition - limits: False, iteration_count: 5, max_iterations: 5
2025-08-04 22:45:01,141 - flujo - WARNING - Step 'error_refine' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'error_refine': Starting Iteration 1/5
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_1'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step '_capture_artifact': 'refined_content_1'
INFO     flujo:telemetry.py:54 LoopStep 'error_refine': Starting Iteration 2/5
WARNING  flujo:telemetry.py:54 Step 'refine_with_error_step' agent execution attempt 1 failed: Intentional failure in refinement 2
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_3'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step '_capture_artifact': 'refined_content_3'
INFO     flujo:telemetry.py:54 LoopStep 'error_refine': Starting Iteration 3/5
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_4'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step '_capture_artifact': 'refined_content_4'
INFO     flujo:telemetry.py:54 LoopStep 'error_refine': Starting Iteration 4/5
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_5'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step '_capture_artifact': 'refined_content_5'
INFO     flujo:telemetry.py:54 LoopStep 'error_refine': Starting Iteration 5/5
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'refine_with_error_step': 'refined_content_6'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step '_capture_artifact': 'refined_content_6'
WARNING  flujo:telemetry.py:54 Step 'error_refine' failed. Halting pipeline execution.
__________ test_refine_until_with_context_updates_metadata_conflicts ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_refine_until_with_context_updates.py:350: in test_refine_until_with_context_updates_metadata_conflicts
    assert "reached max_loops" in result.step_history[-1].feedback.lower()
E   AssertionError: assert 'reached max_loops' in 'loop terminated after reaching max_loops (3)'
E    +  where 'loop terminated after reaching max_loops (3)' = <built-in method lower of str object at 0x10aa14c30>()
E    +    where <built-in method lower of str object at 0x10aa14c30> = 'Loop terminated after reaching max_loops (3)'.lower
E    +      where 'Loop terminated after reaching max_loops (3)' = StepResult(name='metadata_refine', output='metadata_content_3', success=False, attempts=3, latency_s=0.0046120419865474105, token_counts=6, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (3)', branch_context=RefineContext(run_id='run_cc9f76f7fa414399836d222c5dee8764', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], refinement_count=3, total_iterations=6, refinement_history=[], current_quality=0.11, best_quality=0.11, refinement_data={'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}, 'metadata_3': {'refine_index': 3, 'refine_iteration': 5, 'refine_metadata': {'iteration': 3, 'timestamp': 'now', 'data': 'refinement_3'}}}), metadata_={'iterations': 3, 'exit_reason': 'max_iterations'}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === metadata_refine
2025-08-04 22:45:01,169 - flujo - INFO - LoopStep 'metadata_refine': Starting Iteration 1/3
2025-08-04 22:45:01,170 - flujo - INFO - Counting string output as 1 token for step 'metadata_generator_step': 'metadata_content_1'
2025-08-04 22:45:01,170 - flujo - INFO - Counting string output as 1 token for step '_capture_artifact': 'metadata_content_1'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_cc9f76f7fa414399836d222c5dee8764', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 1, 'total_iterations': 2, 'refinement_history': [], 'current_quality': 0.07, 'best_quality': 0.07, 'refinement_data': {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] actual_source_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 2
[DEBUG] actual_source_value: 2
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.07
[DEBUG] actual_source_value: 0.07
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.07
[DEBUG] actual_source_value: 0.07
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}}
[DEBUG] actual_source_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_cc9f76f7fa414399836d222c5dee8764', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 1, 'total_iterations': 2, 'refinement_history': [], 'current_quality': 0.07, 'best_quality': 0.07, 'refinement_data': {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] actual_source_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 2
[DEBUG] actual_source_value: 2
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.07
[DEBUG] actual_source_value: 0.07
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.07
[DEBUG] actual_source_value: 0.07
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}}
[DEBUG] actual_source_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 3
2025-08-04 22:45:01,171 - flujo - INFO - LoopStep 'metadata_refine': Starting Iteration 2/3
2025-08-04 22:45:01,171 - flujo - INFO - Counting string output as 1 token for step 'metadata_generator_step': 'metadata_content_2'
2025-08-04 22:45:01,171 - flujo - INFO - Counting string output as 1 token for step '_capture_artifact': 'metadata_content_2'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_cc9f76f7fa414399836d222c5dee8764', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 2, 'total_iterations': 4, 'refinement_history': [], 'current_quality': 0.09, 'best_quality': 0.09, 'refinement_data': {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] actual_source_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 2
[DEBUG] actual_source_value: 2
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 4
[DEBUG] actual_source_value: 4
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.09
[DEBUG] actual_source_value: 0.09
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.09
[DEBUG] actual_source_value: 0.09
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}}
[DEBUG] actual_source_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_cc9f76f7fa414399836d222c5dee8764', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 2, 'total_iterations': 4, 'refinement_history': [], 'current_quality': 0.09, 'best_quality': 0.09, 'refinement_data': {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] actual_source_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 2
[DEBUG] actual_source_value: 2
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 4
[DEBUG] actual_source_value: 4
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.09
[DEBUG] actual_source_value: 0.09
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.09
[DEBUG] actual_source_value: 0.09
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}}
[DEBUG] actual_source_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 2: Checking condition - limits: False, iteration_count: 2, max_iterations: 3
2025-08-04 22:45:01,172 - flujo - INFO - LoopStep 'metadata_refine': Starting Iteration 3/3
2025-08-04 22:45:01,172 - flujo - INFO - Counting string output as 1 token for step 'metadata_generator_step': 'metadata_content_3'
2025-08-04 22:45:01,173 - flujo - INFO - Counting string output as 1 token for step '_capture_artifact': 'metadata_content_3'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_cc9f76f7fa414399836d222c5dee8764', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 3, 'total_iterations': 6, 'refinement_history': [], 'current_quality': 0.11, 'best_quality': 0.11, 'refinement_data': {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}, 'metadata_3': {'refine_index': 3, 'refine_iteration': 5, 'refine_metadata': {'iteration': 3, 'timestamp': 'now', 'data': 'refinement_3'}}}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] actual_source_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 3
[DEBUG] actual_source_value: 3
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 6
[DEBUG] actual_source_value: 6
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.11
[DEBUG] actual_source_value: 0.11
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.11
[DEBUG] actual_source_value: 0.11
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}, 'metadata_3': {'refine_index': 3, 'refine_iteration': 5, 'refine_metadata': {'iteration': 3, 'timestamp': 'now', 'data': 'refinement_3'}}}
[DEBUG] actual_source_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}, 'metadata_3': {'refine_index': 3, 'refine_iteration': 5, 'refine_metadata': {'iteration': 3, 'timestamp': 'now', 'data': 'refinement_3'}}}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] source_context type: <class 'tests.integration.test_refine_until_with_context_updates.RefineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_cc9f76f7fa414399836d222c5dee8764', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_count': 3, 'total_iterations': 6, 'refinement_history': [], 'current_quality': 0.11, 'best_quality': 0.11, 'refinement_data': {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}, 'metadata_3': {'refine_index': 3, 'refine_iteration': 5, 'refine_metadata': {'iteration': 3, 'timestamp': 'now', 'data': 'refinement_3'}}}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] actual_source_value: run_cc9f76f7fa414399836d222c5dee8764
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_count
[DEBUG] current_value: 3
[DEBUG] actual_source_value: 3
[DEBUG] Field unchanged: refinement_count
[DEBUG] Processing field: total_iterations
[DEBUG] current_value: 6
[DEBUG] actual_source_value: 6
[DEBUG] Field unchanged: total_iterations
[DEBUG] Processing field: refinement_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: refinement_history
[DEBUG] No new items to add to list field: refinement_history
[DEBUG] Processing field: current_quality
[DEBUG] current_value: 0.11
[DEBUG] actual_source_value: 0.11
[DEBUG] Field unchanged: current_quality
[DEBUG] Processing field: best_quality
[DEBUG] current_value: 0.11
[DEBUG] actual_source_value: 0.11
[DEBUG] Field unchanged: best_quality
[DEBUG] Processing field: refinement_data
[DEBUG] current_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}, 'metadata_3': {'refine_index': 3, 'refine_iteration': 5, 'refine_metadata': {'iteration': 3, 'timestamp': 'now', 'data': 'refinement_3'}}}
[DEBUG] actual_source_value: {'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}, 'metadata_3': {'refine_index': 3, 'refine_iteration': 5, 'refine_metadata': {'iteration': 3, 'timestamp': 'now', 'data': 'refinement_3'}}}
[DEBUG] Merging dictionaries for field: refinement_data
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 3: Checking condition - limits: False, iteration_count: 3, max_iterations: 3
2025-08-04 22:45:01,174 - flujo - WARNING - Step 'metadata_refine' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'metadata_refine': Starting Iteration 1/3
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'metadata_generator_step': 'metadata_content_1'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step '_capture_artifact': 'metadata_content_1'
INFO     flujo:telemetry.py:54 LoopStep 'metadata_refine': Starting Iteration 2/3
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'metadata_generator_step': 'metadata_content_2'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step '_capture_artifact': 'metadata_content_2'
INFO     flujo:telemetry.py:54 LoopStep 'metadata_refine': Starting Iteration 3/3
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'metadata_generator_step': 'metadata_content_3'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step '_capture_artifact': 'metadata_content_3'
WARNING  flujo:telemetry.py:54 Step 'metadata_refine' failed. Halting pipeline execution.
_ TestBug2FixParallelStepRaceCondition.test_fix_parallel_steps_with_atomic_usage_tracking _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_cost_tracking_bug_fixes.py:165: in test_fix_parallel_steps_with_atomic_usage_tracking
    assert any(results)  # At least one should detect the breach
    ^^^^^^^^^^^^^^^^^^^
E   assert False
E    +  where False = any([None, None, None, None, None])
____________________ test_context_include_keys_optimization ____________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_step_enhancements.py:135: in test_context_include_keys_optimization
    result_selective = await gather_result(
flujo/testing/utils.py:84: in gather_result
    async for result in runner.run_async(data, **kwargs):
flujo/application/runner.py:582: in run_async
    async for chunk in self._execute_steps(
flujo/application/runner.py:413: in _execute_steps
    async for item in execution_manager.execute_steps(
flujo/application/core/execution_manager.py:155: in execute_steps
    async for item in self.step_coordinator.execute_step(
flujo/application/core/step_coordinator.py:115: in execute_step
    step_result = await backend.execute_step(request)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/infra/backends.py:53: in execute_step
    return await self._executor.execute(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/core/ultra_executor.py:539: in execute
    return await self._handle_parallel_step(
flujo/application/core/ultra_executor.py:1705: in _handle_parallel_step
    branch_context = type(context)(initial_prompt=context.initial_prompt)
                                                  ^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/pydantic/main.py:991: in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E   AttributeError: 'LargeContext' object has no attribute 'initial_prompt'
_ TestBug2ParallelStepRaceCondition.test_parallel_steps_with_atomic_usage_tracking _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_cost_tracking_critical_bugs.py:252: in test_parallel_steps_with_atomic_usage_tracking
    assert any(results)  # At least one should detect the breach
    ^^^^^^^^^^^^^^^^^^^
E   assert False
E    +  where False = any([None, None, None, None, None])
_____________________ test_context_include_keys_isolation ______________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_step_enhancements.py:171: in test_context_include_keys_isolation
    result = await gather_result(runner, "input", initial_context_data=context.model_dump())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/testing/utils.py:84: in gather_result
    async for result in runner.run_async(data, **kwargs):
flujo/application/runner.py:582: in run_async
    async for chunk in self._execute_steps(
flujo/application/runner.py:413: in _execute_steps
    async for item in execution_manager.execute_steps(
flujo/application/core/execution_manager.py:155: in execute_steps
    async for item in self.step_coordinator.execute_step(
flujo/application/core/step_coordinator.py:115: in execute_step
    step_result = await backend.execute_step(request)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/infra/backends.py:53: in execute_step
    return await self._executor.execute(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/core/ultra_executor.py:539: in execute
    return await self._handle_parallel_step(
flujo/application/core/ultra_executor.py:1705: in _handle_parallel_step
    branch_context = type(context)(initial_prompt=context.initial_prompt)
                                                  ^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/pydantic/main.py:991: in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E   AttributeError: 'LargeContext' object has no attribute 'initial_prompt'
______ TestStrictPricingModeIntegration.test_strict_mode_on_failure_case _______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_cost_tracking_integration.py:307: in test_strict_mode_on_failure_case
    with pytest.raises(PricingNotConfiguredError) as exc_info:
E   Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,367 - flujo - INFO - Extracted tokens for step 'test_step': prompt=100, completion=50
2025-08-04 22:45:01,367 - flujo - INFO - Extracted model ID for step 'test_step' from 'model_id': openai:gpt-4o
2025-08-04 22:45:01,367 - flujo - WARNING - Step 'test_step' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='gpt-4o' in flujo.toml.
2025-08-04 22:45:01,367 - flujo - INFO - Extracted tokens for step 'test_step': prompt=100, completion=50
2025-08-04 22:45:01,367 - flujo - WARNING - Step 'test_step' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='gpt-4o' in flujo.toml.
2025-08-04 22:45:01,367 - flujo - WARNING - Step 'test_step' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:01,367 - flujo - ERROR - Step 'test_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Extracted tokens for step 'test_step': prompt=100, completion=50
INFO     flujo:telemetry.py:54 Extracted model ID for step 'test_step' from 'model_id': openai:gpt-4o
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='gpt-4o' in flujo.toml.
INFO     flujo:telemetry.py:54 Extracted tokens for step 'test_step': prompt=100, completion=50
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='openai', model='gpt-4o' in flujo.toml.
ERROR    flujo:telemetry.py:54 Step 'test_step' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'test_step' failed. Halting pipeline execution.
__ TestStrictPricingModeIntegration.test_strict_mode_on_with_unknown_provider __
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_cost_tracking_integration.py:445: in test_strict_mode_on_with_unknown_provider
    with pytest.raises(PricingNotConfiguredError) as exc_info:
E   Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,376 - flujo - INFO - Extracted tokens for step 'test_step': prompt=100, completion=50
2025-08-04 22:45:01,376 - flujo - INFO - Extracted model ID for step 'test_step' from 'model_id': unknown:unknown-model
2025-08-04 22:45:01,376 - flujo - WARNING - Step 'test_step' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='unknown', model='unknown-model' in flujo.toml.
2025-08-04 22:45:01,376 - flujo - INFO - Extracted tokens for step 'test_step': prompt=100, completion=50
2025-08-04 22:45:01,376 - flujo - WARNING - Step 'test_step' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='unknown', model='unknown-model' in flujo.toml.
2025-08-04 22:45:01,376 - flujo - WARNING - Step 'test_step' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:01,376 - flujo - ERROR - Step 'test_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Extracted tokens for step 'test_step': prompt=100, completion=50
INFO     flujo:telemetry.py:54 Extracted model ID for step 'test_step' from 'model_id': unknown:unknown-model
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 1 failed: Strict pricing is enabled, but no configuration was found for provider='unknown', model='unknown-model' in flujo.toml.
INFO     flujo:telemetry.py:54 Extracted tokens for step 'test_step': prompt=100, completion=50
WARNING  flujo:telemetry.py:54 Step 'test_step' agent execution attempt 2 failed: Strict pricing is enabled, but no configuration was found for provider='unknown', model='unknown-model' in flujo.toml.
ERROR    flujo:telemetry.py:54 Step 'test_step' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'test_step' failed. Halting pipeline execution.
______ TestSQLiteBackendEdgeCases.test_backup_filename_conflicts_handling ______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:34: in test_backup_filename_conflicts_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_ TestSQLiteBackendEdgeCases.test_backup_filename_conflicts_with_existing_files _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:71: in test_backup_filename_conflicts_with_existing_files
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
________ TestSQLiteBackendEdgeCases.test_backup_rename_failure_fallback ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:95: in test_backup_rename_failure_fallback
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,574 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:01,676 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_____________________ test_resume_after_crash_file_backend _____________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_crash_recovery.py:91: in test_resume_after_crash_file_backend
    assert crash_state["current_step_index"] == 1
E   assert 0 == 1
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,680 - flujo - INFO - Logfire telemetry is disabled or failed to initialize. Using standard Python logging.
2025-08-04 22:45:01,871 - flujo - INFO - Counting string output as 1 token for step 'transform': 'middle'
________ TestSQLiteBackendEdgeCases.test_backup_remove_failure_handling ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:117: in test_backup_remove_failure_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database

During handling of the above exception, another exception occurred:
tests/integration/test_sqlite_backend_edge_cases.py:116: in test_backup_remove_failure_handling
    with pytest.raises(sqlite3.DatabaseError, match="Database corruption recovery failed"):
E   AssertionError: Regex pattern did not match.
E    Regex: 'Database corruption recovery failed'
E    Input: 'file is not a database'
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:01,701 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:01,802 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
________ TestSQLiteBackendEdgeCases.test_special_characters_in_filename ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:129: in test_special_characters_in_filename
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
______________ TestSQLiteBackendEdgeCases.test_very_long_filename ______________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:147: in test_very_long_filename
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
____________________ test_resume_after_crash_sqlite_backend ____________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_crash_recovery.py:141: in test_resume_after_crash_sqlite_backend
    assert idx == 1
E   assert 0 == 1
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:02,159 - flujo - INFO - Logfire telemetry is disabled or failed to initialize. Using standard Python logging.
2025-08-04 22:45:02,345 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_resume_after_crash_sqlite0/state.db
2025-08-04 22:45:02,348 - flujo - INFO - Saved state for run_id=sqlite_run
2025-08-04 22:45:02,349 - flujo - INFO - Counting string output as 1 token for step 'transform': 'middle'
__________ TestOptimizationIntegration.test_configuration_management ___________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/regression/test_executor_core_optimization_regression.py:538: in test_configuration_management
    assert config_manager.current_config is not None
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'dict' object has no attribute 'current_config'
_________ TestOptimizationIntegration.test_performance_recommendations _________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/regression/test_executor_core_optimization_regression.py:556: in test_performance_recommendations
    assert "type" in rec
E   AssertionError: assert 'type' in 'Consider increasing cache size for better performance'
_________ TestSQLiteBackendEdgeCases.test_concurrent_backup_operations _________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:196: in test_concurrent_backup_operations
    await asyncio.gather(*tasks)
tests/integration/test_sqlite_backend_edge_cases.py:192: in create_backup
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:02,343 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
2025-08-04 22:45:02,344 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
2025-08-04 22:45:02,344 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
2025-08-04 22:45:02,344 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
2025-08-04 22:45:02,344 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:02,445 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
2025-08-04 22:45:02,445 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
2025-08-04 22:45:02,445 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
2025-08-04 22:45:02,446 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
2025-08-04 22:45:02,446 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
______________ test_proactive_cancellation_with_multiple_branches ______________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_step_enhancements.py:259: in test_proactive_cancellation_with_multiple_branches
    assert execution_time < 0.3  # Should be much faster than the 0.5s delay of branch_4
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   assert 0.5455325419898145 < 0.3
----------------------------- Captured stdout call -----------------------------
[DEBUG] CostlyAgent.run called with breach_event: True
[DEBUG] CostlyAgent starting sleep for 0.1s
[DEBUG] CostlyAgent.run called with breach_event: True
[DEBUG] CostlyAgent starting sleep for 0.1s
[DEBUG] CostlyAgent.run called with breach_event: True
[DEBUG] CostlyAgent starting sleep for 0.1s
[DEBUG] CostlyAgent.run called with breach_event: True
[DEBUG] CostlyAgent starting sleep for 0.5s
[DEBUG] CostlyAgent completed sleep, returning result
2025-08-04 22:45:02,021 - flujo - INFO - Using explicit cost from 'Output' for step 'branch_1': cost=$0.05, tokens=100
[DEBUG] CostlyAgent completed sleep, returning result
2025-08-04 22:45:02,022 - flujo - INFO - Using explicit cost from 'Output' for step 'branch_2': cost=$0.05, tokens=100
[DEBUG] CostlyAgent completed sleep, returning result
2025-08-04 22:45:02,022 - flujo - INFO - Using explicit cost from 'Output' for step 'branch_3': cost=$0.05, tokens=100
[DEBUG] CostlyAgent completed sleep, returning result
2025-08-04 22:45:02,455 - flujo - INFO - Using explicit cost from 'Output' for step 'branch_4': cost=$0.01, tokens=100
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Using explicit cost from 'Output' for step 'branch_1': cost=$0.05, tokens=100
INFO     flujo:telemetry.py:54 Using explicit cost from 'Output' for step 'branch_2': cost=$0.05, tokens=100
INFO     flujo:telemetry.py:54 Using explicit cost from 'Output' for step 'branch_3': cost=$0.05, tokens=100
INFO     flujo:telemetry.py:54 Using explicit cost from 'Output' for step 'branch_4': cost=$0.01, tokens=100
_____________ TestAgenticLoopLogging.test_pause_and_resume_logging _____________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_agentic_loop_logging.py:170: in test_pause_and_resume_logging
    resumed = await run_agentic_loop_pipeline(pipeline, "goal", resume_from=paused)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/recipes/factories.py:433: in run_agentic_loop_pipeline
    result = await runner.resume_async(resume_from, human_input)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/runner.py:770: in resume_async
    raise OrchestratorError("Pipeline is not paused")
E   flujo.exceptions.OrchestratorError: Pipeline is not paused
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === AgenticExplorationLoop
2025-08-04 22:45:02,560 - flujo - INFO - LoopStep 'AgenticExplorationLoop': Starting Iteration 1/10
2025-08-04 22:45:02,560 - flujo - WARNING - Step 'AgenticExplorationLoop' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:02,560 - flujo - ERROR - Step 'command_executor_step' failed with critical error: Need input
2025-08-04 22:45:02,560 - flujo - ERROR - Error in LoopStep 'AgenticExplorationLoop': Need input
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'AgenticExplorationLoop': Starting Iteration 1/10
ERROR    flujo:telemetry.py:54 Step 'command_executor_step' failed with critical error: Need input
ERROR    flujo:telemetry.py:54 Error in LoopStep 'AgenticExplorationLoop': Need input
WARNING  flujo:telemetry.py:54 Step 'AgenticExplorationLoop' failed. Halting pipeline execution.
____________ TestSQLiteBackendEdgeCases.test_infinite_loop_bug_fix _____________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:221: in test_infinite_loop_bug_fix
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_____ TestSQLiteBackendEdgeCases.test_infinite_loop_bug_with_continue_fix ______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:250: in test_infinite_loop_bug_with_continue_fix
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
___________________ test_proactive_cancellation_token_limits ___________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_step_enhancements.py:305: in test_proactive_cancellation_token_limits
    assert execution_time < threshold, (
E   AssertionError: Execution took too long: 0.549s (threshold: 0.400s). This indicates proactive cancellation may not be working correctly.
E   assert 0.5490237920312211 < 0.4
----------------------------- Captured stdout call -----------------------------
[DEBUG] CostlyAgent.run called with breach_event: True
[DEBUG] CostlyAgent starting sleep for 0.05s
[DEBUG] CostlyAgent.run called with breach_event: True
[DEBUG] CostlyAgent starting sleep for 0.5s
[DEBUG] CostlyAgent completed sleep, returning result
2025-08-04 22:45:02,567 - flujo - INFO - Using explicit cost from 'Output' for step 'high_tokens': cost=$0.01, tokens=150
[DEBUG] CostlyAgent completed sleep, returning result
2025-08-04 22:45:03,060 - flujo - INFO - Using explicit cost from 'Output' for step 'low_tokens': cost=$0.01, tokens=10
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Using explicit cost from 'Output' for step 'high_tokens': cost=$0.01, tokens=150
INFO     flujo:telemetry.py:54 Using explicit cost from 'Output' for step 'low_tokens': cost=$0.01, tokens=10
_______ TestSQLiteBackendEdgeCases.test_backup_path_update_after_cleanup _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:279: in test_backup_path_update_after_cleanup
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2282-12-22 12:13:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2282-12-22 12:13:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_________________ test_backward_compatibility_no_usage_limits __________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_step_enhancements.py:352: in test_backward_compatibility_no_usage_limits
    assert result.step_history[-1].output["a"].value == "input"
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'str' object has no attribute 'value'
----------------------------- Captured stdout call -----------------------------
[DEBUG] CostlyAgent.run called with breach_event: False
[DEBUG] CostlyAgent starting sleep for 0.1s
[DEBUG] CostlyAgent.run called with breach_event: False
[DEBUG] CostlyAgent starting sleep for 0.1s
[DEBUG] CostlyAgent completed sleep, returning result
2025-08-04 22:45:03,177 - flujo - INFO - Using explicit cost from 'Output' for step 'a': cost=$0.1, tokens=100
[DEBUG] CostlyAgent completed sleep, returning result
2025-08-04 22:45:03,178 - flujo - INFO - Using explicit cost from 'Output' for step 'b': cost=$0.1, tokens=100
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Using explicit cost from 'Output' for step 'a': cost=$0.1, tokens=100
INFO     flujo:telemetry.py:54 Using explicit cost from 'Output' for step 'b': cost=$0.1, tokens=100
______________ test_context_include_keys_with_nonexistent_fields _______________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_step_enhancements.py:373: in test_context_include_keys_with_nonexistent_fields
    result = await gather_result(runner, "input", initial_context_data=context.model_dump())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/testing/utils.py:84: in gather_result
    async for result in runner.run_async(data, **kwargs):
flujo/application/runner.py:582: in run_async
    async for chunk in self._execute_steps(
flujo/application/runner.py:413: in _execute_steps
    async for item in execution_manager.execute_steps(
flujo/application/core/execution_manager.py:155: in execute_steps
    async for item in self.step_coordinator.execute_step(
flujo/application/core/step_coordinator.py:115: in execute_step
    step_result = await backend.execute_step(request)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/infra/backends.py:53: in execute_step
    return await self._executor.execute(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/core/ultra_executor.py:539: in execute
    return await self._handle_parallel_step(
flujo/application/core/ultra_executor.py:1705: in _handle_parallel_step
    branch_context = type(context)(initial_prompt=context.initial_prompt)
                                                  ^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/pydantic/main.py:991: in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
E   AttributeError: 'LargeContext' object has no attribute 'initial_prompt'
______ TestSQLiteBackendEdgeCases.test_race_condition_in_backup_creation _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:311: in test_race_condition_in_backup_creation
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_________ TestSQLiteBackendEdgeCases.test_readonly_directory_fallback __________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:333: in test_readonly_directory_fallback
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_________ TestSQLiteBackendEdgeCases.test_backup_pattern_glob_handling _________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:365: in test_backup_pattern_glob_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
__________ TestSQLiteBackendEdgeCases.test_backup_stat_error_handling __________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:401: in test_backup_stat_error_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_ TestParallelUsageGovernorStress.test_stress_parallel_usage_governor_breach_detection _
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_usage_governor_stress.py:110: in test_stress_parallel_usage_governor_breach_detection
    assert any(results), "At least one operation should detect the breach"
E   AssertionError: At least one operation should detect the breach
E   assert False
E    +  where False = any([None, None, None, None, None, None, ...])
_ TestParallelUsageGovernorStress.test_stress_parallel_usage_governor_token_limits _
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_usage_governor_stress.py:254: in test_stress_parallel_usage_governor_token_limits
    assert any(results), "At least one operation should detect the token breach"
E   AssertionError: At least one operation should detect the token breach
E   assert False
E    +  where False = any([None, None, None, None, None, None, ...])
_ TestParallelUsageGovernorStress.test_stress_parallel_usage_governor_no_limits _
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_parallel_usage_governor_stress.py:300: in test_stress_parallel_usage_governor_no_limits
    results = await add_usage_concurrently()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/integration/test_parallel_usage_governor_stress.py:296: in add_usage_concurrently
    results = await asyncio.gather(*tasks)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/core/ultra_executor.py:2025: in add_usage
    if self.limits.total_cost_usd_limit is not None and self.total_cost > self.limits.total_cost_usd_limit:
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'NoneType' object has no attribute 'total_cost_usd_limit'
_________ TestSQLiteBackendEdgeCases.test_backup_unlink_error_handling _________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:426: in test_backup_unlink_error_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_____ TestSQLiteBackendEdgeCases.test_backup_min_function_with_none_values _____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:460: in test_backup_min_function_with_none_values
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_______ TestSQLiteBackendEdgeCases.test_backup_empty_directory_handling ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:475: in test_backup_empty_directory_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
____ TestSQLiteBackendEdgeCases.test_backup_max_attempts_exceeded_handling _____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:499: in test_backup_max_attempts_exceeded_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
___ TestSQLiteBackendEdgeCases.test_backup_continue_statement_effectiveness ____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:536: in test_backup_continue_statement_effectiveness
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_____________________ test_file_backend_resume_after_crash _____________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_persistence_backends.py:96: in test_file_backend_resume_after_crash
    assert wf.current_step_index == 3
E   AssertionError: assert 2 == 3
E    +  where 2 = WorkflowState(run_id='run_file', pipeline_id='unknown', pipeline_name='unknown', pipeline_version='latest', current_step_index=2, pipeline_context={'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_file', 'scratchpad': {'status': 'completed'}}, last_step_output='mid done', step_history=[{'attempts': 1, 'branch_context': {'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_file', 'scratchpad': {'status': 'completed'}}, 'cost_usd': 0.0, 'feedback': None, 'latency_s': 0.0003927089856006205, 'metadata_': {}, 'name': 's1', 'output': 'mid', 'step_history': [], 'success': True, 'token_counts': 1}, {'attempts': 1, 'branch_context': {'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_file', 'scratchpad': {'status': 'completed'}}, 'cost_usd': 0.0, 'feedback': None, 'latency_s': 0.0001551249879412353, 'metadata_': {}, 'name': 's2', 'output': 'mid done', 'step_history': [], 'success': True, 'token_counts': 1}], status='completed', created_at=datetime.datetime(2025, 8, 4, 22, 45, 5, 67415), updated_at=datetime.datetime(2025, 8, 4, 22, 45, 5, 76673)).current_step_index
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:04,884 - flujo - INFO - Logfire telemetry is disabled or failed to initialize. Using standard Python logging.
2025-08-04 22:45:05,068 - flujo - INFO - Counting string output as 1 token for step 's1': 'mid'
2025-08-04 22:45:05,075 - flujo - INFO - Counting string output as 1 token for step 's1': 'mid'
2025-08-04 22:45:05,075 - flujo - INFO - Counting string output as 1 token for step 's2': 'mid done'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 's1': 'mid'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 's2': 'mid done'
_______ TestSQLiteBackendEdgeCases.test_backup_path_reset_after_cleanup ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:571: in test_backup_path_reset_after_cleanup
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
______ TestSQLiteBackendEdgeCases.test_backup_counter_reset_after_cleanup ______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:601: in test_backup_counter_reset_after_cleanup
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
____________________ test_sqlite_backend_resume_after_crash ____________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_persistence_backends.py:125: in test_sqlite_backend_resume_after_crash
    assert wf.current_step_index == 3
E   AssertionError: assert 2 == 3
E    +  where 2 = WorkflowState(run_id='run_sqlite', pipeline_id='unknown', pipeline_name='unknown', pipeline_version='latest', current_step_index=2, pipeline_context={'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_sqlite', 'scratchpad': {'status': 'completed'}}, last_step_output='mid done', step_history=[{'attempts': 1, 'branch_context': {'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_sqlite', 'scratchpad': {'status': 'completed'}}, 'cost_usd': 0.0, 'feedback': None, 'latency_s': 0.0003035829868167639, 'metadata_': {}, 'name': 's1', 'output': 'mid', 'step_history': [], 'success': True, 'token_counts': 1}, {'attempts': 1, 'branch_context': {'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_sqlite', 'scratchpad': {'status': 'completed'}}, 'cost_usd': 0.0, 'feedback': None, 'latency_s': 0.00019966700347140431, 'metadata_': {}, 'name': 's2', 'output': 'mid done', 'step_history': [], 'success': True, 'token_counts': 1}], status='completed', created_at=datetime.datetime(2025, 8, 4, 22, 45, 5, 544094), updated_at=datetime.datetime(2025, 8, 4, 22, 45, 5, 558920)).current_step_index
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:05,358 - flujo - INFO - Logfire telemetry is disabled or failed to initialize. Using standard Python logging.
{
    "name": "s",
    "context": {
        "trace_id": "0xbdb2e8ad5fe58f286b68bf90205a0b33",
        "span_id": "0x3b5810ab10a743da",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": "0x99b8de3eea309212",
    "start_time": "2025-08-05T05:45:00.430057Z",
    "end_time": "2025-08-05T05:45:00.430418Z",
    "status": {
        "status_code": "OK"
    },
    "attributes": {
        "step_input": "in",
        "success": true,
        "latency_s": 0.0001872080028988421
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.36.0",
            "service.name": "unknown_service"
        },
        "schema_url": ""
    }
}
{
    "name": "pipeline_run",
    "context": {
        "trace_id": "0xbdb2e8ad5fe58f286b68bf90205a0b33",
        "span_id": "0x99b8de3eea309212",
        "trace_state": "[]"
    },
    "kind": "SpanKind.INTERNAL",
    "parent_id": null,
    "start_time": "2025-08-05T05:45:00.429935Z",
    "end_time": "2025-08-05T05:45:00.430940Z",
    "status": {
        "status_code": "OK"
    },
    "attributes": {
        "initial_input": "in"
    },
    "events": [],
    "links": [],
    "resource": {
        "attributes": {
            "telemetry.sdk.language": "python",
            "telemetry.sdk.name": "opentelemetry",
            "telemetry.sdk.version": "1.36.0",
            "service.name": "unknown_service"
        },
        "schema_url": ""
    }
}
2025-08-04 22:45:05,541 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_sqlite_backend_resume_aft0/state.db
2025-08-04 22:45:05,544 - flujo - INFO - Saved state for run_id=run_sqlite
2025-08-04 22:45:05,545 - flujo - INFO - Counting string output as 1 token for step 's1': 'mid'
2025-08-04 22:45:05,550 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_sqlite_backend_resume_aft0/state.db
2025-08-04 22:45:05,552 - flujo - INFO - Saved state for run_id=run_sqlite
2025-08-04 22:45:05,554 - flujo - INFO - Counting string output as 1 token for step 's1': 'mid'
2025-08-04 22:45:05,555 - flujo - INFO - Counting string output as 1 token for step 's2': 'mid done'
2025-08-04 22:45:05,557 - flujo - INFO - Saved state for run_id=run_sqlite
2025-08-04 22:45:05,559 - flujo - INFO - Saved state for run_id=run_sqlite
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_sqlite_backend_resume_aft0/state.db
INFO     flujo:telemetry.py:54 Saved state for run_id=run_sqlite
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 's1': 'mid'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 's2': 'mid done'
INFO     flujo:telemetry.py:54 Saved state for run_id=run_sqlite
INFO     flujo:telemetry.py:54 Saved state for run_id=run_sqlite
____________ TestCircularReferenceSerialization.test_self_reference ____________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_bug_regression.py:70: in test_self_reference
    assert dumped["parent"] is None
E   AssertionError: assert {'children': [], 'name': 'self', 'parent': None} is None
______________________ test_update_list_of_nested_models _______________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_pipeline_context_updates.py:93: in test_update_list_of_nested_models
    assert result.step_history[-1].output == [1, 2]
E   AssertionError: assert 1 == [1, 2]
E    +  where 1 = StepResult(name='reader', output=1, success=True, attempts=1, latency_s=6.312504410743713e-05, token_counts=0, cost_usd=0.0, feedback=None, branch_context=ContextWithNesting(counter=0, nested_item=None, list_of_items=[NestedModel(value=1, name='nested'), NestedModel(value=2, name='nested')]), metadata_={}, step_history=[]).output
______________ TestFailedStepRecording.test_failed_step_recording ______________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_bug_regression.py:165: in test_failed_step_recording
    steps = await backend.list_run_steps(run_id)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:1201: in list_run_steps
    cursor = await db.execute(
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.OperationalError: no such column: start_time
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:05,738 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_failed_step_recording0/test.db
2025-08-04 22:45:05,740 - flujo - WARNING - Step 'failing_step' failed. Halting pipeline execution.
2025-08-04 22:45:05,741 - flujo - INFO - Saved state for run_id=test_run
2025-08-04 22:45:05,743 - flujo - INFO - Saved state for run_id=test_run
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_failed_step_recording0/test.db
WARNING  flujo:telemetry.py:54 Step 'failing_step' failed. Halting pipeline execution.
INFO     flujo:telemetry.py:54 Saved state for run_id=test_run
INFO     flujo:telemetry.py:54 Saved state for run_id=test_run
_______________________ test_runner_respects_max_retries _______________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_pipeline_runner.py:35: in test_runner_respects_max_retries
    assert agent.call_count == 3
E   assert 1 == 3
E    +  where 1 = <flujo.testing.utils.StubAgent object at 0x11551ff90>.call_count
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:05,775 - flujo - INFO - Counting string output as 1 token for step 'test': 'a'
2025-08-04 22:45:05,775 - flujo - WARNING - Step 'test' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:05,775 - flujo - ERROR - Step 'test' plugin failed: Plugin failed: None
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'test': 'a'
ERROR    flujo:telemetry.py:54 Step 'test' plugin failed: Plugin failed: None
WARNING  flujo:telemetry.py:54 Step 'test' failed. Halting pipeline execution.
________________________ test_feedback_enriches_prompt _________________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_pipeline_runner.py:51: in test_feedback_enriches_prompt
    assert sol_agent.call_count == 2
E   assert 1 == 2
E    +  where 1 = <flujo.testing.utils.StubAgent object at 0x115539650>.call_count
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:05,781 - flujo - INFO - Counting string output as 1 token for step 'solution': 'sol1'
2025-08-04 22:45:05,781 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:05,781 - flujo - ERROR - Step 'solution' plugin failed: Plugin failed: SQL Error: XYZ
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'solution': 'sol1'
ERROR    flujo:telemetry.py:54 Step 'solution' plugin failed: Plugin failed: SQL Error: XYZ
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
________ TestLambdaSerializationNullHandling.test_lambda_null_handling _________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_bug_regression.py:203: in test_lambda_null_handling
    steps = await backend.list_run_steps("test_run")
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:1201: in list_run_steps
    cursor = await db.execute(
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.OperationalError: no such column: start_time
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:05,767 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lambda_null_handling0/test.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lambda_null_handling0/test.db
_______ TestSQLiteBackendEdgeCases.test_backup_infinite_loop_prevention ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:625: in test_backup_infinite_loop_prevention
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
___________________ test_timeout_and_redirect_loop_detection ___________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_pipeline_runner.py:142: in test_timeout_and_redirect_loop_detection
    with pytest.raises(InfiniteRedirectError):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.InfiniteRedirectError'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:05,792 - flujo - INFO - Counting string output as 1 token for step 's': 'ok'
2025-08-04 22:45:05,844 - flujo - INFO - Counting string output as 1 token for step 'loop': 'a1'
2025-08-04 22:45:05,844 - flujo - INFO - Step 'loop' redirecting to agent: <flujo.testing.utils.StubAgent object at 0x1159d3d90>
2025-08-04 22:45:05,844 - flujo - INFO - Counting string output as 1 token for step 'loop': 'a2'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 's': 'ok'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'loop': 'a1'
INFO     flujo:telemetry.py:54 Step 'loop' redirecting to agent: <flujo.testing.utils.StubAgent object at 0x1159d3d90>
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'loop': 'a2'
_______________________ test_resources_passed_to_plugin ________________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_pipeline_runner_with_resources.py:93: in test_resources_passed_to_plugin
    assert result.step_history[0].success
E   assert False
E    +  where False = StepResult(name='plugin_step', output='queried_products', success=False, attempts=1, latency_s=0.0005497080273926258, token_counts=1, cost_usd=0.0, feedback="Plugin failed: Plugin ResourceUsingPlugin failed: ResourceUsingPlugin.validate() missing 1 required keyword-only argument: 'resources'", branch_context=None, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:05,869 - flujo - INFO - Counting string output as 1 token for step 'plugin_step': 'queried_products'
2025-08-04 22:45:05,869 - flujo - WARNING - Step 'plugin_step' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:05,869 - flujo - ERROR - Plugin ResourceUsingPlugin failed: ResourceUsingPlugin.validate() missing 1 required keyword-only argument: 'resources'
2025-08-04 22:45:05,869 - flujo - ERROR - Step 'plugin_step' plugin failed: Plugin ResourceUsingPlugin failed: ResourceUsingPlugin.validate() missing 1 required keyword-only argument: 'resources'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'plugin_step': 'queried_products'
ERROR    flujo:telemetry.py:54 Plugin ResourceUsingPlugin failed: ResourceUsingPlugin.validate() missing 1 required keyword-only argument: 'resources'
ERROR    flujo:telemetry.py:54 Step 'plugin_step' plugin failed: Plugin ResourceUsingPlugin failed: ResourceUsingPlugin.validate() missing 1 required keyword-only argument: 'resources'
WARNING  flujo:telemetry.py:54 Step 'plugin_step' failed. Halting pipeline execution.
_____________________ test_prompt_processor_modifies_input _____________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_processors.py:51: in test_prompt_processor_modifies_input
    assert agent.inputs[0] == "hello world"
E   AssertionError: assert 'hello' == 'hello world'
E     
E     - hello world
E     + hello
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:05,902 - flujo - INFO - Counting string output as 1 token for step 'solution': 'ok'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'solution': 'ok'
_______________________ test_processor_receives_context ________________________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_processors.py:71: in test_processor_receives_context
    assert agent.inputs[0].startswith("X:")
E   AssertionError: assert False
E    +  where False = <built-in method startswith of str object at 0x1144c81b0>('X:')
E    +    where <built-in method startswith of str object at 0x1144c81b0> = 'hello'.startswith
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:05,909 - flujo - INFO - Counting string output as 1 token for step 'solution': 'ok'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'solution': 'ok'
______________ TestCIErrorRecovery.test_partial_failure_handling _______________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ci_compatibility.py:280: in test_partial_failure_handling
    assert isinstance(result, dict)
E   AssertionError: assert False
E    +  where False = isinstance('<unserializable: dict>', dict)
________ TestSQLiteBackendEdgeCases.test_backup_cleanup_attempts_limit _________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:654: in test_backup_cleanup_attempts_limit
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
______________________ test_runner_uses_sqlite_by_default ______________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
/Users/alvaro/Documents/Code/flujo/tests/unit/test_default_backend.py:20: in test_runner_uses_sqlite_by_default
    assert isinstance(runner.state_backend, SQLiteBackend)
E   assert False
E    +  where False = isinstance(<tests.conftest.NoOpStateBackend object at 0x10c30f950>, SQLiteBackend)
E    +    where <tests.conftest.NoOpStateBackend object at 0x10c30f950> = <flujo.application.runner.Flujo object at 0x10c3c8f10>.state_backend
___________ TestUsageGovernor.test_check_usage_limits_cost_exceeded ____________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_execution_manager.py:178: in test_check_usage_limits_cost_exceeded
    usage_governor.check_usage_limits(pipeline_result, None)
flujo/application/core/usage_governor.py:97: in check_usage_limits
    raise error
E   flujo.exceptions.UsageLimitExceededError: Cost limit of $10 exceeded

During handling of the above exception, another exception occurred:
tests/unit/test_execution_manager.py:177: in test_check_usage_limits_cost_exceeded
    with pytest.raises(UsageLimitExceededError, match="Cost limit of \\$10.0 exceeded"):
E   AssertionError: Regex pattern did not match.
E    Regex: 'Cost limit of \\$10.0 exceeded'
E    Input: 'Cost limit of $10 exceeded'
________ TestSQLiteBackendEdgeCases.test_backup_stat_exception_handling ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:688: in test_backup_stat_exception_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
________________ TestDefaultPluginRunner.test_plugin_execution _________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_executor_components.py:392: in test_plugin_execution
    assert result == "processed data"
E   AssertionError: assert PluginOutcome(success=True, feedback=None, redirect_to=None, new_solution='processed data') == 'processed data'
__________ TestDefaultPluginRunner.test_plugin_with_priority_ordering __________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_executor_components.py:423: in test_plugin_with_priority_ordering
    assert result == "processed by plugin1"
E   AssertionError: assert PluginOutcome(success=True, feedback=None, redirect_to=None, new_solution='processed by plugin2') == 'processed by plugin1'
_ TestExecutorCoreParallelMigration.test_executor_core_dynamic_router_delegates_to_parallel _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_executor_core_parallel_migration.py:164: in test_executor_core_dynamic_router_delegates_to_parallel
    result = await executor_core._handle_dynamic_router_step(
E   TypeError: ExecutorCore._handle_dynamic_router_step() got an unexpected keyword argument 'router_step'
_______________ test_successful_fallback_correctly_sets_metrics ________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_fallback.py:167: in test_successful_fallback_correctly_sets_metrics
    assert sr.feedback is None  # Should be cleared on successful fallback
    ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert '' is None
E    +  where '' = StepResult(name='p', output='success', success=True, attempts=2, latency_s=0.00023125007282942533, token_counts=6, cost_usd=0.2, feedback='', branch_context=PipelineContext(run_id='run_c16de9d4d413457dbf0e7e5dba08b554', initial_prompt='in', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'fallback_triggered': True, 'original_error': 'Plugin failed: primary failed'}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:06,857 - flujo - INFO - Counting string output as 1 token for step 'p': 'bad'
2025-08-04 22:45:06,857 - flujo - INFO - Using explicit cost from 'CostlyOutput' for step 'fb': cost=$0.2, tokens=5
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:06,857 - flujo - ERROR - Step 'p' plugin failed: Plugin failed: primary failed
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'p': 'bad'
ERROR    flujo:telemetry.py:54 Step 'p' plugin failed: Plugin failed: primary failed
INFO     flujo:telemetry.py:54 Using explicit cost from 'CostlyOutput' for step 'fb': cost=$0.2, tokens=5
_____________________ test_fallback_with_high_cost_agents ______________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_fallback_edge_cases.py:119: in test_fallback_with_high_cost_agents
    assert sr.cost_usd == 999.99  # Should be fallback cost
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert 1999.98 == 999.99
E    +  where 1999.98 = StepResult(name='p', output='fallback', success=True, attempts=2, latency_s=0.00022508297115564346, token_counts=1999998, cost_usd=1999.98, feedback='', branch_context=PipelineContext(run_id='run_f432b6128ad04b4ca8508059718e85f3', initial_prompt='in', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'fallback_triggered': True, 'original_error': 'Plugin failed: primary failed'}, step_history=[]).cost_usd
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:06,867 - flujo - INFO - Using explicit cost from 'HighCostResult' for step 'p': cost=$999.99, tokens=999999
2025-08-04 22:45:06,867 - flujo - INFO - Using explicit cost from 'HighCostResult' for step 'fb': cost=$999.99, tokens=999999
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:06,867 - flujo - ERROR - Step 'p' plugin failed: Plugin failed: primary failed
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Using explicit cost from 'HighCostResult' for step 'p': cost=$999.99, tokens=999999
ERROR    flujo:telemetry.py:54 Step 'p' plugin failed: Plugin failed: primary failed
INFO     flujo:telemetry.py:54 Using explicit cost from 'HighCostResult' for step 'fb': cost=$999.99, tokens=999999
_____________________ test_fallback_with_negative_metrics ______________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_fallback_edge_cases.py:199: in test_fallback_with_negative_metrics
    assert sr.cost_usd == -0.1  # Should be fallback cost
    ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert -0.2 == -0.1
E    +  where -0.2 = StepResult(name='p', output='negative', success=True, attempts=2, latency_s=0.00021824997384101152, token_counts=-10, cost_usd=-0.2, feedback='', branch_context=PipelineContext(run_id='run_03ee61b834244d07acfdfcc168389ad7', initial_prompt='in', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'fallback_triggered': True, 'original_error': 'Plugin failed: primary failed'}, step_history=[]).cost_usd
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:06,878 - flujo - INFO - Using explicit cost from 'NegativeResult' for step 'p': cost=$-0.1, tokens=-5
2025-08-04 22:45:06,879 - flujo - INFO - Using explicit cost from 'NegativeResult' for step 'fb': cost=$-0.1, tokens=-5
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:06,879 - flujo - ERROR - Step 'p' plugin failed: Plugin failed: primary failed
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Using explicit cost from 'NegativeResult' for step 'p': cost=$-0.1, tokens=-5
ERROR    flujo:telemetry.py:54 Step 'p' plugin failed: Plugin failed: primary failed
INFO     flujo:telemetry.py:54 Using explicit cost from 'NegativeResult' for step 'fb': cost=$-0.1, tokens=-5
_______________________ test_fallback_with_none_feedback _______________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_fallback_edge_cases.py:289: in test_fallback_with_none_feedback
    assert "Agent execution failed" in sr.feedback
E   AssertionError: assert 'Agent execution failed' in 'Original error: Plugin failed: None; Fallback error: Plugin failed: None'
E    +  where 'Original error: Plugin failed: None; Fallback error: Plugin failed: None' = StepResult(name='p', output='oops', success=False, attempts=2, latency_s=0.0002603329485282302, token_counts=6, cost_usd=0.2, feedback='Original error: Plugin failed: None; Fallback error: Plugin failed: None', branch_context=None, metadata_={'fallback_triggered': True}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:06,890 - flujo - INFO - Counting string output as 1 token for step 'p': 'bad'
2025-08-04 22:45:06,891 - flujo - INFO - Using explicit cost from 'CostlyOutput' for step 'fb': cost=$0.2, tokens=5
2025-08-04 22:45:06,891 - flujo - WARNING - Step 'p' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:06,890 - flujo - ERROR - Step 'p' plugin failed: Plugin failed: None
2025-08-04 22:45:06,891 - flujo - ERROR - Step 'fb' plugin failed: Plugin failed: None
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'p': 'bad'
ERROR    flujo:telemetry.py:54 Step 'p' plugin failed: Plugin failed: None
INFO     flujo:telemetry.py:54 Using explicit cost from 'CostlyOutput' for step 'fb': cost=$0.2, tokens=5
ERROR    flujo:telemetry.py:54 Step 'fb' plugin failed: Plugin failed: None
WARNING  flujo:telemetry.py:54 Step 'p' failed. Halting pipeline execution.
_______ TestSQLiteBackendEdgeCases.test_backup_unlink_exception_handling _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:712: in test_backup_unlink_exception_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_____________________ test_fallback_with_complex_metadata ______________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_fallback_edge_cases.py:435: in test_fallback_with_complex_metadata
    assert sr.metadata_["original_error"] == "complex failed"
E   AssertionError: assert 'Plugin faile...omplex failed' == 'complex failed'
E     
E     - complex failed
E     + Plugin failed: complex failed
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:06,906 - flujo - INFO - Counting string output as 1 token for step 'p': 'bad'
2025-08-04 22:45:06,907 - flujo - INFO - Using explicit cost from 'CostlyOutput' for step 'fb': cost=$0.2, tokens=5
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:06,906 - flujo - ERROR - Step 'p' plugin failed: Plugin failed: complex failed
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'p': 'bad'
ERROR    flujo:telemetry.py:54 Step 'p' plugin failed: Plugin failed: complex failed
INFO     flujo:telemetry.py:54 Using explicit cost from 'CostlyOutput' for step 'fb': cost=$0.2, tokens=5
___________________ test_sqlite_backend_migrates_existing_db ___________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_file_sqlite_backends.py:97: in test_sqlite_backend_migrates_existing_db
    await backend.save_state("run1", state)
flujo/state/backends/sqlite.py:714: in save_state
    await self._with_retries(_save)
flujo/state/backends/sqlite.py:602: in _with_retries
    result = await coro_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:684: in _save
    await db.execute(
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.OperationalError: table workflow_state has no column named pipeline_name
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:06,932 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_sqlite_backend_migrates_e0/state.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_sqlite_backend_migrates_e0/state.db
_______________________ test_backends_serialize_pydantic _______________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_file_sqlite_backends.py:125: in test_backends_serialize_pydantic
    await sb.save_state("run1", state)
flujo/state/backends/sqlite.py:714: in save_state
    await self._with_retries(_save)
flujo/state/backends/sqlite.py:602: in _with_retries
    result = await coro_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:652: in _save
    pipeline_context_json = _fast_json_dumps(pipeline_context)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:38: in _fast_json_dumps
    return orjson.dumps(obj, option=orjson.OPT_SORT_KEYS).decode("utf-8")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: Type is not JSON serializable: MyModel
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:06,966 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_backends_serialize_pydant0/s.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_backends_serialize_pydant0/s.db
___________________ test_backends_deserialize_special_types ____________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_file_sqlite_backends.py:238: in test_backends_deserialize_special_types
    assert loaded_s["pipeline_context"]["val"] == "inf"
E   AssertionError: assert None == 'inf'
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,035 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_backends_deserialize_spec0/s.db
2025-08-04 22:45:07,036 - flujo - INFO - Saved state for run_id=run1
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_backends_deserialize_spec0/s.db
INFO     flujo:telemetry.py:54 Saved state for run_id=run1
______________________________ test_lens_commands ______________________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_lens_cli.py:80: in test_lens_commands
    assert result.exit_code == 0
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,102 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands0/ops.db
2025-08-04 22:45:07,111 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands0/ops.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands0/ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands0/ops.db
_______________________ test_lens_commands_with_filters ________________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_lens_cli.py:128: in test_lens_commands_with_filters
    assert result.exit_code == 0
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,121 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_filter0/ops.db
2025-08-04 22:45:07,140 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_filter0/ops.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_filter0/ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_filter0/ops.db
_________________________ test_lens_show_detailed_run __________________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_lens_cli.py:199: in test_lens_show_detailed_run
    assert result.exit_code == 0
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,148 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_show_detailed_run0/ops.db
2025-08-04 22:45:07,161 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_show_detailed_run0/ops.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_show_detailed_run0/ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_show_detailed_run0/ops.db
____________________ test_lens_commands_with_empty_database ____________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_lens_cli.py:225: in test_lens_commands_with_empty_database
    assert result.exit_code == 0
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,188 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_empty_0/empty_ops.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_empty_0/empty_ops.db
______________________ test_lens_commands_with_failed_run ______________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_lens_cli.py:293: in test_lens_commands_with_failed_run
    assert result.exit_code == 0
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,197 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_failed0/failed_ops.db
2025-08-04 22:45:07,208 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_failed0/failed_ops.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_failed0/failed_ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_failed0/failed_ops.db
______________ test_lens_commands_with_environment_configuration _______________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_lens_cli.py:333: in test_lens_commands_with_environment_configuration
    assert result.exit_code == 0
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,217 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_enviro0/env_ops.db
2025-08-04 22:45:07,227 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_enviro0/env_ops.db
STDOUT (sqlite:////{db_path}): 
STDERR (sqlite:////{db_path}): Error accessing backend: no such column: start_time

------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_enviro0/env_ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_commands_with_enviro0/env_ops.db
____________________ test_lens_cli_relative_path_resolution ____________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
/Users/alvaro/Documents/Code/flujo/tests/unit/test_lens_cli.py:363: in test_lens_cli_relative_path_resolution
    assert result.exit_code == 0, result.stdout + result.stderr
E   AssertionError: Error accessing backend: no such column: start_time
E     
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,237 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_cli_relative_path_re0/manual_testing/ops.db
2025-08-04 22:45:07,249 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_cli_relative_path_re0/manual_testing/ops.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_cli_relative_path_re0/manual_testing/ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_cli_relative_path_re0/manual_testing/ops.db
_____________________ test_lens_show_with_verbose_options ______________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_lens_cli.py:389: in test_lens_show_with_verbose_options
    assert result.exit_code == 0
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,259 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_show_with_verbose_op0/verbose_ops.db
2025-08-04 22:45:07,272 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_show_with_verbose_op0/verbose_ops.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_show_with_verbose_op0/verbose_ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_lens_show_with_verbose_op0/verbose_ops.db
________ TestSQLiteBackendEdgeCases.test_backup_glob_exception_handling ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:736: in test_backup_glob_exception_handling
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2009-02-13 15:31:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2009-02-13 15:31:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
__________ TestInMemoryMonitorUsage.test_monitor_records_failed_calls __________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_monitor_integration.py:92: in test_monitor_records_failed_calls
    assert call["input_data"] == "test_input"
E   AssertionError: assert 'test_input\n... Test failure' == 'test_input'
E     
E     - test_input
E     + test_input
E     ?           +
E     + Agent execution failed on attempt 1: Test failure
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,407 - flujo - WARNING - Step 'failing_step' agent execution attempt 1 failed: Test failure
2025-08-04 22:45:07,407 - flujo - WARNING - Step 'failing_step' agent execution attempt 2 failed: Test failure
2025-08-04 22:45:07,408 - flujo - WARNING - Step 'failing_step' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:07,407 - flujo - ERROR - Step 'failing_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'failing_step' agent execution attempt 1 failed: Test failure
WARNING  flujo:telemetry.py:54 Step 'failing_step' agent execution attempt 2 failed: Test failure
ERROR    flujo:telemetry.py:54 Step 'failing_step' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'failing_step' failed. Halting pipeline execution.
_______ TestParallelStepExecution.test_basic_parallel_execution_no_merge _______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:118: in test_basic_parallel_execution_no_merge
    assert result.success
E   assert False
E    +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c4678d0>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c4678d0>, metadata_={}, step_history=[]), 'branch2': StepResult(name='test_parallel_branch2', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c467150>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c467150>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0008629999938420951, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c4678d0>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type; Branch 'branch2': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c467150>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=None, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,567 - flujo - INFO - Counting string output as 1 token for step 'step1': 'output1'
2025-08-04 22:45:07,568 - flujo - INFO - Counting string output as 1 token for step 'step2': 'output2'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:07,568 - flujo - ERROR - Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c4678d0>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-08-04 22:45:07,568 - flujo - ERROR - Branch branch2 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c467150>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1', 'branch2']
DEBUG    flujo:telemetry.py:54 Executing branch: branch1
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: step1
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 579e965b01fe151ebbde43b53c92b3563326f41c48945e60c42fe40ec4efd698
DEBUG    flujo:telemetry.py:54 Cache miss for step: step1
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: step1
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: step1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'step1': 'output1'
DEBUG    flujo:telemetry.py:54 Step 'step1' completed successfully
ERROR    flujo:telemetry.py:54 Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c4678d0>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
DEBUG    flujo:telemetry.py:54 Executing branch: branch2
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: step2
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 7f1a270685e7b76ef7d9d16def3d0207290e5f5c4462d2cd02b527d3fa46f8eb
DEBUG    flujo:telemetry.py:54 Cache miss for step: step2
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: step2
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: step2
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'step2': 'output2'
DEBUG    flujo:telemetry.py:54 Step 'step2' completed successfully
ERROR    flujo:telemetry.py:54 Branch branch2 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c467150>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
DEBUG    flujo:telemetry.py:54 Context merging check: context=True, merge_strategy=MergeStrategy.NO_MERGE
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel completed: success=False
_ TestParallelStepExecution.test_parallel_execution_with_context_include_keys __
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:142: in test_parallel_execution_with_context_include_keys
    result = await executor._handle_parallel_step(
flujo/application/core/ultra_executor.py:1705: in _handle_parallel_step
    branch_context = type(context)(initial_prompt=context.initial_prompt)
                                                  ^^^^^^^^^^^^^^^^^^^^^^
tests/unit/test_parallel_step_strategies.py:54: in __getattr__
    raise AttributeError(f"MockContext has no attribute '{item}'")
E   AttributeError: MockContext has no attribute 'initial_prompt'
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1']
_____ TestParallelStepExecution.test_parallel_execution_cost_limit_breach ______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:221: in test_parallel_execution_cost_limit_breach
    with pytest.raises(UsageLimitExceededError):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1']
DEBUG    flujo:telemetry.py:54 Executing branch: branch1
DEBUG    flujo:telemetry.py:54 Branch branch1 completed: success=True
DEBUG    flujo:telemetry.py:54 Context merging check: context=False, merge_strategy=MergeStrategy.NO_MERGE
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel completed: success=True
_____ TestParallelStepExecution.test_parallel_execution_token_limit_breach _____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:263: in test_parallel_execution_token_limit_breach
    with pytest.raises(UsageLimitExceededError):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1']
DEBUG    flujo:telemetry.py:54 Executing branch: branch1
DEBUG    flujo:telemetry.py:54 Branch branch1 completed: success=True
DEBUG    flujo:telemetry.py:54 Context merging check: context=False, merge_strategy=MergeStrategy.NO_MERGE
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel completed: success=True
______ TestParallelStepExecution.test_parallel_execution_merge_overwrite _______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:392: in test_parallel_execution_merge_overwrite
    assert result.success
E   assert False
E    +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c1ebb90>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c1ebb90>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0005666249780915678, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c1ebb90>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c2b9250>, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,686 - flujo - INFO - Counting string output as 1 token for step 'step1': 'output1'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:07,686 - flujo - ERROR - Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c1ebb90>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1']
DEBUG    flujo:telemetry.py:54 Executing branch: branch1
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: step1
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 579e965b01fe151ebbde43b53c92b3563326f41c48945e60c42fe40ec4efd698
DEBUG    flujo:telemetry.py:54 Cache miss for step: step1
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: step1
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: step1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'step1': 'output1'
DEBUG    flujo:telemetry.py:54 Step 'step1' completed successfully
ERROR    flujo:telemetry.py:54 Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c1ebb90>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
DEBUG    flujo:telemetry.py:54 Context merging check: context=True, merge_strategy=MergeStrategy.OVERWRITE
DEBUG    flujo:telemetry.py:54 Context merging: strategy=MergeStrategy.OVERWRITE, successful_branches=0
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel completed: success=False
______ TestParallelStepExecution.test_parallel_execution_merge_scratchpad ______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:422: in test_parallel_execution_merge_scratchpad
    assert result.success
E   assert False
E    +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c152710>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c152710>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0005230410024523735, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c152710>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c1eaa10>, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,693 - flujo - INFO - Counting string output as 1 token for step 'step1': 'output1'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:07,693 - flujo - ERROR - Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c152710>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1']
DEBUG    flujo:telemetry.py:54 Executing branch: branch1
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: step1
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 02d14c4507d63db247ab921de6ecdcbf0fd561c899b5dab14278f675c114991c
DEBUG    flujo:telemetry.py:54 Cache miss for step: step1
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: step1
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: step1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'step1': 'output1'
DEBUG    flujo:telemetry.py:54 Step 'step1' completed successfully
ERROR    flujo:telemetry.py:54 Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c152710>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
DEBUG    flujo:telemetry.py:54 Context merging check: context=True, merge_strategy=MergeStrategy.MERGE_SCRATCHPAD
DEBUG    flujo:telemetry.py:54 Context merging: strategy=MergeStrategy.MERGE_SCRATCHPAD, successful_branches=0
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel completed: success=False
_______ TestSQLiteBackendEdgeCases.test_backup_fallback_timestamp_naming _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:761: in test_backup_fallback_timestamp_naming
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2282-12-22 12:13:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2282-12-22 12:13:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_ TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:465: in test_parallel_execution_merge_scratchpad_no_scratchpad
    assert result.success
E   assert False
E    +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c264a90>, input_type=TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext object at 0x10c264a90>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.000687791034579277, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c264a90>, input_type=TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext object at 0x10c266990>, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,700 - flujo - INFO - Counting string output as 1 token for step 'step1': 'output1'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:07,700 - flujo - ERROR - Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c264a90>, input_type=TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1']
DEBUG    flujo:telemetry.py:54 Executing branch: branch1
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: step1
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 4045d17ba9e8ba0e4f6b3e6a564ed5027831c9ed7893a14af713c678d095b0d9
DEBUG    flujo:telemetry.py:54 Cache miss for step: step1
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: step1
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: step1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'step1': 'output1'
DEBUG    flujo:telemetry.py:54 Step 'step1' completed successfully
ERROR    flujo:telemetry.py:54 Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c264a90>, input_type=TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
DEBUG    flujo:telemetry.py:54 Context merging check: context=True, merge_strategy=MergeStrategy.MERGE_SCRATCHPAD
DEBUG    flujo:telemetry.py:54 Context merging: strategy=MergeStrategy.MERGE_SCRATCHPAD, successful_branches=0
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel completed: success=False
___ TestParallelStepExecution.test_parallel_execution_custom_merge_strategy ____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:502: in test_parallel_execution_custom_merge_strategy
    assert result.success
E   assert False
E    +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c29a110>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c29a110>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0005427919677458704, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c29a110>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c264450>, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,707 - flujo - INFO - Counting string output as 1 token for step 'step1': 'output1'
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:07,707 - flujo - ERROR - Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c29a110>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1']
DEBUG    flujo:telemetry.py:54 Executing branch: branch1
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: step1
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 579e965b01fe151ebbde43b53c92b3563326f41c48945e60c42fe40ec4efd698
DEBUG    flujo:telemetry.py:54 Cache miss for step: step1
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: step1
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: step1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'step1': 'output1'
DEBUG    flujo:telemetry.py:54 Step 'step1' completed successfully
ERROR    flujo:telemetry.py:54 Branch branch1 failed with exception: 1 validation error for PipelineResult
final_pipeline_context
  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c29a110>, input_type=MockContext]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
DEBUG    flujo:telemetry.py:54 Context merging check: context=True, merge_strategy=<function TestParallelStepExecution.test_parallel_execution_custom_merge_strategy.<locals>.custom_merge_strategy at 0x10c1a71a0>
DEBUG    flujo:telemetry.py:54 Context merging: strategy=<function TestParallelStepExecution.test_parallel_execution_custom_merge_strategy.<locals>.custom_merge_strategy at 0x10c1a71a0>, successful_branches=0
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel completed: success=False
_____ TestParallelStepExecution.test_parallel_execution_task_cancellation ______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:568: in test_parallel_execution_task_cancellation
    with pytest.raises(UsageLimitExceededError):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1']
DEBUG    flujo:telemetry.py:54 Executing branch: branch1
DEBUG    flujo:telemetry.py:54 Branch branch1 completed: success=True
DEBUG    flujo:telemetry.py:54 Context merging check: context=False, merge_strategy=MergeStrategy.NO_MERGE
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel completed: success=True
____ TestParallelStepExecution.test_parallel_execution_merge_context_update ____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:625: in test_parallel_execution_merge_context_update
    assert result.success
E   assert False
E    +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback="Branch execution failed: 'Pipeline' object has no attribute 'name'", branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_context_update.<locals>.TestContext object at 0x10c1c61d0>, metadata_={}, step_history=[]), 'branch2': StepResult(name='test_parallel_branch2', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback="Branch execution failed: 'Pipeline' object has no attribute 'name'", branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_context_update.<locals>.TestContext object at 0x10c13a650>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.00023750000400468707, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 'Pipeline' object has no attribute 'name'; Branch 'branch2': Branch execution failed: 'Pipeline' object has no attribute 'name'", branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_context_update.<locals>.TestContext object at 0x10bfbaa90>, metadata_={}, step_history=[]).success
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:07,822 - flujo - ERROR - Branch branch1 failed with exception: 'Pipeline' object has no attribute 'name'
2025-08-04 22:45:07,822 - flujo - ERROR - Branch branch2 failed with exception: 'Pipeline' object has no attribute 'name'
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['branch1', 'branch2']
DEBUG    flujo:telemetry.py:54 Executing branch: branch1
ERROR    flujo:telemetry.py:54 Branch branch1 failed with exception: 'Pipeline' object has no attribute 'name'
DEBUG    flujo:telemetry.py:54 Executing branch: branch2
ERROR    flujo:telemetry.py:54 Branch branch2 failed with exception: 'Pipeline' object has no attribute 'name'
DEBUG    flujo:telemetry.py:54 Context merging check: context=True, merge_strategy=MergeStrategy.CONTEXT_UPDATE
DEBUG    flujo:telemetry.py:54 Context merging: strategy=MergeStrategy.CONTEXT_UPDATE, successful_branches=0
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel completed: success=False
________________ test_parallel_usage_limit_enforced_atomically _________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_parallel_step_strategies.py:698: in test_parallel_usage_limit_enforced_atomically
    with pytest.raises(UsageLimitExceededError):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,840 - flujo - INFO - Using explicit cost from 'StepResult' for step 's0': cost=$0.21000000000000002, tokens=51
2025-08-04 22:45:07,850 - flujo - INFO - Using explicit cost from 'StepResult' for step 's1': cost=$0.21000000000000002, tokens=51
2025-08-04 22:45:07,861 - flujo - INFO - Using explicit cost from 'StepResult' for step 's2': cost=$0.21000000000000002, tokens=51
2025-08-04 22:45:07,872 - flujo - INFO - Using explicit cost from 'StepResult' for step 's3': cost=$0.21000000000000002, tokens=51
2025-08-04 22:45:07,882 - flujo - INFO - Using explicit cost from 'StepResult' for step 's4': cost=$0.21000000000000002, tokens=51
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === HANDLING PARALLEL STEP === test_parallel_race
DEBUG    flujo:telemetry.py:54 Parallel step branches: ['b0', 'b1', 'b2', 'b3', 'b4']
DEBUG    flujo:telemetry.py:54 Executing branch: b0
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: s0
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: total_cost_usd_limit=1.0 total_tokens_limit=250
DEBUG    flujo:telemetry.py:54 Generated cache key: c430795c00c34322b89bac245b46d21fa0eb35510b46e06575c4721dddc8ca99
DEBUG    flujo:telemetry.py:54 Cache miss for step: s0
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: s0
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: s0
DEBUG    flujo:telemetry.py:54 Executing branch: b1
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: s1
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: total_cost_usd_limit=1.0 total_tokens_limit=250
DEBUG    flujo:telemetry.py:54 Generated cache key: f4918ee9751e562105477fd65f97d12c13b6236d7ae4c09ae9905b96174c021e
DEBUG    flujo:telemetry.py:54 Cache miss for step: s1
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: s1
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: s1
DEBUG    flujo:telemetry.py:54 Executing branch: b2
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: s2
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: total_cost_usd_limit=1.0 total_tokens_limit=250
DEBUG    flujo:telemetry.py:54 Generated cache key: f139f20599b8e2fa51405108a8391e2e8840e7340be8a0c40ad8205eafc82f70
DEBUG    flujo:telemetry.py:54 Cache miss for step: s2
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: s2
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: s2
DEBUG    flujo:telemetry.py:54 Executing branch: b3
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: s3
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: total_cost_usd_limit=1.0 total_tokens_limit=250
DEBUG    flujo:telemetry.py:54 Generated cache key: 5bedd223f8993da5f764d495a7c8e23dd86a98bdb48c12a9c3ea1bcd2f2d2e9a
DEBUG    flujo:telemetry.py:54 Cache miss for step: s3
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: s3
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: s3
DEBUG    flujo:telemetry.py:54 Executing branch: b4
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step[Any, Any]'>
DEBUG    flujo:telemetry.py:54 Step name: s4
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: True
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: total_cost_usd_limit=1.0 total_tokens_limit=250
DEBUG    flujo:telemetry.py:54 Generated cache key: 804e16fa358fbc9767c8584ac59d02f7f916a3887d8a5d3df336a3dcf2400314
DEBUG    flujo:telemetry.py:54 Cache miss for step: s4
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: s4
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: s4
INFO     flujo:telemetry.py:54 Using explicit cost from 'StepResult' for step 's0': cost=$0.21000000000000002, tokens=51
DEBUG    flujo:telemetry.py:54 Step 's0' completed successfully
DEBUG    flujo:telemetry.py:54 Branch b0 completed: success=True
INFO     flujo:telemetry.py:54 Using explicit cost from 'StepResult' for step 's1': cost=$0.21000000000000002, tokens=51
DEBUG    flujo:telemetry.py:54 Step 's1' completed successfully
DEBUG    flujo:telemetry.py:54 Branch b1 completed: success=True
INFO     flujo:telemetry.py:54 Using explicit cost from 'StepResult' for step 's2': cost=$0.21000000000000002, tokens=51
DEBUG    flujo:telemetry.py:54 Step 's2' completed successfully
DEBUG    flujo:telemetry.py:54 Branch b2 completed: success=True
INFO     flujo:telemetry.py:54 Using explicit cost from 'StepResult' for step 's3': cost=$0.21000000000000002, tokens=51
DEBUG    flujo:telemetry.py:54 Step 's3' completed successfully
DEBUG    flujo:telemetry.py:54 Branch b3 completed: success=True
INFO     flujo:telemetry.py:54 Using explicit cost from 'StepResult' for step 's4': cost=$0.21000000000000002, tokens=51
DEBUG    flujo:telemetry.py:54 Step 's4' completed successfully
DEBUG    flujo:telemetry.py:54 Branch b4 completed: success=True
DEBUG    flujo:telemetry.py:54 Context merging check: context=False, merge_strategy=MergeStrategy.NO_MERGE
DEBUG    flujo:telemetry.py:54 Parallel step test_parallel_race completed: success=True
_ TestPersistenceOptimizationEdgeCases.test_persistence_frequency_optimization _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_persistence_edge_cases.py:81: in test_persistence_frequency_optimization
    steps = await backend.list_run_steps(run_id)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:1201: in list_run_steps
    cursor = await db.execute(
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.OperationalError: no such column: start_time
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,926 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_persistence_frequency_opt0/test.db
2025-08-04 22:45:07,932 - flujo - INFO - Saved state for run_id=test_run
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_persistence_frequency_opt0/test.db
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run test_run
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run test_run
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run test_run
INFO     flujo:telemetry.py:54 Saved state for run_id=test_run
____ TestPersistenceOptimizationEdgeCases.test_persistence_on_step_failure _____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_persistence_edge_cases.py:156: in test_persistence_on_step_failure
    steps = await backend.list_run_steps(run_id)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:1201: in list_run_steps
    cursor = await db.execute(
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.OperationalError: no such column: start_time
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:07,957 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_persistence_on_step_failu0/test.db
2025-08-04 22:45:07,960 - flujo - WARNING - Step 'failing_step' failed. Halting pipeline execution.
2025-08-04 22:45:07,961 - flujo - INFO - Saved state for run_id=test_failure_run
2025-08-04 22:45:07,963 - flujo - INFO - Saved state for run_id=test_failure_run
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_persistence_on_step_failu0/test.db
WARNING  flujo:telemetry.py:54 Step 'failing_step' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run test_failure_run
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run test_failure_run
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run test_failure_run
INFO     flujo:telemetry.py:54 Saved state for run_id=test_failure_run
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run test_failure_run
INFO     flujo:telemetry.py:54 Saved state for run_id=test_failure_run
____ TestSQLiteBackendEdgeCases.test_backup_all_slots_undeletable_fallback _____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:791: in test_backup_all_slots_undeletable_fallback
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2282-12-22 12:13:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2282-12-22 12:13:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
__________ TestSQLiteBackendEdgeCases.test_backup_stat_always_raises ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:818: in test_backup_stat_always_raises
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2282-12-22 12:13:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2282-12-22 12:13:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
_ TestPersistencePerformanceOverhead.test_default_backend_performance_overhead _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_persistence_performance.py:101: in test_default_backend_performance_overhead
    assert overhead_percentage <= overhead_limit, (
E   AssertionError: Default persistence overhead (722.93%) exceeds 35.0% limit
E   assert 722.9315621198879 <= 35.0
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:08,339 - flujo - INFO - Counting string output as 1 token for step 'solution': 'output'
2025-08-04 22:45:08,343 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_default_backend_performan0/with_backend_ab2418a8.db
2025-08-04 22:45:08,346 - flujo - INFO - Saved state for run_id=run_8e7bfb1d376b45f6b2f405d518a5d7f2
2025-08-04 22:45:08,347 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,347 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,348 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,349 - flujo - INFO - Saved state for run_id=run_8e7bfb1d376b45f6b2f405d518a5d7f2
2025-08-04 22:45:08,351 - flujo - INFO - Saved state for run_id=run_8e7bfb1d376b45f6b2f405d518a5d7f2
2025-08-04 22:45:08,353 - flujo - INFO - Saved state for run_id=run_8e7bfb1d376b45f6b2f405d518a5d7f2
2025-08-04 22:45:08,357 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,357 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,357 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,360 - flujo - INFO - Saved state for run_id=run_33ba7cc60e8a44b3b09ffb2a60a073b1
2025-08-04 22:45:08,361 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,361 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,362 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,363 - flujo - INFO - Saved state for run_id=run_33ba7cc60e8a44b3b09ffb2a60a073b1
2025-08-04 22:45:08,365 - flujo - INFO - Saved state for run_id=run_33ba7cc60e8a44b3b09ffb2a60a073b1
2025-08-04 22:45:08,368 - flujo - INFO - Saved state for run_id=run_33ba7cc60e8a44b3b09ffb2a60a073b1
2025-08-04 22:45:08,371 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,371 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,371 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,375 - flujo - INFO - Saved state for run_id=run_a26c08bf1c46454f8b5ef3d15a0e246f
2025-08-04 22:45:08,376 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,376 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,377 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,378 - flujo - INFO - Saved state for run_id=run_a26c08bf1c46454f8b5ef3d15a0e246f
2025-08-04 22:45:08,380 - flujo - INFO - Saved state for run_id=run_a26c08bf1c46454f8b5ef3d15a0e246f
2025-08-04 22:45:08,382 - flujo - INFO - Saved state for run_id=run_a26c08bf1c46454f8b5ef3d15a0e246f
2025-08-04 22:45:08,385 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,385 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,386 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,389 - flujo - INFO - Saved state for run_id=run_698b6944e65b49838b866063fa460d03
2025-08-04 22:45:08,390 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,390 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,391 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,392 - flujo - INFO - Saved state for run_id=run_698b6944e65b49838b866063fa460d03
2025-08-04 22:45:08,394 - flujo - INFO - Saved state for run_id=run_698b6944e65b49838b866063fa460d03
2025-08-04 22:45:08,397 - flujo - INFO - Saved state for run_id=run_698b6944e65b49838b866063fa460d03
2025-08-04 22:45:08,400 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,400 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,400 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,404 - flujo - INFO - Saved state for run_id=run_f1df1066f0fe47eaa24be93904d5d31e
2025-08-04 22:45:08,405 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,405 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,406 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,407 - flujo - INFO - Saved state for run_id=run_f1df1066f0fe47eaa24be93904d5d31e
2025-08-04 22:45:08,410 - flujo - INFO - Saved state for run_id=run_f1df1066f0fe47eaa24be93904d5d31e
2025-08-04 22:45:08,412 - flujo - INFO - Saved state for run_id=run_f1df1066f0fe47eaa24be93904d5d31e
2025-08-04 22:45:08,416 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,416 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,416 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,420 - flujo - INFO - Saved state for run_id=run_cb59358923494e76875a71274e81f685
2025-08-04 22:45:08,421 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,421 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,422 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,423 - flujo - INFO - Saved state for run_id=run_cb59358923494e76875a71274e81f685
2025-08-04 22:45:08,426 - flujo - INFO - Saved state for run_id=run_cb59358923494e76875a71274e81f685
2025-08-04 22:45:08,428 - flujo - INFO - Saved state for run_id=run_cb59358923494e76875a71274e81f685
2025-08-04 22:45:08,432 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,432 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,432 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,436 - flujo - INFO - Saved state for run_id=run_86c9754971b74245b720683ac88081f4
2025-08-04 22:45:08,436 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,437 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,438 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,439 - flujo - INFO - Saved state for run_id=run_86c9754971b74245b720683ac88081f4
2025-08-04 22:45:08,441 - flujo - INFO - Saved state for run_id=run_86c9754971b74245b720683ac88081f4
2025-08-04 22:45:08,444 - flujo - INFO - Saved state for run_id=run_86c9754971b74245b720683ac88081f4
2025-08-04 22:45:08,447 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,448 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,448 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,451 - flujo - INFO - Saved state for run_id=run_4ab84fae6818412ea8996369e8e54e7c
2025-08-04 22:45:08,452 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,452 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,454 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,455 - flujo - INFO - Saved state for run_id=run_4ab84fae6818412ea8996369e8e54e7c
2025-08-04 22:45:08,457 - flujo - INFO - Saved state for run_id=run_4ab84fae6818412ea8996369e8e54e7c
2025-08-04 22:45:08,459 - flujo - INFO - Saved state for run_id=run_4ab84fae6818412ea8996369e8e54e7c
2025-08-04 22:45:08,463 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,463 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,463 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,466 - flujo - INFO - Saved state for run_id=run_c418e7917d0a422ba68f358d31efd3ae
2025-08-04 22:45:08,467 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,467 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,468 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,469 - flujo - INFO - Saved state for run_id=run_c418e7917d0a422ba68f358d31efd3ae
2025-08-04 22:45:08,472 - flujo - INFO - Saved state for run_id=run_c418e7917d0a422ba68f358d31efd3ae
2025-08-04 22:45:08,474 - flujo - INFO - Saved state for run_id=run_c418e7917d0a422ba68f358d31efd3ae
2025-08-04 22:45:08,478 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,478 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,478 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,481 - flujo - INFO - Saved state for run_id=run_906e0aeab2494c7499a13ce156066779
2025-08-04 22:45:08,482 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,482 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,483 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,484 - flujo - INFO - Saved state for run_id=run_906e0aeab2494c7499a13ce156066779
2025-08-04 22:45:08,487 - flujo - INFO - Saved state for run_id=run_906e0aeab2494c7499a13ce156066779
2025-08-04 22:45:08,490 - flujo - INFO - Saved state for run_id=run_906e0aeab2494c7499a13ce156066779
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:08,347 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,357 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,361 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,371 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,376 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,385 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,390 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,400 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,405 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,416 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,421 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,432 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,437 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,448 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,452 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,463 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,467 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,478 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,482 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 Flujo backend: LocalBackend, executor: <class 'flujo.application.core.ultra_executor.ExecutorCore'>
DEBUG    flujo:telemetry.py:54 Flujo backend: LocalBackend, executor: <class 'flujo.application.core.ultra_executor.ExecutorCore'>
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_1416b31b92734960971fcdfcec2c13c1
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_1416b31b92734960971fcdfcec2c13c1
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_1416b31b92734960971fcdfcec2c13c1
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: a0f6928ba07dd8ac7faefea647b391f2ba086447198ec056a5b91e16f369af0b
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'solution': 'output'
DEBUG    flujo:telemetry.py:54 Step 'solution' completed successfully
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_1416b31b92734960971fcdfcec2c13c1
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_1416b31b92734960971fcdfcec2c13c1
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_1416b31b92734960971fcdfcec2c13c1
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_1416b31b92734960971fcdfcec2c13c1
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_1416b31b92734960971fcdfcec2c13c1
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_1416b31b92734960971fcdfcec2c13c1
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_default_backend_performan0/with_backend_ab2418a8.db
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_8e7bfb1d376b45f6b2f405d518a5d7f2
INFO     flujo:telemetry.py:54 Saved state for run_id=run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 35e35dce35e7dcda55549db29696108f4a0faaec297a7e8b258da3c50c0dae36
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_8e7bfb1d376b45f6b2f405d518a5d7f2
INFO     flujo:telemetry.py:54 Saved state for run_id=run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_8e7bfb1d376b45f6b2f405d518a5d7f2
INFO     flujo:telemetry.py:54 Saved state for run_id=run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_8e7bfb1d376b45f6b2f405d518a5d7f2
INFO     flujo:telemetry.py:54 Saved state for run_id=run_8e7bfb1d376b45f6b2f405d518a5d7f2
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 0b043997abb77ff517a01704763e977fa517c10d8b102d9433e09608e68f2d74
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_a10b6dcd2b7742018584d5bb2c0f5595
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_33ba7cc60e8a44b3b09ffb2a60a073b1
INFO     flujo:telemetry.py:54 Saved state for run_id=run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 82f9d133ce17b8661daef947cca9da1802f777c83dff129f2ea3460dc6564bea
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_33ba7cc60e8a44b3b09ffb2a60a073b1
INFO     flujo:telemetry.py:54 Saved state for run_id=run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_33ba7cc60e8a44b3b09ffb2a60a073b1
INFO     flujo:telemetry.py:54 Saved state for run_id=run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_33ba7cc60e8a44b3b09ffb2a60a073b1
INFO     flujo:telemetry.py:54 Saved state for run_id=run_33ba7cc60e8a44b3b09ffb2a60a073b1
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 43940657b167505d8b5ecff7157e5e24c607c5369046181fbb095d6fd368bde9
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_3d7386b528aa4ff7a7bfc9700e3724e9
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_a26c08bf1c46454f8b5ef3d15a0e246f
INFO     flujo:telemetry.py:54 Saved state for run_id=run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 0228bb96fe9f77221d0a3767380f20a37245392c40fc9484aba178f70a55aea6
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_a26c08bf1c46454f8b5ef3d15a0e246f
INFO     flujo:telemetry.py:54 Saved state for run_id=run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_a26c08bf1c46454f8b5ef3d15a0e246f
INFO     flujo:telemetry.py:54 Saved state for run_id=run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_a26c08bf1c46454f8b5ef3d15a0e246f
INFO     flujo:telemetry.py:54 Saved state for run_id=run_a26c08bf1c46454f8b5ef3d15a0e246f
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: dfe65c9b921573172935b80890b358c59b6358093d833116aeba8025f27a37e5
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_a5631358804f4543b789e79c68920eed
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_698b6944e65b49838b866063fa460d03
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_698b6944e65b49838b866063fa460d03
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_698b6944e65b49838b866063fa460d03
INFO     flujo:telemetry.py:54 Saved state for run_id=run_698b6944e65b49838b866063fa460d03
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 71daa6cb1caaa320e11b80713ef2b7743280fa881e0b2865c7641964279adf8a
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_698b6944e65b49838b866063fa460d03
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_698b6944e65b49838b866063fa460d03
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_698b6944e65b49838b866063fa460d03
INFO     flujo:telemetry.py:54 Saved state for run_id=run_698b6944e65b49838b866063fa460d03
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_698b6944e65b49838b866063fa460d03
INFO     flujo:telemetry.py:54 Saved state for run_id=run_698b6944e65b49838b866063fa460d03
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_698b6944e65b49838b866063fa460d03
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_698b6944e65b49838b866063fa460d03
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_698b6944e65b49838b866063fa460d03
INFO     flujo:telemetry.py:54 Saved state for run_id=run_698b6944e65b49838b866063fa460d03
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 91900c4e6220fe295b73b3c1adbfcd479e27188822f5f8762c6e5e046e94d63c
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_97edcf3e34434d5cae55cf9f19880123
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_f1df1066f0fe47eaa24be93904d5d31e
INFO     flujo:telemetry.py:54 Saved state for run_id=run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: d9676dbfd727932db7bea8f936758e031f6c926f465221988fb5891b209adbe8
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_f1df1066f0fe47eaa24be93904d5d31e
INFO     flujo:telemetry.py:54 Saved state for run_id=run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_f1df1066f0fe47eaa24be93904d5d31e
INFO     flujo:telemetry.py:54 Saved state for run_id=run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_f1df1066f0fe47eaa24be93904d5d31e
INFO     flujo:telemetry.py:54 Saved state for run_id=run_f1df1066f0fe47eaa24be93904d5d31e
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 6da47408c2a35d3d56f6c094fee0800ed2a4c22855d7e30a16e9b743ca491a72
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_97c9091ac4d8436d95b76f162f852a47
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_cb59358923494e76875a71274e81f685
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_cb59358923494e76875a71274e81f685
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_cb59358923494e76875a71274e81f685
INFO     flujo:telemetry.py:54 Saved state for run_id=run_cb59358923494e76875a71274e81f685
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 4e9a5ec05da666030fda575568294deedd0a6d023754719835f26b2f2032e780
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_cb59358923494e76875a71274e81f685
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_cb59358923494e76875a71274e81f685
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_cb59358923494e76875a71274e81f685
INFO     flujo:telemetry.py:54 Saved state for run_id=run_cb59358923494e76875a71274e81f685
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_cb59358923494e76875a71274e81f685
INFO     flujo:telemetry.py:54 Saved state for run_id=run_cb59358923494e76875a71274e81f685
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_cb59358923494e76875a71274e81f685
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_cb59358923494e76875a71274e81f685
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_cb59358923494e76875a71274e81f685
INFO     flujo:telemetry.py:54 Saved state for run_id=run_cb59358923494e76875a71274e81f685
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: f403d83ad4847cf554dbabbcc1b9ffe5b6959601ad7150f0a9a86224ae555ed2
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_aae645677e3041db944af1aec84390f1
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_86c9754971b74245b720683ac88081f4
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_86c9754971b74245b720683ac88081f4
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_86c9754971b74245b720683ac88081f4
INFO     flujo:telemetry.py:54 Saved state for run_id=run_86c9754971b74245b720683ac88081f4
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 4e8fa3272746788123e9f8739e19ac8f773b89157580ee5f68fda8e04e60e454
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_86c9754971b74245b720683ac88081f4
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_86c9754971b74245b720683ac88081f4
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_86c9754971b74245b720683ac88081f4
INFO     flujo:telemetry.py:54 Saved state for run_id=run_86c9754971b74245b720683ac88081f4
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_86c9754971b74245b720683ac88081f4
INFO     flujo:telemetry.py:54 Saved state for run_id=run_86c9754971b74245b720683ac88081f4
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_86c9754971b74245b720683ac88081f4
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_86c9754971b74245b720683ac88081f4
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_86c9754971b74245b720683ac88081f4
INFO     flujo:telemetry.py:54 Saved state for run_id=run_86c9754971b74245b720683ac88081f4
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: bfbce72697418312ffa0a3a38a22d09d692c21460d54369fa997e07f8a5ac02b
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_add118ff5ac940f9a35c93009f08dd9a
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_4ab84fae6818412ea8996369e8e54e7c
INFO     flujo:telemetry.py:54 Saved state for run_id=run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: a50b4b4d5c6fe350fec30119888014eb84bece31ab86c0fffe8a90eafacb4c2e
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_4ab84fae6818412ea8996369e8e54e7c
INFO     flujo:telemetry.py:54 Saved state for run_id=run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_4ab84fae6818412ea8996369e8e54e7c
INFO     flujo:telemetry.py:54 Saved state for run_id=run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_4ab84fae6818412ea8996369e8e54e7c
INFO     flujo:telemetry.py:54 Saved state for run_id=run_4ab84fae6818412ea8996369e8e54e7c
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 3d0aa0969a1e8ff8f073e6cec02360dfe620695b62328e7d714b2e581288e2bd
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_099a1e7977e243ee9d80e92e60d567e4
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_c418e7917d0a422ba68f358d31efd3ae
INFO     flujo:telemetry.py:54 Saved state for run_id=run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: e13d9946ae4fa1f061f3ef8ddee3e1fff83666444241e8143665fb5a08b85f72
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_c418e7917d0a422ba68f358d31efd3ae
INFO     flujo:telemetry.py:54 Saved state for run_id=run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_c418e7917d0a422ba68f358d31efd3ae
INFO     flujo:telemetry.py:54 Saved state for run_id=run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_c418e7917d0a422ba68f358d31efd3ae
INFO     flujo:telemetry.py:54 Saved state for run_id=run_c418e7917d0a422ba68f358d31efd3ae
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 90842383e5a35c970add04c9f87b88d5183a0e2a7f5552a6777391e6ee326fad
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_7c79c31b1d3548a284ba61a80b9a8de4
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_906e0aeab2494c7499a13ce156066779
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_906e0aeab2494c7499a13ce156066779
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_906e0aeab2494c7499a13ce156066779
INFO     flujo:telemetry.py:54 Saved state for run_id=run_906e0aeab2494c7499a13ce156066779
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 6d5cd1749b07254232d0171275dc50f3787bbda39e6edb93c92bab400f2acdc3
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_906e0aeab2494c7499a13ce156066779
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_906e0aeab2494c7499a13ce156066779
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_906e0aeab2494c7499a13ce156066779
INFO     flujo:telemetry.py:54 Saved state for run_id=run_906e0aeab2494c7499a13ce156066779
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_906e0aeab2494c7499a13ce156066779
INFO     flujo:telemetry.py:54 Saved state for run_id=run_906e0aeab2494c7499a13ce156066779
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_906e0aeab2494c7499a13ce156066779
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_906e0aeab2494c7499a13ce156066779
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_906e0aeab2494c7499a13ce156066779
INFO     flujo:telemetry.py:54 Saved state for run_id=run_906e0aeab2494c7499a13ce156066779
_ TestPersistencePerformanceOverhead.test_persistence_overhead_with_large_context _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_persistence_performance.py:182: in test_persistence_overhead_with_large_context
    assert overhead_percentage <= overhead_limit, (
E   AssertionError: Persistence overhead with large context (585.07%) exceeds 35.0%
E   assert 585.0727111312593 <= 35.0
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:08,503 - flujo - INFO - Counting string output as 1 token for step 'solution': 'output'
2025-08-04 22:45:08,507 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_persistence_overhead_with0/with_backend_3b6b2bc6.db
2025-08-04 22:45:08,511 - flujo - INFO - Saved state for run_id=run_164293c23b704bbd8bc4946ee781b2e2
2025-08-04 22:45:08,512 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,512 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,513 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,514 - flujo - INFO - Saved state for run_id=run_164293c23b704bbd8bc4946ee781b2e2
2025-08-04 22:45:08,516 - flujo - INFO - Saved state for run_id=run_164293c23b704bbd8bc4946ee781b2e2
2025-08-04 22:45:08,519 - flujo - INFO - Saved state for run_id=run_164293c23b704bbd8bc4946ee781b2e2
2025-08-04 22:45:08,523 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,523 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,523 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,527 - flujo - INFO - Saved state for run_id=run_6f685a9b3b6042feb29fc4e3585d8337
2025-08-04 22:45:08,528 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,528 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,529 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,530 - flujo - INFO - Saved state for run_id=run_6f685a9b3b6042feb29fc4e3585d8337
2025-08-04 22:45:08,532 - flujo - INFO - Saved state for run_id=run_6f685a9b3b6042feb29fc4e3585d8337
2025-08-04 22:45:08,535 - flujo - INFO - Saved state for run_id=run_6f685a9b3b6042feb29fc4e3585d8337
2025-08-04 22:45:08,539 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,539 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,539 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,543 - flujo - INFO - Saved state for run_id=run_fa45f3ee1f6d441c8afe072bdbd1ad68
2025-08-04 22:45:08,544 - flujo - WARNING - Step 'solution' agent execution attempt 1 failed: No more outputs available
2025-08-04 22:45:08,544 - flujo - WARNING - Step 'solution' agent execution attempt 2 failed: No more outputs available
2025-08-04 22:45:08,545 - flujo - WARNING - Step 'solution' failed. Halting pipeline execution.
2025-08-04 22:45:08,546 - flujo - INFO - Saved state for run_id=run_fa45f3ee1f6d441c8afe072bdbd1ad68
2025-08-04 22:45:08,549 - flujo - INFO - Saved state for run_id=run_fa45f3ee1f6d441c8afe072bdbd1ad68
2025-08-04 22:45:08,551 - flujo - INFO - Saved state for run_id=run_fa45f3ee1f6d441c8afe072bdbd1ad68
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:08,512 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,523 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,528 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,539 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
2025-08-04 22:45:08,544 - flujo - ERROR - Step 'solution' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 Flujo backend: LocalBackend, executor: <class 'flujo.application.core.ultra_executor.ExecutorCore'>
DEBUG    flujo:telemetry.py:54 Flujo backend: LocalBackend, executor: <class 'flujo.application.core.ultra_executor.ExecutorCore'>
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_99541120da6949e2b66a085b1cf68bfa
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_99541120da6949e2b66a085b1cf68bfa
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_99541120da6949e2b66a085b1cf68bfa
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 068c90b477dc70b22dfa64aa4dd913695b529dd19a630a5244d6d5e92c70940b
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'solution': 'output'
DEBUG    flujo:telemetry.py:54 Step 'solution' completed successfully
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_99541120da6949e2b66a085b1cf68bfa
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_99541120da6949e2b66a085b1cf68bfa
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_99541120da6949e2b66a085b1cf68bfa
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_99541120da6949e2b66a085b1cf68bfa
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_99541120da6949e2b66a085b1cf68bfa
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_99541120da6949e2b66a085b1cf68bfa
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_persistence_overhead_with0/with_backend_3b6b2bc6.db
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_164293c23b704bbd8bc4946ee781b2e2
INFO     flujo:telemetry.py:54 Saved state for run_id=run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 65542ec9e5016ab2b15dddcf397fda2920d11e32701414e1cee59e74b9112113
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_164293c23b704bbd8bc4946ee781b2e2
INFO     flujo:telemetry.py:54 Saved state for run_id=run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_164293c23b704bbd8bc4946ee781b2e2
INFO     flujo:telemetry.py:54 Saved state for run_id=run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_164293c23b704bbd8bc4946ee781b2e2
INFO     flujo:telemetry.py:54 Saved state for run_id=run_164293c23b704bbd8bc4946ee781b2e2
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: fed75e84713404f882c338bdf87a343959a25415f6b0a6753d75364eb1938c6b
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_1729e60baf104bdab19f59212d248795
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_6f685a9b3b6042feb29fc4e3585d8337
INFO     flujo:telemetry.py:54 Saved state for run_id=run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: ddf9d96efc28d593adee0d659b3419b2ce3d2a913ec440b786d1ad16cdb60920
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_6f685a9b3b6042feb29fc4e3585d8337
INFO     flujo:telemetry.py:54 Saved state for run_id=run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_6f685a9b3b6042feb29fc4e3585d8337
INFO     flujo:telemetry.py:54 Saved state for run_id=run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_6f685a9b3b6042feb29fc4e3585d8337
INFO     flujo:telemetry.py:54 Saved state for run_id=run_6f685a9b3b6042feb29fc4e3585d8337
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 316ac5875f681de0d29020f3c6e085e66fa738f79eaf29ea673c5624219c0dba
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_4eb809894005459cadc4008d3ab72cdb
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_fa45f3ee1f6d441c8afe072bdbd1ad68
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_fa45f3ee1f6d441c8afe072bdbd1ad68
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_fa45f3ee1f6d441c8afe072bdbd1ad68
INFO     flujo:telemetry.py:54 Saved state for run_id=run_fa45f3ee1f6d441c8afe072bdbd1ad68
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: solution
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 70449b630084d62bd0a078fcba0811da4e0995a950f3e1b70feba764129a1b2d
DEBUG    flujo:telemetry.py:54 Cache miss for step: solution
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: solution
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 1 failed: No more outputs available
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: solution
WARNING  flujo:telemetry.py:54 Step 'solution' agent execution attempt 2 failed: No more outputs available
ERROR    flujo:telemetry.py:54 Step 'solution' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'solution' failed. Halting pipeline execution.
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_fa45f3ee1f6d441c8afe072bdbd1ad68
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_fa45f3ee1f6d441c8afe072bdbd1ad68
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_fa45f3ee1f6d441c8afe072bdbd1ad68
INFO     flujo:telemetry.py:54 Saved state for run_id=run_fa45f3ee1f6d441c8afe072bdbd1ad68
DEBUG    flujo.application.core.state_manager:state_manager.py:294 Skipped context serialization for unchanged run run_fa45f3ee1f6d441c8afe072bdbd1ad68
INFO     flujo:telemetry.py:54 Saved state for run_id=run_fa45f3ee1f6d441c8afe072bdbd1ad68
DEBUG    flujo.application.core.state_manager:state_manager.py:259 About to serialize context for run run_fa45f3ee1f6d441c8afe072bdbd1ad68
DEBUG    flujo.application.core.state_manager:state_manager.py:262 Successfully serialized context for run run_fa45f3ee1f6d441c8afe072bdbd1ad68
DEBUG    flujo.application.core.state_manager:state_manager.py:266 Serialized and cached context for run run_fa45f3ee1f6d441c8afe072bdbd1ad68
INFO     flujo:telemetry.py:54 Saved state for run_id=run_fa45f3ee1f6d441c8afe072bdbd1ad68
____________ test_plugin_receives_context_and_strict_plugin_errors _____________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_pipeline_context.py:123: in test_plugin_receives_context_and_strict_plugin_errors
    assert kwargs_plugin.kwargs == {}
E   assert None == {}
E    +  where None = <tests.unit.test_pipeline_context.KwargsPlugin object at 0x10c315110>.kwargs
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:08,610 - flujo - INFO - Counting string output as 1 token for step 's': 'in'
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 Flujo backend: LocalBackend, executor: <class 'flujo.application.core.ultra_executor.ExecutorCore'>
DEBUG    flujo:telemetry.py:54 === LOCAL BACKEND EXECUTE STEP ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: s
DEBUG    flujo:telemetry.py:54 Step is ParallelStep: False
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: s
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: c31d34d8c9e4cb1a8ced973d0e1dbed0bc0f17656f7bb6f22afb98770d84a788
DEBUG    flujo:telemetry.py:54 Cache miss for step: s
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: s
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: s
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 's': 'in'
DEBUG    flujo:telemetry.py:54 Step 's' completed successfully
_______________________ test_prompt_robust_serialization _______________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_prompt_formatter.py:61: in test_prompt_robust_serialization
    assert "<unserializable: Unknown>" in result
E   AssertionError: assert '<unserializable: Unknown>' in 'Value: <unserializable: Wrapper>'
__________ TestSQLiteBackendEdgeCases.test_backup_glob_always_raises ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:840: in test_backup_glob_always_raises
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2282-12-22 12:13:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2282-12-22 12:13:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
___ TestSchemaMigrationRobustness.test_migration_handles_corrupted_database ____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_schema_migration_robustness.py:146: in test_migration_handles_corrupted_database
    await backend.save_run_start(run_data)
flujo/state/backends/sqlite.py:1076: in save_run_start
    await self._ensure_init()
flujo/state/backends/sqlite.py:554: in _ensure_init
    await self._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:08,683 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:08,783 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
2025-08-04 22:45:08,783 - flujo - ERROR - Failed to initialize DB: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize DB: file is not a database
_ TestSchemaMigrationRobustness.test_migration_handles_schema_version_changes __
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_schema_migration_robustness.py:317: in test_migration_handles_schema_version_changes
    await backend2.save_run_start(run_data)
flujo/state/backends/sqlite.py:1109: in save_run_start
    await self._with_retries(_save)
flujo/state/backends/sqlite.py:602: in _with_retries
    result = await coro_func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:1085: in _save
    await db.execute(
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.OperationalError: table runs has no column named execution_time_ms
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:09,040 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_migration_handles_schema_0/test_migration.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_migration_handles_schema_0/test_migration.db
_ TestSchemaMigrationRobustness.test_migration_handles_foreign_key_constraints _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_schema_migration_robustness.py:396: in test_migration_handles_foreign_key_constraints
    steps = await backend2.list_run_steps("fk_test")
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:1201: in list_run_steps
    cursor = await db.execute(
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.OperationalError: no such column: start_time
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:09,077 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_migration_handles_foreign0/test_migration.db
2025-08-04 22:45:09,081 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_migration_handles_foreign0/test_migration.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_migration_handles_foreign0/test_migration.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw2/test_migration_handles_foreign0/test_migration.db
_____ TestCLIPerformanceEdgeCases.test_lens_list_with_large_mixed_database _____
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_cli_performance_edge_cases.py:100: in test_lens_list_with_large_mixed_database
    assert result.exit_code == 0
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
---------------------------- Captured stdout setup -----------------------------
2025-08-04 22:45:06,280 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_lens_list_with_large_mixe0/mixed_ops.db
------------------------------ Captured log setup ------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_lens_list_with_large_mixe0/mixed_ops.db
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:09,096 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_lens_list_with_large_mixe0/mixed_ops.db

Large mixed database list performance: 0.008s
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_lens_list_with_large_mixe0/mixed_ops.db
___________ TestSerializationEdgeCases.test_custom_types_edge_cases ____________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_serialization_edge_cases.py:299: in test_custom_types_edge_cases
    with pytest.raises(TypeError):
E   Failed: DID NOT RAISE <class 'TypeError'>
____________ TestSerializationEdgeCases.test_collections_edge_cases ____________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_serialization_edge_cases.py:386: in test_collections_edge_cases
    serialized = safe_serialize(request_data)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/utils/serialization.py:533: in safe_serialize
    return {
flujo/utils/serialization.py:536: in <dictcomp>
    ): safe_serialize(
flujo/utils/serialization.py:533: in safe_serialize
    return {
flujo/utils/serialization.py:536: in <dictcomp>
    ): safe_serialize(
flujo/utils/serialization.py:528: in safe_serialize
    raise TypeError(
E   TypeError: Object of type OrderedDict is not serializable. Register a custom serializer using register_custom_serializer(OrderedDict, lambda obj: obj.__dict__) or provide a default_serializer.
_________ TestSQLiteBackendEdgeCases.test_backup_unlink_always_raises __________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:862: in test_backup_unlink_always_raises
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2282-12-22 12:13:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2282-12-22 12:13:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
____ TestSQLiteBackendEdgeCases.test_backup_permission_and_race_conditions _____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_sqlite_backend_edge_cases.py:895: in test_backup_permission_and_race_conditions
    await backend._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2282-12-22 12:13:30,000 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2282-12-22 12:13:30,000 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
__________________________ test_stateful_hitl_resume ___________________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_stateful_hitl.py:61: in test_stateful_hitl_resume
    assert ctx.scratchpad["pre"] == "hello"
           ^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'pre'
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:09,504 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw0/test_stateful_hitl_resume0/state.db
2025-08-04 22:45:09,508 - flujo - INFO - Saved state for run_id=hitl_run
2025-08-04 22:45:09,509 - flujo - INFO - Counting string output as 1 token for step 'setup': 'setup'
2025-08-04 22:45:09,511 - flujo - INFO - Saved state for run_id=hitl_run
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw0/test_stateful_hitl_resume0/state.db
INFO     flujo:telemetry.py:54 Saved state for run_id=hitl_run
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'setup': 'setup'
INFO     flujo:telemetry.py:54 Saved state for run_id=hitl_run
_____________________________ test_basic_streaming _____________________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_streaming_pipeline.py:35: in test_basic_streaming
    assert "".join(chunks) == "hi"
E   AssertionError: assert '' == 'hi'
E     
E     - hi
_____________________ test_context_and_resources_in_stream _____________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_streaming_pipeline.py:83: in test_context_and_resources_in_stream
    assert final.final_pipeline_context.count == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = Ctx(count=0).count
E    +    where Ctx(count=0) = PipelineResult(step_history=[StepResult(name='s', output=<async_generator object CtxStreamAgent.stream at 0x10aae5a80>, success=True, attempts=1, latency_s=0.00010258401744067669, token_counts=0, cost_usd=0.0, feedback=None, branch_context=Ctx(count=0), metadata_={}, step_history=[])], total_cost_usd=0.0, total_tokens=0, final_pipeline_context=Ctx(count=0), trace_tree=Span(span_id='7f058838-99bb-4f54-b335-da14edde44c4', name='pipeline_root', start_time=286542.660477625, end_time=286542.660805583, parent_span_id=None, attributes={'initial_input': '3'}, children=[Span(span_id='7e57c0aa-7be8-41fa-93d4-fe3ef975b8c3', name='s', start_time=286542.660532541, end_time=286542.660732541, parent_span_id='7f058838-99bb-4f54-b335-da14edde44c4', attributes={'step_type': 'Step', 'step_input': '3', 'success': True, 'attempts': 1, 'latency_s': 0.00010258401744067669, 'cost_usd': 0.0, 'token_counts': 0}, children=[], status='completed')], status='completed')).final_pipeline_context
___________ test_pipeline_handles_streaming_agent_failure_gracefully ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_streaming_pipeline.py:101: in test_pipeline_handles_streaming_agent_failure_gracefully
    assert collected == ["H", "e", "l"]
E   AssertionError: assert [] == ['H', 'e', 'l']
E     
E     Right contains 3 more items, first extra item: 'H'
E     
E     Full diff:
E     + []
E     - [
E     -     'H',
E     -     'e',
E     -     'l',
E     - ]
___________________ test_non_strict_validation_pass_through ____________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_strict_validation.py:22: in test_non_strict_validation_pass_through
    assert hist.success is True
E   AssertionError: assert False is True
E    +  where False = StepResult(name='validate', output='ok', success=False, attempts=1, latency_s=0.00015708297723904252, token_counts=1, cost_usd=0.0, feedback='Validation failed: Validation failed: bad', branch_context=None, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:09,844 - flujo - INFO - Counting string output as 1 token for step 'validate': 'ok'
2025-08-04 22:45:09,844 - flujo - WARNING - Step 'validate' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:09,844 - flujo - ERROR - Step 'validate' validation failed: Validation failed: bad
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'validate': 'ok'
ERROR    flujo:telemetry.py:54 Step 'validate' validation failed: Validation failed: bad
WARNING  flujo:telemetry.py:54 Step 'validate' failed. Halting pipeline execution.
_____________________ test_strict_validation_drops_output ______________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_strict_validation.py:35: in test_strict_validation_drops_output
    assert hist.output is None
E   AssertionError: assert 'bad' is None
E    +  where 'bad' = StepResult(name='validate', output='bad', success=False, attempts=1, latency_s=0.0001559159718453884, token_counts=1, cost_usd=0.0, feedback='Validation failed: Validation failed: bad', branch_context=None, metadata_={}, step_history=[]).output
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:09,848 - flujo - INFO - Counting string output as 1 token for step 'validate': 'bad'
2025-08-04 22:45:09,848 - flujo - WARNING - Step 'validate' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:09,848 - flujo - ERROR - Step 'validate' validation failed: Validation failed: bad
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'validate': 'bad'
ERROR    flujo:telemetry.py:54 Step 'validate' validation failed: Validation failed: bad
WARNING  flujo:telemetry.py:54 Step 'validate' failed. Halting pipeline execution.
______________ test_streaming_step_returns_stepresult_on_failure _______________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_unified_error_handling.py:70: in test_streaming_step_returns_stepresult_on_failure
    assert not result.success
E   AssertionError: assert not True
E    +  where True = StepResult(name='failing', output=<async_generator object FailingAgent.stream at 0x10c121380>, success=True, attempts=1, latency_s=7.270800415426493e-05, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).success
____________________ test_critical_exceptions_are_re_raised ____________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_unified_error_handling.py:117: in test_critical_exceptions_are_re_raised
    with pytest.raises(InfiniteFallbackError, match="Test infinite fallback"):
E   Failed: DID NOT RAISE <class 'flujo.exceptions.InfiniteFallbackError'>
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:09,899 - flujo - ERROR - Step 'paused' failed with critical error: Test pause
------------------------------ Captured log call -------------------------------
ERROR    flujo:telemetry.py:54 Step 'paused' failed with critical error: Test pause
_________________________ test_consistent_api_contract _________________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_unified_error_handling.py:141: in test_consistent_api_contract
    assert not result2.success
E   AssertionError: assert not True
E    +  where True = StepResult(name='failing', output=<async_generator object FailingAgent.stream at 0x10c122880>, success=True, attempts=1, latency_s=5.162501474842429e-05, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:09,902 - flujo - WARNING - Step 'failing' agent execution attempt 1 failed: Test failure
2025-08-04 22:45:09,902 - flujo - WARNING - Step 'failing' agent execution attempt 2 failed: Test failure
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:09,902 - flujo - ERROR - Step 'failing' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'failing' agent execution attempt 1 failed: Test failure
WARNING  flujo:telemetry.py:54 Step 'failing' agent execution attempt 2 failed: Test failure
ERROR    flujo:telemetry.py:54 Step 'failing' agent failed after 2 attempts
_________________________ test_governor_with_loop_step _________________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_usage_governor.py:137: in test_governor_with_loop_step
    with pytest.raises(UsageLimitExceededError) as exc_info:
E   Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === governed_loop
2025-08-04 22:45:10,130 - flujo - INFO - LoopStep 'governed_loop': Starting Iteration 1/5
2025-08-04 22:45:10,131 - flujo - INFO - Using explicit cost from 'MockAgentOutput' for step 'metric_step': cost=$0.1, tokens=100
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_f897053c71be4be88d88741418e48a5e', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_f897053c71be4be88d88741418e48a5e
[DEBUG] actual_source_value: run_f897053c71be4be88d88741418e48a5e
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_f897053c71be4be88d88741418e48a5e', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_f897053c71be4be88d88741418e48a5e
[DEBUG] actual_source_value: run_f897053c71be4be88d88741418e48a5e
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
2025-08-04 22:45:10,131 - flujo - WARNING - Exit condition evaluation failed: 'int' object has no attribute 'value'
2025-08-04 22:45:10,131 - flujo - WARNING - Step 'governed_loop' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:10,131 - flujo - ERROR - Error in exit condition for LoopStep 'governed_loop': 'int' object has no attribute 'value'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'governed_loop': Starting Iteration 1/5
INFO     flujo:telemetry.py:54 Using explicit cost from 'MockAgentOutput' for step 'metric_step': cost=$0.1, tokens=100
WARNING  flujo:telemetry.py:54 Exit condition evaluation failed: 'int' object has no attribute 'value'
ERROR    flujo:telemetry.py:54 Error in exit condition for LoopStep 'governed_loop': 'int' object has no attribute 'value'
WARNING  flujo:telemetry.py:54 Step 'governed_loop' failed. Halting pipeline execution.
_________________ test_governor_halts_loop_step_mid_iteration __________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_usage_governor.py:166: in test_governor_halts_loop_step_mid_iteration
    assert not loop_result.success
E   AssertionError: assert not True
E    +  where True = StepResult(name='breach_loop', output=3, success=True, attempts=2, latency_s=0.0022381669841706753, token_counts=200, cost_usd=0.2, feedback='', branch_context=PipelineContext(run_id='run_8c2d3d5f032245efb77072ac83f742db', initial_prompt='0', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'iterations': 2, 'exit_reason': 'usage_limit_exceeded'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === breach_loop
2025-08-04 22:45:10,138 - flujo - INFO - LoopStep 'breach_loop': Starting Iteration 1/5
2025-08-04 22:45:10,138 - flujo - INFO - Using explicit cost from 'MockAgentOutput' for step 'metric_step': cost=$0.1, tokens=100
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_8c2d3d5f032245efb77072ac83f742db', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] actual_source_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_8c2d3d5f032245efb77072ac83f742db', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] actual_source_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: True, iteration_count: 1, max_iterations: 5
[DEBUG] Iteration 1: Usage limit checking condition met
[DEBUG] Iteration 1: Checking usage limits - cost: 0.1, limit: 0.25
[DEBUG] Iteration 1: Usage limits check passed
2025-08-04 22:45:10,139 - flujo - INFO - LoopStep 'breach_loop': Starting Iteration 2/5
2025-08-04 22:45:10,139 - flujo - INFO - Using explicit cost from 'MockAgentOutput' for step 'metric_step': cost=$0.1, tokens=100
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_8c2d3d5f032245efb77072ac83f742db', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] actual_source_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_8c2d3d5f032245efb77072ac83f742db', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] actual_source_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 2: Checking condition - limits: True, iteration_count: 2, max_iterations: 5
[DEBUG] Iteration 2: Usage limit checking condition met
[DEBUG] Iteration 2: Checking usage limits - cost: 0.2, limit: 0.25
[DEBUG] Iteration 2: Usage limits check passed
2025-08-04 22:45:10,140 - flujo - INFO - LoopStep 'breach_loop': Starting Iteration 3/5
2025-08-04 22:45:10,140 - flujo - INFO - Using explicit cost from 'MockAgentOutput' for step 'metric_step': cost=$0.1, tokens=100
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_8c2d3d5f032245efb77072ac83f742db', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] actual_source_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_8c2d3d5f032245efb77072ac83f742db', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] actual_source_value: run_8c2d3d5f032245efb77072ac83f742db
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Raising UsageLimitExceededError: current=0.2, prospective=0.30000000000000004, limit=0.25
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'breach_loop': Starting Iteration 1/5
INFO     flujo:telemetry.py:54 Using explicit cost from 'MockAgentOutput' for step 'metric_step': cost=$0.1, tokens=100
INFO     flujo:telemetry.py:54 LoopStep 'breach_loop': Starting Iteration 2/5
INFO     flujo:telemetry.py:54 Using explicit cost from 'MockAgentOutput' for step 'metric_step': cost=$0.1, tokens=100
INFO     flujo:telemetry.py:54 LoopStep 'breach_loop': Starting Iteration 3/5
INFO     flujo:telemetry.py:54 Using explicit cost from 'MockAgentOutput' for step 'metric_step': cost=$0.1, tokens=100
________________ test_governor_loop_with_nested_parallel_limit _________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_usage_governor.py:226: in test_governor_loop_with_nested_parallel_limit
    assert not loop_result.success
E   AssertionError: assert not True
E    +  where True = StepResult(name='outer_loop', output={'a': 1, 'b': 1}, success=True, attempts=2, latency_s=0.0033984999754466116, token_counts=400, cost_usd=0.4, feedback='', branch_context=PipelineContext(run_id='run_3df897314f0d49c696206775fcf74c67', initial_prompt='0', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'iterations': 2, 'exit_reason': 'usage_limit_exceeded'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === outer_loop
2025-08-04 22:45:10,152 - flujo - INFO - LoopStep 'outer_loop': Starting Iteration 1/10
2025-08-04 22:45:10,152 - flujo - INFO - Using explicit cost from 'MockAgentOutput' for step 'a': cost=$0.1, tokens=100
2025-08-04 22:45:10,153 - flujo - INFO - Using explicit cost from 'MockAgentOutput' for step 'b': cost=$0.1, tokens=100
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_3df897314f0d49c696206775fcf74c67', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] actual_source_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_3df897314f0d49c696206775fcf74c67', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] actual_source_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: True, iteration_count: 1, max_iterations: 10
[DEBUG] Iteration 1: Usage limit checking condition met
[DEBUG] Iteration 1: Checking usage limits - cost: 0.2, limit: 0.5
[DEBUG] Iteration 1: Usage limits check passed
2025-08-04 22:45:10,154 - flujo - INFO - LoopStep 'outer_loop': Starting Iteration 2/10
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_3df897314f0d49c696206775fcf74c67', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] actual_source_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_3df897314f0d49c696206775fcf74c67', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] actual_source_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 2: Checking condition - limits: True, iteration_count: 2, max_iterations: 10
[DEBUG] Iteration 2: Usage limit checking condition met
[DEBUG] Iteration 2: Checking usage limits - cost: 0.4, limit: 0.5
[DEBUG] Iteration 2: Usage limits check passed
2025-08-04 22:45:10,155 - flujo - INFO - LoopStep 'outer_loop': Starting Iteration 3/10
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_3df897314f0d49c696206775fcf74c67', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] actual_source_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_3df897314f0d49c696206775fcf74c67', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] actual_source_value: run_3df897314f0d49c696206775fcf74c67
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Raising UsageLimitExceededError: current=0.4, prospective=0.6000000000000001, limit=0.5
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'outer_loop': Starting Iteration 1/10
INFO     flujo:telemetry.py:54 Using explicit cost from 'MockAgentOutput' for step 'a': cost=$0.1, tokens=100
INFO     flujo:telemetry.py:54 Using explicit cost from 'MockAgentOutput' for step 'b': cost=$0.1, tokens=100
INFO     flujo:telemetry.py:54 LoopStep 'outer_loop': Starting Iteration 2/10
INFO     flujo:telemetry.py:54 LoopStep 'outer_loop': Starting Iteration 3/10
______________________ test_persist_feedback_and_results _______________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_validation_persistence.py:43: in test_persist_feedback_and_results
    assert ctx.feedback_history and ctx.feedback_history[0] == result.step_history[0].feedback
E   assert ([])
E    +  where [] = Ctx(feedback_history=[], validation_history=[]).feedback_history
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:10,237 - flujo - INFO - Counting string output as 1 token for step 'validate': 'bad'
2025-08-04 22:45:10,238 - flujo - WARNING - Step 'validate' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:10,237 - flujo - ERROR - Step 'validate' validation failed: Validation failed: bad output
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'validate': 'bad'
ERROR    flujo:telemetry.py:54 Step 'validate' validation failed: Validation failed: bad output
WARNING  flujo:telemetry.py:54 Step 'validate' failed. Halting pipeline execution.
_______________________ test_persist_results_on_success ________________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_validation_persistence.py:65: in test_persist_results_on_success
    assert len(ctx.validation_history) == 1
E   assert 0 == 1
E    +  where 0 = len([])
E    +    where [] = Ctx(feedback_history=[], validation_history=[]).validation_history
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:10,241 - flujo - INFO - Counting string output as 1 token for step 'validate': 'ok'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'validate': 'ok'
______ TestConditionalStepRegression.test_metrics_accumulation_regression ______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/regression/test_conditional_step_regression.py:136: in test_metrics_accumulation_regression
    assert result.latency_s == 1.0
E   assert 0.00018525001360103488 == 1.0
E    +  where 0.00018525001360103488 = StepResult(name='test_conditional', output='test_output', success=True, attempts=1, latency_s=0.00018525001360103488, token_counts=100, cost_usd=0.01, feedback="Branch 'branch_a' executed successfully", branch_context=PipelineContext(run_id='run_be1a040b650b49d88cf51e1555d47050', initial_prompt='test_data', scratchpad={}, hitl_history=[], command_log=[]), metadata_={'executed_branch_key': 'branch_a'}, step_history=[]).latency_s
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:10,422 - flujo - INFO - Condition evaluated to branch key 'branch_a'
2025-08-04 22:45:10,422 - flujo - INFO - Executing branch for key 'branch_a'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Condition evaluated to branch key 'branch_a'
INFO     flujo:telemetry.py:54 Executing branch for key 'branch_a'
________ TestConditionalStepRegression.test_context_handling_regression ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/regression/test_conditional_step_regression.py:181: in test_context_handling_regression
    assert context_setter_called is True
E   assert False is True
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:10,427 - flujo - INFO - Condition evaluated to branch key 'branch_a'
2025-08-04 22:45:10,427 - flujo - INFO - Executing branch for key 'branch_a'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'dict'>
[DEBUG] source_context type: <class 'dict'>
[DEBUG] excluded_fields: {'command_log'}
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:10,428 - flujo.utils.context - ERROR - Failed to merge context updates: 'dict' object has no attribute '__dict__'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Condition evaluated to branch key 'branch_a'
INFO     flujo:telemetry.py:54 Executing branch for key 'branch_a'
ERROR    flujo.utils.context:context.py:276 Failed to merge context updates: 'dict' object has no attribute '__dict__'
____ TestOptimizationBackwardCompatibility.test_configuration_serialization ____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/regression/test_executor_core_optimization_regression.py:292: in test_configuration_serialization
    restored_config = OptimizationConfig.from_dict(config_dict)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: type object 'OptimizationConfig' has no attribute 'from_dict'
___________ TestOptimizationErrorHandling.test_invalid_step_handling ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/regression/test_executor_core_optimization_regression.py:352: in test_invalid_step_handling
    result = await error_prone_executor.execute(invalid_step, test_data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
flujo/application/core/ultra_executor.py:564: in execute
    return await self._execute_simple_step(
flujo/application/core/ultra_executor.py:718: in _execute_simple_step
    primary_result = StepResult(
E   pydantic_core._pydantic_core.ValidationError: 1 validation error for StepResult
E   name
E     Input should be a valid string [type=string_type, input_value=<Mock name='mock.step.name' id='4498202256'>, input_type=Mock]
E       For further information visit https://errors.pydantic.dev/2.11/v/string_type
___________ TestSerializationProperties.test_unsupported_types_raise ___________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_serialization_properties.py:267: in test_unsupported_types_raise
    with pytest.raises(TypeError):
E   Failed: DID NOT RAISE <class 'TypeError'>
_ TestSQLiteBackendRobustness.test_schema_migration_handles_corrupted_database _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_sqlite_backend_robustness.py:167: in test_schema_migration_handles_corrupted_database
    await backend.save_run_start(run_data)
flujo/state/backends/sqlite.py:1076: in save_run_start
    await self._ensure_init()
flujo/state/backends/sqlite.py:554: in _ensure_init
    await self._init_db()
flujo/state/backends/sqlite.py:450: in _init_db
    await self._init_db(retry_count + 1, max_retries)
flujo/state/backends/sqlite.py:317: in _init_db
    await db.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging for better concurrency
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.DatabaseError: file is not a database
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:11,292 - flujo - WARNING - Database initialization failed, retrying (1/1): file is not a database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:11,393 - flujo - ERROR - Failed to initialize database after 1 retries: file is not a database
2025-08-04 22:45:11,394 - flujo - ERROR - Failed to initialize DB: file is not a database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: file is not a database
ERROR    flujo:telemetry.py:54 Failed to initialize DB: file is not a database
_ TestSQLiteBackendBackupEdgeCases.test_backup_with_special_characters_in_filename _
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_sqlite_edge_cases.py:145: in test_backup_with_special_characters_in_filename
    assert len(backup_files) == 1
E   assert 0 == 1
E    +  where 0 = len([])
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:11,592 - flujo - WARNING - Database initialization failed, retrying (1/1): Corrupted database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:11,693 - flujo - ERROR - Failed to initialize database after 1 retries: Corrupted database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): Corrupted database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: Corrupted database
_______ TestSQLiteBackendBackupEdgeCases.test_backup_with_long_filename ________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_sqlite_edge_cases.py:169: in test_backup_with_long_filename
    assert len(backup_files) == 1
E   assert 0 == 1
E    +  where 0 = len([])
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:11,699 - flujo - WARNING - Database initialization failed, retrying (1/1): Corrupted database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:11,800 - flujo - ERROR - Failed to initialize database after 1 retries: Corrupted database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): Corrupted database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: Corrupted database
____ TestSQLiteBackendBackupEdgeCases.test_backup_with_no_write_permissions ____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_sqlite_edge_cases.py:191: in test_backup_with_no_write_permissions
    assert not db_path.exists()
E   AssertionError: assert not True
E    +  where True = exists()
E    +    where exists = PosixPath('/private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw0/test_backup_with_no_write_perm0/test.db').exists
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:11,808 - flujo - WARNING - Database initialization failed, retrying (1/1): Corrupted database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:11,909 - flujo - ERROR - Failed to initialize database after 1 retries: Corrupted database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): Corrupted database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: Corrupted database
_______ TestCLIPerformanceEdgeCases.test_lens_list_with_various_filters ________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_cli_performance_edge_cases.py:125: in test_lens_list_with_various_filters
    assert result.exit_code == 0
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
---------------------------- Captured stdout setup -----------------------------
2025-08-04 22:45:09,107 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_lens_list_with_various_fi0/mixed_ops.db
------------------------------ Captured log setup ------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_lens_list_with_various_fi0/mixed_ops.db
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:11,983 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_lens_list_with_various_fi0/mixed_ops.db
Filter ['lens', 'list', '--status', 'completed'] performance: 0.007s
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_lens_list_with_various_fi0/mixed_ops.db
______ TestSQLiteBackendBackupEdgeCases.test_backup_with_disk_full_error _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_sqlite_edge_cases.py:213: in test_backup_with_disk_full_error
    assert not db_path.exists()
E   AssertionError: assert not True
E    +  where True = exists()
E    +    where exists = PosixPath('/private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw0/test_backup_with_disk_full_err0/test.db').exists
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:11,915 - flujo - WARNING - Database initialization failed, retrying (1/1): Corrupted database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:12,016 - flujo - ERROR - Failed to initialize database after 1 retries: Corrupted database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): Corrupted database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: Corrupted database
___ TestSQLiteBackendConcurrencyEdgeCases.test_concurrent_backup_operations ____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_sqlite_edge_cases.py:482: in test_concurrent_backup_operations
    assert len(backup_files) >= 1  # At least one backup should be created
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   assert 0 >= 1
E    +  where 0 = len([])
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:12,189 - flujo - WARNING - Database initialization failed, retrying (1/1): Corrupted database
2025-08-04 22:45:12,189 - flujo - WARNING - Database initialization failed, retrying (1/1): Corrupted database
2025-08-04 22:45:12,189 - flujo - WARNING - Database initialization failed, retrying (1/1): Corrupted database
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:12,289 - flujo - ERROR - Failed to initialize database after 1 retries: Corrupted database
2025-08-04 22:45:12,289 - flujo - ERROR - Failed to initialize database after 1 retries: Corrupted database
2025-08-04 22:45:12,289 - flujo - ERROR - Failed to initialize database after 1 retries: Corrupted database
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): Corrupted database
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): Corrupted database
WARNING  flujo:telemetry.py:54 Database initialization failed, retrying (1/1): Corrupted database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: Corrupted database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: Corrupted database
ERROR    flujo:telemetry.py:54 Failed to initialize database after 1 retries: Corrupted database
___________ TestStreamingBytesBug.test_text_streaming_agent_protocol ___________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_streaming_bytes_bug.py:102: in test_text_streaming_agent_protocol
    assert result.output == "hello world!"
E   AssertionError: assert <async_generator object StubTextStreamingAgent.stream at 0x11e1cb060> == 'hello world!'
E    +  where <async_generator object StubTextStreamingAgent.stream at 0x11e1cb060> = StepResult(name='test_step', output=<async_generator object StubTextStreamingAgent.stream at 0x11e1cb060>, success=True, attempts=1, latency_s=0.00013991701416671276, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: test_step
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 19b6258137d65e0c3fe9077b12bca6628666336ca1a1e1f948ba2149a17b40b3
DEBUG    flujo:telemetry.py:54 Cache miss for step: test_step
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: test_step
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: test_step
DEBUG    flujo:telemetry.py:54 Step 'test_step' completed successfully
__________ TestStreamingBytesBug.test_binary_streaming_agent_protocol __________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_streaming_bytes_bug.py:120: in test_binary_streaming_agent_protocol
    assert result.output == b"data1data2data3"
E   AssertionError: assert <async_generator object StubBinaryStreamingAgent.stream at 0x10cb90ac0> == b'data1data2data3'
E    +  where <async_generator object StubBinaryStreamingAgent.stream at 0x10cb90ac0> = StepResult(name='test_step', output=<async_generator object StubBinaryStreamingAgent.stream at 0x10cb90ac0>, success=True, attempts=1, latency_s=0.00011037499643862247, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: test_step
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 19b6258137d65e0c3fe9077b12bca6628666336ca1a1e1f948ba2149a17b40b3
DEBUG    flujo:telemetry.py:54 Cache miss for step: test_step
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: test_step
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: test_step
DEBUG    flujo:telemetry.py:54 Step 'test_step' completed successfully
__________ TestStreamingBytesBug.test_legacy_streaming_agent_fallback __________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_streaming_bytes_bug.py:138: in test_legacy_streaming_agent_fallback
    assert result.output == "hello world!"
E   AssertionError: assert <async_generator object StubLegacyStreamingAgent.stream at 0x10caba180> == 'hello world!'
E    +  where <async_generator object StubLegacyStreamingAgent.stream at 0x10caba180> = StepResult(name='test_step', output=<async_generator object StubLegacyStreamingAgent.stream at 0x10caba180>, success=True, attempts=1, latency_s=0.00010820897296071053, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: test_step
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 19b6258137d65e0c3fe9077b12bca6628666336ca1a1e1f948ba2149a17b40b3
DEBUG    flujo:telemetry.py:54 Cache miss for step: test_step
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: test_step
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: test_step
DEBUG    flujo:telemetry.py:54 Step 'test_step' completed successfully
______ TestStreamingBytesBug.test_legacy_binary_streaming_agent_fallback _______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_streaming_bytes_bug.py:158: in test_legacy_binary_streaming_agent_fallback
    assert result.output == b"data1data2data3"
E   AssertionError: assert <async_generator object StubLegacyStreamingAgent.stream at 0x10cc69460> == b'data1data2data3'
E    +  where <async_generator object StubLegacyStreamingAgent.stream at 0x10cc69460> = StepResult(name='test_step', output=<async_generator object StubLegacyStreamingAgent.stream at 0x10cc69460>, success=True, attempts=1, latency_s=0.00012870796490460634, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: test_step
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 19b6258137d65e0c3fe9077b12bca6628666336ca1a1e1f948ba2149a17b40b3
DEBUG    flujo:telemetry.py:54 Cache miss for step: test_step
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: test_step
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: test_step
DEBUG    flujo:telemetry.py:54 Step 'test_step' completed successfully
_______ TestStreamingBytesBug.test_mixed_stream_types_handled_gracefully _______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_streaming_bytes_bug.py:178: in test_mixed_stream_types_handled_gracefully
    assert result.output == expected_str
E   assert <async_generator object StubLegacyStreamingAgent.stream at 0x10cc68e40> == "['text', b'binary', 'more_text']"
E    +  where <async_generator object StubLegacyStreamingAgent.stream at 0x10cc68e40> = StepResult(name='test_step', output=<async_generator object StubLegacyStreamingAgent.stream at 0x10cc68e40>, success=True, attempts=1, latency_s=0.00011845800327137113, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: test_step
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 19b6258137d65e0c3fe9077b12bca6628666336ca1a1e1f948ba2149a17b40b3
DEBUG    flujo:telemetry.py:54 Cache miss for step: test_step
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: test_step
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: test_step
DEBUG    flujo:telemetry.py:54 Step 'test_step' completed successfully
__________ TestStreamingBytesBug.test_empty_stream_handled_correctly ___________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_streaming_bytes_bug.py:196: in test_empty_stream_handled_correctly
    assert result.output == ""
E   AssertionError: assert <async_generator object StubTextStreamingAgent.stream at 0x10caa19a0> == ''
E    +  where <async_generator object StubTextStreamingAgent.stream at 0x10caa19a0> = StepResult(name='test_step', output=<async_generator object StubTextStreamingAgent.stream at 0x10caa19a0>, success=True, attempts=1, latency_s=0.00011470797471702099, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: test_step
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 19b6258137d65e0c3fe9077b12bca6628666336ca1a1e1f948ba2149a17b40b3
DEBUG    flujo:telemetry.py:54 Cache miss for step: test_step
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: test_step
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: test_step
DEBUG    flujo:telemetry.py:54 Step 'test_step' completed successfully
________________ TestStreamingBytesBug.test_single_bytes_chunk _________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_streaming_bytes_bug.py:214: in test_single_bytes_chunk
    assert result.output == b"single_chunk"
E   AssertionError: assert <async_generator object StubBinaryStreamingAgent.stream at 0x10d85adc0> == b'single_chunk'
E    +  where <async_generator object StubBinaryStreamingAgent.stream at 0x10d85adc0> = StepResult(name='test_step', output=<async_generator object StubBinaryStreamingAgent.stream at 0x10d85adc0>, success=True, attempts=1, latency_s=0.0001040829811245203, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: test_step
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 19b6258137d65e0c3fe9077b12bca6628666336ca1a1e1f948ba2149a17b40b3
DEBUG    flujo:telemetry.py:54 Cache miss for step: test_step
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: test_step
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: test_step
DEBUG    flujo:telemetry.py:54 Step 'test_step' completed successfully
________________ TestStreamingBytesBug.test_large_bytes_stream _________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_streaming_bytes_bug.py:233: in test_large_bytes_stream
    assert result.output == expected_bytes
E   AssertionError: assert <async_generator object StubBinaryStreamingAgent.stream at 0x10cb90f20> == b'chunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkc...unkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunk'
E    +  where <async_generator object StubBinaryStreamingAgent.stream at 0x10cb90f20> = StepResult(name='test_step', output=<async_generator object StubBinaryStreamingAgent.stream at 0x10cb90f20>, success=True, attempts=1, latency_s=0.00033425004221498966, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: test_step
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 19b6258137d65e0c3fe9077b12bca6628666336ca1a1e1f948ba2149a17b40b3
DEBUG    flujo:telemetry.py:54 Cache miss for step: test_step
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: test_step
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: test_step
DEBUG    flujo:telemetry.py:54 Step 'test_step' completed successfully
________________ TestUltraStepExecutor.test_streaming_execution ________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:394: in test_streaming_execution
    assert chunks == ["chunk1", "chunk2", "chunk3"]
E   AssertionError: assert [] == ['chunk1', 'chunk2', 'chunk3']
E     
E     Right contains 3 more items, first extra item: 'chunk1'
E     
E     Full diff:
E     + []
E     - [
E     -     'chunk1',
E     -     'chunk2',
E     -     'chunk3',
E     - ]
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'unittest.mock.Mock'>
DEBUG    flujo:telemetry.py:54 Step name: test_step
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 2a2916c330388defe8be2f7353a618b8c719d5f6c50d3e07fee518c3775cd7aa
DEBUG    flujo:telemetry.py:54 Cache miss for step: test_step
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: test_step
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/4 for step: test_step
DEBUG    flujo:telemetry.py:54 Step 'test_step' completed successfully
_______________ TestUltraStepExecutor.test_cache_key_generation ________________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:466: in test_cache_key_generation
    assert key1 != key2  # Different steps should have different keys
    ^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
__________ TestUltraStepExecutor.test_agent_identification_stability ___________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:654: in test_agent_identification_stability
    assert key1 != key2
E   AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
__________ TestUltraStepExecutor.test_regression_cache_key_stability ___________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1026: in test_regression_cache_key_stability
    assert key1 != key2
E   AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
______ TestUltraStepExecutor.test_step_with_plugins_validators_fallbacks _______
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:756: in test_step_with_plugins_validators_fallbacks
    assert result.success is True
E   assert False is True
E    +  where False = StepResult(name='step_with_plugins', output='test_output', success=False, attempts=1, latency_s=0.0009979999740608037, token_counts=1, cost_usd=0.0, feedback="Plugin failed: unsupported operand type(s) for +=: 'float' and 'Mock'", branch_context=None, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:12,824 - flujo - INFO - Counting string output as 1 token for step 'step_with_plugins': 'test_output'
2025-08-04 22:45:12,824 - flujo - INFO - Step 'step_with_plugins' redirecting to agent: <Mock name='mock.redirect_to' id='4499155344'>
2025-08-04 22:45:12,825 - flujo - INFO - Using explicit cost from 'Mock' for step 'step_with_plugins': cost=$<Mock name='mock.redirect_to.run().cost_usd' id='4499156368'>, tokens=<Mock name='mock.redirect_to.run().token_counts' id='4499155920'>
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:12,825 - flujo - ERROR - Step 'step_with_plugins' plugin failed: unsupported operand type(s) for +=: 'float' and 'Mock'
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'unittest.mock.Mock'>
DEBUG    flujo:telemetry.py:54 Step name: step_with_plugins
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 8bae053b97dba1521d8473312ac1b9f1bce10789053d5ece4d9d505697d6a2e3
DEBUG    flujo:telemetry.py:54 Cache miss for step: step_with_plugins
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: step_with_plugins
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/4 for step: step_with_plugins
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'step_with_plugins': 'test_output'
INFO     flujo:telemetry.py:54 Step 'step_with_plugins' redirecting to agent: <Mock name='mock.redirect_to' id='4499155344'>
INFO     flujo:telemetry.py:54 Using explicit cost from 'Mock' for step 'step_with_plugins': cost=$<Mock name='mock.redirect_to.run().cost_usd' id='4499156368'>, tokens=<Mock name='mock.redirect_to.run().token_counts' id='4499155920'>
ERROR    flujo:telemetry.py:54 Step 'step_with_plugins' plugin failed: unsupported operand type(s) for +=: 'float' and 'Mock'
__________ TestUltraStepExecutor.test_regression_cache_with_resources __________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1126: in test_regression_cache_with_resources
    assert "cache_hit" not in (result3.metadata_ or {})
E   AssertionError: assert 'cache_hit' not in (({'cache_hit': True}))
E    +  where {'cache_hit': True} = StepResult(name='test_step', output='test output 1', success=True, attempts=1, latency_s=0.0001176249934360385, token_counts=1, cost_usd=0.0, feedback=None, branch_context=None, metadata_={'cache_hit': True}, step_history=[]).metadata_
___ TestUltraStepExecutor.test_regression_cache_key_includes_all_components ____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1155: in test_regression_cache_key_includes_all_components
    assert key1 != key4  # Different resources
    ^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert '0322bc00cdaa9a3baf7b8908cdefa0610676e2a0f1713a70e95a0a2bd266937f' != '0322bc00cdaa9a3baf7b8908cdefa0610676e2a0f1713a70e95a0a2bd266937f'
__ TestUltraStepExecutor.test_regression_agent_identification_includes_module __
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1284: in test_regression_agent_identification_includes_module
    assert key1 != key2
E   AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
_ TestUltraStepExecutor.test_regression_cache_performance_with_module_dataclasses _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1397: in test_regression_cache_performance_with_module_dataclasses
    assert len(executor.cache._store) > 0
E   assert 0 > 0
E    +  where 0 = len(OrderedDict())
E    +    where OrderedDict() = _LRUCache(max_size=1000, ttl=3600, _store=OrderedDict())._store
E    +      where _LRUCache(max_size=1000, ttl=3600, _store=OrderedDict()) = <flujo.application.core.ultra_executor.ExecutorCore object at 0x10c4151d0>.cache
____ TestUltraStepExecutor.test_regression_consistent_agent_config_hashing _____
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1332: in test_regression_consistent_agent_config_hashing
    assert key1 != key3
E   AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
__________ TestUltraStepExecutor.test_unified_error_handling_contract __________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1652: in test_unified_error_handling_contract
    assert not result.success
E   AssertionError: assert not True
E    +  where True = StepResult(name='streaming', output=<async_generator object TestUltraStepExecutor.test_unified_error_handling_contract.<locals>.FailingAgent.stream at 0x10ad9ab20>, success=True, attempts=1, latency_s=6.870803190395236e-05, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:12,891 - flujo - WARNING - Step 'simple' agent execution attempt 1 failed: Test failure
2025-08-04 22:45:12,891 - flujo - WARNING - Step 'simple' agent execution attempt 2 failed: Test failure
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:12,891 - flujo - ERROR - Step 'simple' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: simple
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 74fed62e517d81c553239341ab53c54fb53dddccb50000978a73a6ba5f8faef6
DEBUG    flujo:telemetry.py:54 Cache miss for step: simple
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: simple
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: simple
WARNING  flujo:telemetry.py:54 Step 'simple' agent execution attempt 1 failed: Test failure
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/2 for step: simple
WARNING  flujo:telemetry.py:54 Step 'simple' agent execution attempt 2 failed: Test failure
ERROR    flujo:telemetry.py:54 Step 'simple' agent failed after 2 attempts
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: streaming
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: a3312512564039268d72281332fa94bd5dcbc0505ee0f27bc478c70322c343d2
DEBUG    flujo:telemetry.py:54 Cache miss for step: streaming
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: streaming
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/2 for step: streaming
DEBUG    flujo:telemetry.py:54 Step 'streaming' completed successfully
_ TestUltraStepExecutor.test_regression_agent_identification_handles_edge_cases _
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1462: in test_regression_agent_identification_handles_edge_cases
    assert len(set(keys)) == len(keys), "Different agent types should generate different keys"
E   AssertionError: Different agent types should generate different keys
E   assert 1 == 4
E    +  where 1 = len({'d0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'})
E    +    where {'d0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'} = set(['d0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'])
E    +  and   4 = len(['d0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'])
____ TestUltraStepExecutor.test_regression_input_validation_ultra_executor _____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1828: in test_regression_input_validation_ultra_executor
    with pytest.raises(ValueError, match="cache_ttl must be non-negative"):
E   Failed: DID NOT RAISE <class 'ValueError'>
____ TestUltraStepExecutor.test_regression_independent_latency_measurement _____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1936: in test_regression_independent_latency_measurement
    assert start_time_captured, "start_time should be captured inside the retry loop"
E   AssertionError: start_time should be captured inside the retry loop
E   assert False
____ TestUltraStepExecutor.test_regression_constructor_validation_preserved ____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1994: in test_regression_constructor_validation_preserved
    assert "if cache_ttl < 0:" in content, "UltraStepExecutor should validate cache_ttl"
E   AssertionError: UltraStepExecutor should validate cache_ttl
E   assert 'if cache_ttl < 0:' in '"""\nUltra-optimized step executor v2 with modular, policy-driven architecture.\n\nThis is a complete rewrite of the UltraStepExecutor with:\n- Modular design with clear separation of concerns\n- Deterministic behavior across processes and restarts\n- Pluggable components via dependency injection\n- Robust isolation between concerns\n- Exhaustive accounting of successful and failed attempts\n- Backward compatibility with existing SDK signatures\n\nAuthor: Flujo Team\nVersion: 2.0\n"""\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextvars\nimport time\nimport hashlib\nimport copy\nimport multiprocessing\nfrom abc import abstractmethod\nfrom collections import OrderedDict, defaultdict\nfrom dataclasses import dataclass, field\nfrom functools import cached_property, wraps\nfrom multiprocessing import cpu_count\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Generic,\n    Optional,\n    Protocol,\n    TypeVar,\n    Dict,\n    List,\n    Type,\n    Tuple,\n    Union,\n    cast,\n)\nimport types\nfrom types import SimpleNamespace\nfrom asyncio import Task\nimport weakref\nfrom weakref import WeakKeyDictionary\n\nfrom ...domain.dsl.step import ... of ExecutorCore with additional performance features."""\n    \n    def get_optimization_stats(self):\n        """Get optimization statistics."""\n        return {\n            \'cache_hits\': 0,\n            \'cache_misses\': 0,\n            \'optimization_enabled\': True,\n            \'performance_score\': 95.0,\n            \'execution_stats\': {\n                \'total_steps\': 0,\n                \'successful_steps\': 0,\n                \'failed_steps\': 0,\n                \'average_execution_time\': 0.0,\n            },\n            \'optimization_config\': OptimizationConfig().to_dict(),\n        }\n    \n    def get_config_manager(self):\n        """Get configuration manager."""\n        return {\n            \'current_config\': OptimizationConfig(),\n            \'available_configs\': [\'default\', \'high_performance\', \'memory_efficient\'],\n        }\n    \n    def get_performance_recommendations(self):\n        """Get performance recommendations."""\n        return [\n            "Consider increasing cache size for better performance",\n            "Enable object pooling for memory optimization",\n            "Use batch processing for multiple steps",\n        ]'
___ TestUltraExecutorCumulativeLimits.test_usage_tracker_cumulative_tracking ___
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor_cumulative_limits.py:44: in test_usage_tracker_cumulative_tracking
    total_cost, total_tokens = await usage_tracker.get_current_totals()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
_____ TestUltraExecutorCumulativeLimits.test_usage_tracker_limit_checking ______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor_cumulative_limits.py:60: in test_usage_tracker_limit_checking
    cost, tokens = await usage_tracker.get_current_totals()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
______ TestUltraExecutorCumulativeLimits.test_usage_tracker_thread_safety ______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor_cumulative_limits.py:84: in test_usage_tracker_thread_safety
    final_cost, final_tokens = await add_usage_concurrently()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/unit/test_ultra_executor_cumulative_limits.py:81: in add_usage_concurrently
    return await usage_tracker.get_current_totals()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
___ TestUltraExecutorCumulativeLimits.test_legacy_guard_method_compatibility ___
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor_cumulative_limits.py:106: in test_legacy_guard_method_compatibility
    cost, tokens = await usage_tracker.get_current_totals()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
__ TestUltraExecutorCumulativeLimits.test_usage_tracker_multiple_limit_checks __
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor_cumulative_limits.py:118: in test_usage_tracker_multiple_limit_checks
    cost, tokens = await usage_tracker.get_current_totals()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
_______ TestUltraExecutorCumulativeLimits.test_usage_tracker_zero_limits _______
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor_cumulative_limits.py:154: in test_usage_tracker_zero_limits
    cost, tokens = await usage_tracker.get_current_totals()
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
___ TestUltraExecutorCumulativeLimits.test_usage_tracker_precision_handling ____
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor_cumulative_limits.py:168: in test_usage_tracker_precision_handling
    total_cost, total_tokens = await usage_tracker.get_current_totals()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
_____________ TestUltraStepExecutor.test_retry_latency_measurement _____________
[gw0] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_ultra_executor.py:1763: in test_retry_latency_measurement
    assert result.latency_s < 0.1  # Should be closer to 0.05s than 0.15s
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert 0.20221333298832178 < 0.1
E    +  where 0.20221333298832178 = StepResult(name='retry_test', output='success', success=True, attempts=3, latency_s=0.20221333298832178, token_counts=1, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).latency_s
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:13,223 - flujo - WARNING - Step 'retry_test' agent execution attempt 1 failed: Attempt 1 failed
2025-08-04 22:45:13,325 - flujo - WARNING - Step 'retry_test' agent execution attempt 2 failed: Attempt 2 failed
2025-08-04 22:45:13,325 - flujo - INFO - Counting string output as 1 token for step 'retry_test': 'success'
------------------------------ Captured log call -------------------------------
DEBUG    flujo:telemetry.py:54 === EXECUTOR CORE EXECUTE ===
DEBUG    flujo:telemetry.py:54 Step type: <class 'flujo.domain.dsl.step.Step'>
DEBUG    flujo:telemetry.py:54 Step name: retry_test
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with breach_event: False
DEBUG    flujo:telemetry.py:54 ExecutorCore.execute called with limits: None
DEBUG    flujo:telemetry.py:54 Generated cache key: 0baaf0dee6aad1c4d963ccc47f49e2e96df86cb621aecc6c2fe8ae747d9fe4fa
DEBUG    flujo:telemetry.py:54 Cache miss for step: retry_test
DEBUG    flujo:telemetry.py:54 Routing to agent step handler: retry_test
DEBUG    flujo:telemetry.py:54 Agent execution attempt 1/4 for step: retry_test
WARNING  flujo:telemetry.py:54 Step 'retry_test' agent execution attempt 1 failed: Attempt 1 failed
DEBUG    flujo:telemetry.py:54 Agent execution attempt 2/4 for step: retry_test
WARNING  flujo:telemetry.py:54 Step 'retry_test' agent execution attempt 2 failed: Attempt 2 failed
DEBUG    flujo:telemetry.py:54 Agent execution attempt 3/4 for step: retry_test
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'retry_test': 'success'
DEBUG    flujo:telemetry.py:54 Step 'retry_test' completed successfully
___ TestCLIPerformanceEdgeCases.test_cli_performance_with_concurrent_access ____
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_cli_performance_edge_cases.py:174: in test_cli_performance_with_concurrent_access
    assert result.exit_code == 0, f"Command {commands[i]} failed: {result.stdout}"
E   AssertionError: Command ['lens', 'list'] failed: 
E   assert 1 == 0
E    +  where 1 = <Result SystemExit(1)>.exit_code
---------------------------- Captured stdout setup -----------------------------
2025-08-04 22:45:16,555 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
------------------------------ Captured log setup ------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:19,055 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
2025-08-04 22:45:19,062 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
2025-08-04 22:45:19,068 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
2025-08-04 22:45:19,076 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
2025-08-04 22:45:19,083 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
Concurrent CLI access total time: 0.036s
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_cli_performance_with_conc0/mixed_ops.db
_________ TestCLIPerformanceEdgeCases.test_database_index_optimization _________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_cli_performance_edge_cases.py:237: in test_database_index_optimization
    runs = asyncio.run(backend.list_runs(status="completed"))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:190: in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:118: in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:650: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:916: in list_runs
    async with db.execute(query, params) as cursor:
.venv/lib/python3.11/site-packages/aiosqlite/context.py:41: in __aenter__
    self._obj = await self._coro
                ^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.OperationalError: no such column: start_time
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:21,621 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_database_index_optimizati0/index_test.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_database_index_optimizati0/index_test.db
____________ TestCLIPerformanceEdgeCases.test_database_memory_usage ____________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/unit/test_cli_performance_edge_cases.py:325: in test_database_memory_usage
    runs = asyncio.run(backend.list_runs(limit=50))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:190: in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:118: in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py:650: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
flujo/state/backends/sqlite.py:916: in list_runs
    async with db.execute(query, params) as cursor:
.venv/lib/python3.11/site-packages/aiosqlite/context.py:41: in __aenter__
    self._obj = await self._coro
                ^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:183: in execute
    cursor = await self._execute(self._conn.execute, sql, parameters)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:122: in _execute
    return await future
           ^^^^^^^^^^^^
.venv/lib/python3.11/site-packages/aiosqlite/core.py:105: in run
    result = function()
             ^^^^^^^^^^
E   sqlite3.OperationalError: no such column: start_time
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:21,997 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_database_memory_usage0/memory_test.db
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw3/test_database_memory_usage0/memory_test.db
_____________________ test_golden_transcript_agentic_loop ______________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/e2e/test_golden_transcript_agentic_loop.py:124: in test_golden_transcript_agentic_loop
    assert final_context.scratchpad.get("status") == "paused"
E   AssertionError: assert 'failed' == 'paused'
E     
E     - paused
E     + failed
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === AgenticExplorationLoop
2025-08-04 22:45:24,657 - flujo - INFO - LoopStep 'AgenticExplorationLoop': Starting Iteration 1/5
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.e2e.test_golden_transcript_agentic_loop.AgenticLoopContext'>
[DEBUG] source_context type: <class 'tests.e2e.test_golden_transcript_agentic_loop.AgenticLoopContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_dd7547bff6044741b95f751827889cb4', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [{'turn': 2, 'generated_command': {'type': 'run_agent', 'agent_name': 'tool1', 'input_data': 'test_input_1'}, 'execution_result': 'tool1_processed_test_input_1', 'timestamp': datetime.datetime(2025, 8, 5, 5, 45, 24, 658501, tzinfo=datetime.timezone.utc)}], 'final_state': ''}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_dd7547bff6044741b95f751827889cb4
[DEBUG] actual_source_value: run_dd7547bff6044741b95f751827889cb4
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: final_state
[DEBUG] current_value: 
[DEBUG] actual_source_value: 
[DEBUG] Field unchanged: final_state
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.e2e.test_golden_transcript_agentic_loop.AgenticLoopContext'>
[DEBUG] source_context type: <class 'tests.e2e.test_golden_transcript_agentic_loop.AgenticLoopContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_dd7547bff6044741b95f751827889cb4', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [{'turn': 2, 'generated_command': {'type': 'run_agent', 'agent_name': 'tool1', 'input_data': 'test_input_1'}, 'execution_result': 'tool1_processed_test_input_1', 'timestamp': datetime.datetime(2025, 8, 5, 5, 45, 24, 658501, tzinfo=datetime.timezone.utc)}], 'final_state': ''}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_dd7547bff6044741b95f751827889cb4
[DEBUG] actual_source_value: run_dd7547bff6044741b95f751827889cb4
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: final_state
[DEBUG] current_value: 
[DEBUG] actual_source_value: 
[DEBUG] Field unchanged: final_state
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 5
2025-08-04 22:45:24,659 - flujo - INFO - LoopStep 'AgenticExplorationLoop': Starting Iteration 2/5
2025-08-04 22:45:24,661 - flujo - WARNING - Step 'AgenticExplorationLoop' failed. Halting pipeline execution.
2025-08-04 22:45:24,661 - flujo.application.core.state_manager - WARNING - Failed to serialize context for run run_dd7547bff6044741b95f751827889cb4: Object of type datetime is not JSON serializable
2025-08-04 22:45:24,662 - flujo.application.core.state_manager - WARNING - Failed to serialize context for run run_dd7547bff6044741b95f751827889cb4: Object of type datetime is not JSON serializable
2025-08-04 22:45:24,662 - flujo.application.core.state_manager - WARNING - Failed to serialize context for run run_dd7547bff6044741b95f751827889cb4: Object of type datetime is not JSON serializable
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:24,661 - flujo - ERROR - Step 'command_executor_step' failed with critical error: Please review the first result
2025-08-04 22:45:24,661 - flujo - ERROR - Error in LoopStep 'AgenticExplorationLoop': Please review the first result
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'AgenticExplorationLoop': Starting Iteration 1/5
INFO     flujo:telemetry.py:54 LoopStep 'AgenticExplorationLoop': Starting Iteration 2/5
ERROR    flujo:telemetry.py:54 Step 'command_executor_step' failed with critical error: Please review the first result
ERROR    flujo:telemetry.py:54 Error in LoopStep 'AgenticExplorationLoop': Please review the first result
WARNING  flujo:telemetry.py:54 Step 'AgenticExplorationLoop' failed. Halting pipeline execution.
WARNING  flujo.application.core.state_manager:state_manager.py:297 Failed to serialize context for run run_dd7547bff6044741b95f751827889cb4: Object of type datetime is not JSON serializable
WARNING  flujo.application.core.state_manager:state_manager.py:297 Failed to serialize context for run run_dd7547bff6044741b95f751827889cb4: Object of type datetime is not JSON serializable
WARNING  flujo.application.core.state_manager:state_manager.py:297 Failed to serialize context for run run_dd7547bff6044741b95f751827889cb4: Object of type datetime is not JSON serializable
___________________ test_golden_transcript_dynamic_parallel ____________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/e2e/test_golden_transcript_dynamic_parallel.py:128: in test_golden_transcript_dynamic_parallel
    assert len(final_context.executed_branches) == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = len([])
E    +    where [] = DynamicParallelContext(run_id='run_51ff88ac2ff844a9a20463e8eaeb0aeb', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], executed_branches=[], branch_results={}, total_failures=0).executed_branches
----------------------------- Captured stdout call -----------------------------
[DEBUG] Branch branch1 called with data: test_input
[DEBUG] Branch branch1 context is None: False
[DEBUG] Branch branch1 context.executed_branches before: []
[DEBUG] Branch branch1 context.executed_branches after: ['branch1']
[DEBUG] Branch branch1 context.branch_results: {'branch1': 'branch1_processed_test_input'}
2025-08-04 22:45:24,707 - flujo - INFO - Counting string output as 1 token for step 'branch1': 'branch1_processed_test_input'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'branch1': 'branch1_processed_test_input'
______________ test_golden_transcript_dynamic_parallel_selective _______________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/e2e/test_golden_transcript_dynamic_parallel.py:204: in test_golden_transcript_dynamic_parallel_selective
    assert len(final_context.executed_branches) == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = len([])
E    +    where [] = DynamicParallelContext(run_id='run_2ea67dbd5e6441f19aaeff7a3b60727d', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], executed_branches=[], branch_results={}, total_failures=0).executed_branches
----------------------------- Captured stdout call -----------------------------
[DEBUG] Branch branch1 called with data: selective_input
[DEBUG] Branch branch1 context is None: False
[DEBUG] Branch branch1 context.executed_branches before: []
[DEBUG] Branch branch1 context.executed_branches after: ['branch1']
[DEBUG] Branch branch1 context.branch_results: {'branch1': 'branch1_processed_selective_input'}
2025-08-04 22:45:24,713 - flujo - INFO - Counting string output as 1 token for step 'branch1': 'branch1_processed_selective_input'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'branch1': 'branch1_processed_selective_input'
_________________ test_golden_transcript_refine_max_iterations _________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/e2e/test_golden_transcript_refine.py:142: in test_golden_transcript_refine_max_iterations
    assert isinstance(final_output, RefinementCheck)
E   AssertionError: assert False
E    +  where False = isinstance({'value': 2}, <class 'flujo.domain.models.RefinementCheck'>)
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === test_refinement_max
2025-08-04 22:45:24,725 - flujo - INFO - LoopStep 'test_refinement_max': Starting Iteration 1/2
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.e2e.test_golden_transcript_refine.RefinementContext'>
[DEBUG] source_context type: <class 'tests.e2e.test_golden_transcript_refine.RefinementContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_72d46e56da9d4aa8855515d25a817962', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_iterations': 0, 'final_refined_value': 0}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_72d46e56da9d4aa8855515d25a817962
[DEBUG] actual_source_value: run_72d46e56da9d4aa8855515d25a817962
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_iterations
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: refinement_iterations
[DEBUG] Processing field: final_refined_value
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: final_refined_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.e2e.test_golden_transcript_refine.RefinementContext'>
[DEBUG] source_context type: <class 'tests.e2e.test_golden_transcript_refine.RefinementContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_72d46e56da9d4aa8855515d25a817962', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_iterations': 0, 'final_refined_value': 0}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_72d46e56da9d4aa8855515d25a817962
[DEBUG] actual_source_value: run_72d46e56da9d4aa8855515d25a817962
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_iterations
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: refinement_iterations
[DEBUG] Processing field: final_refined_value
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: final_refined_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 2
2025-08-04 22:45:24,727 - flujo - INFO - LoopStep 'test_refinement_max': Starting Iteration 2/2
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.e2e.test_golden_transcript_refine.RefinementContext'>
[DEBUG] source_context type: <class 'tests.e2e.test_golden_transcript_refine.RefinementContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_72d46e56da9d4aa8855515d25a817962', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_iterations': 0, 'final_refined_value': 0}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_72d46e56da9d4aa8855515d25a817962
[DEBUG] actual_source_value: run_72d46e56da9d4aa8855515d25a817962
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_iterations
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: refinement_iterations
[DEBUG] Processing field: final_refined_value
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: final_refined_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.e2e.test_golden_transcript_refine.RefinementContext'>
[DEBUG] source_context type: <class 'tests.e2e.test_golden_transcript_refine.RefinementContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_72d46e56da9d4aa8855515d25a817962', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'refinement_iterations': 0, 'final_refined_value': 0}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_72d46e56da9d4aa8855515d25a817962
[DEBUG] actual_source_value: run_72d46e56da9d4aa8855515d25a817962
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: refinement_iterations
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: refinement_iterations
[DEBUG] Processing field: final_refined_value
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: final_refined_value
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 2: Checking condition - limits: False, iteration_count: 2, max_iterations: 2
2025-08-04 22:45:24,728 - flujo - WARNING - Step 'test_refinement_max' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'test_refinement_max': Starting Iteration 1/2
INFO     flujo:telemetry.py:54 LoopStep 'test_refinement_max': Starting Iteration 2/2
WARNING  flujo:telemetry.py:54 Step 'test_refinement_max' failed. Halting pipeline execution.
___ TestHITLStepMigrationIntegration.test_hitl_step_with_different_contexts ____
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_hitl_step_migration_integration.py:141: in test_hitl_step_with_different_contexts
    await executor_core._handle_hitl_step(
flujo/application/core/ultra_executor.py:2370: in _handle_hitl_step
    raise PausedException(message)
E   flujo.exceptions.PausedException: test_data

During handling of the above exception, another exception occurred:
tests/integration/test_hitl_step_migration_integration.py:153: in test_hitl_step_with_different_contexts
    assert populated_context.scratchpad["existing_key"] == "existing_value"
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'existing_key'
_______________________ test_programmatic_check_failure ________________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_hybrid_validation.py:57: in test_programmatic_check_failure
    assert "FailValidator" in history.feedback
E   AssertionError: assert 'FailValidator' in 'Validation failed: Validation failed: bad output'
E    +  where 'Validation failed: Validation failed: bad output' = StepResult(name='validate', output='bad', success=False, attempts=1, latency_s=9.637494804337621e-05, token_counts=1, cost_usd=0.0, feedback='Validation failed: Validation failed: bad output', branch_context=None, metadata_={}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:24,985 - flujo - INFO - Counting string output as 1 token for step 'validate': 'bad'
2025-08-04 22:45:24,985 - flujo - WARNING - Step 'validate' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:24,985 - flujo - ERROR - Step 'validate' validation failed: Validation failed: bad output
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'validate': 'bad'
ERROR    flujo:telemetry.py:54 Step 'validate' validation failed: Validation failed: bad output
WARNING  flujo:telemetry.py:54 Step 'validate' failed. Halting pipeline execution.
___________________________ test_aggregated_feedback ___________________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_hybrid_validation.py:83: in test_aggregated_feedback
    assert "plugin fail" in fb
E   AssertionError: assert 'plugin fail' in 'Validation failed: Validation failed: bad output'
----------------------------- Captured stdout call -----------------------------
2025-08-04 22:45:24,991 - flujo - INFO - Counting string output as 1 token for step 'validate': 'bad'
2025-08-04 22:45:24,991 - flujo - WARNING - Step 'validate' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:24,991 - flujo - ERROR - Step 'validate' validation failed: Validation failed: bad output
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'validate': 'bad'
ERROR    flujo:telemetry.py:54 Step 'validate' validation failed: Validation failed: bad output
WARNING  flujo:telemetry.py:54 Step 'validate' failed. Halting pipeline execution.
__________________ test_loop_step_iteration_spans_and_logging __________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_loop_step_execution.py:461: in test_loop_step_iteration_spans_and_logging
    assert "LoopStep 'loop_log' exit condition met at iteration 2." in infos
E   assert "LoopStep 'loop_log' exit condition met at iteration 2." in ["LoopStep 'loop_log': Starting Iteration 1/2", "LoopStep 'loop_log': Starting Iteration 2/2"]
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === loop_log
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_772660d1d34c4861880feef864713579', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_772660d1d34c4861880feef864713579
[DEBUG] actual_source_value: run_772660d1d34c4861880feef864713579
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_772660d1d34c4861880feef864713579', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_772660d1d34c4861880feef864713579
[DEBUG] actual_source_value: run_772660d1d34c4861880feef864713579
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 2
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_772660d1d34c4861880feef864713579', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_772660d1d34c4861880feef864713579
[DEBUG] actual_source_value: run_772660d1d34c4861880feef864713579
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] source_context type: <class 'flujo.domain.models.PipelineContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_772660d1d34c4861880feef864713579', 'initial_prompt': '0', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_772660d1d34c4861880feef864713579
[DEBUG] actual_source_value: run_772660d1d34c4861880feef864713579
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
__________________ test_loop_step_error_logging_in_callables ___________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_loop_step_execution.py:508: in test_loop_step_error_logging_in_callables
    assert any("Error in iteration_input_mapper for LoopStep 'loop_err_log'" in m for m in errors)
E   assert False
E    +  where False = any(<generator object test_loop_step_error_logging_in_callables.<locals>.<genexpr> at 0x10aaa7b90>)
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === loop_err_log
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_loop_step_execution.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_loop_step_execution.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'counter': 0}
[DEBUG] Processing field: counter
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: counter
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_loop_step_execution.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_loop_step_execution.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'counter': 0}
[DEBUG] Processing field: counter
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: counter
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 3
________________ test_loop_with_context_updates_error_handling _________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_loop_with_context_updates.py:227: in test_loop_with_context_updates_error_handling
    assert result.step_history[-1].success is True  # Should succeed when exiting by condition
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AssertionError: assert False is True
E    +  where False = StepResult(name='error_loop', output=None, success=False, attempts=2, latency_s=0.0014936249935999513, token_counts=0, cost_usd=0.0, feedback='Loop exited by condition, but last iteration body failed: Loop body failed: Agent execution failed with RuntimeError: Intentional failure', branch_context=LoopContext(run_id='run_c6ae1c925024447da87557289a2f046c', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], iteration_count=3, accumulated_value=0, loop_exit_reason='', final_state={}), metadata_={'iterations': 2, 'exit_reason': 'condition_with_body_failure'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === error_loop
2025-08-04 22:45:25,177 - flujo - INFO - LoopStep 'error_loop': Starting Iteration 1/5
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_loop_with_context_updates.LoopContext'>
[DEBUG] source_context type: <class 'tests.integration.test_loop_with_context_updates.LoopContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_c6ae1c925024447da87557289a2f046c', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'iteration_count': 1, 'accumulated_value': 0, 'loop_exit_reason': '', 'final_state': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_c6ae1c925024447da87557289a2f046c
[DEBUG] actual_source_value: run_c6ae1c925024447da87557289a2f046c
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: iteration_count
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: iteration_count
[DEBUG] Processing field: accumulated_value
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: accumulated_value
[DEBUG] Processing field: loop_exit_reason
[DEBUG] current_value: 
[DEBUG] actual_source_value: 
[DEBUG] Field unchanged: loop_exit_reason
[DEBUG] Processing field: final_state
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: final_state
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_loop_with_context_updates.LoopContext'>
[DEBUG] source_context type: <class 'tests.integration.test_loop_with_context_updates.LoopContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_c6ae1c925024447da87557289a2f046c', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'iteration_count': 1, 'accumulated_value': 0, 'loop_exit_reason': '', 'final_state': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_c6ae1c925024447da87557289a2f046c
[DEBUG] actual_source_value: run_c6ae1c925024447da87557289a2f046c
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: iteration_count
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: iteration_count
[DEBUG] Processing field: accumulated_value
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: accumulated_value
[DEBUG] Processing field: loop_exit_reason
[DEBUG] current_value: 
[DEBUG] actual_source_value: 
[DEBUG] Field unchanged: loop_exit_reason
[DEBUG] Processing field: final_state
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: final_state
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 5
2025-08-04 22:45:25,178 - flujo - INFO - LoopStep 'error_loop': Starting Iteration 2/5
2025-08-04 22:45:25,178 - flujo - WARNING - Step 'failing_step' agent execution attempt 1 failed: Intentional failure
2025-08-04 22:45:25,178 - flujo - WARNING - Step 'failing_step' agent execution attempt 2 failed: Intentional failure
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_loop_with_context_updates.LoopContext'>
[DEBUG] source_context type: <class 'tests.integration.test_loop_with_context_updates.LoopContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_c6ae1c925024447da87557289a2f046c', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'iteration_count': 3, 'accumulated_value': 0, 'loop_exit_reason': '', 'final_state': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_c6ae1c925024447da87557289a2f046c
[DEBUG] actual_source_value: run_c6ae1c925024447da87557289a2f046c
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: iteration_count
[DEBUG] current_value: 3
[DEBUG] actual_source_value: 3
[DEBUG] Field unchanged: iteration_count
[DEBUG] Processing field: accumulated_value
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: accumulated_value
[DEBUG] Processing field: loop_exit_reason
[DEBUG] current_value: 
[DEBUG] actual_source_value: 
[DEBUG] Field unchanged: loop_exit_reason
[DEBUG] Processing field: final_state
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: final_state
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_loop_with_context_updates.LoopContext'>
[DEBUG] source_context type: <class 'tests.integration.test_loop_with_context_updates.LoopContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_c6ae1c925024447da87557289a2f046c', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'iteration_count': 3, 'accumulated_value': 0, 'loop_exit_reason': '', 'final_state': {}}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_c6ae1c925024447da87557289a2f046c
[DEBUG] actual_source_value: run_c6ae1c925024447da87557289a2f046c
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: iteration_count
[DEBUG] current_value: 3
[DEBUG] actual_source_value: 3
[DEBUG] Field unchanged: iteration_count
[DEBUG] Processing field: accumulated_value
[DEBUG] current_value: 0
[DEBUG] actual_source_value: 0
[DEBUG] Field unchanged: accumulated_value
[DEBUG] Processing field: loop_exit_reason
[DEBUG] current_value: 
[DEBUG] actual_source_value: 
[DEBUG] Field unchanged: loop_exit_reason
[DEBUG] Processing field: final_state
[DEBUG] current_value: {}
[DEBUG] actual_source_value: {}
[DEBUG] Merging dictionaries for field: final_state
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
2025-08-04 22:45:25,179 - flujo - WARNING - Step 'error_loop' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-04 22:45:25,178 - flujo - ERROR - Step 'failing_step' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'error_loop': Starting Iteration 1/5
INFO     flujo:telemetry.py:54 LoopStep 'error_loop': Starting Iteration 2/5
WARNING  flujo:telemetry.py:54 Step 'failing_step' agent execution attempt 1 failed: Intentional failure
WARNING  flujo:telemetry.py:54 Step 'failing_step' agent execution attempt 2 failed: Intentional failure
ERROR    flujo:telemetry.py:54 Step 'failing_step' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'error_loop' failed. Halting pipeline execution.
___________________________ test_map_over_sequential ___________________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_step.py:28: in test_map_over_sequential
    assert result.step_history[-1].output == [2, 4, 6]
E   assert [2] == [2, 4, 6]
E     
E     Right contains 2 more items, first extra item: 4
E     
E     Full diff:
E       [
E           2,
E     -     4,
E     -     6,
E       ]
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === mapper
2025-08-04 22:45:25,189 - flujo - INFO - LoopStep 'mapper': Starting Iteration 1/1
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [1, 2, 3]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [1, 2, 3]
[DEBUG] actual_source_value: [1, 2, 3]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [1, 2, 3]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [1, 2, 3]
[DEBUG] actual_source_value: [1, 2, 3]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,189 - flujo - WARNING - Step 'mapper' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'mapper': Starting Iteration 1/1
WARNING  flujo:telemetry.py:54 Step 'mapper' failed. Halting pipeline execution.
____________________________ test_map_over_parallel ____________________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_step.py:43: in test_map_over_parallel
    assert result.step_history[-1].output == [0, 1, 2, 3]
E   assert [0] == [0, 1, 2, 3]
E     
E     Right contains 3 more items, first extra item: 1
E     
E     Full diff:
E       [
E           0,
E     -     1,
E     -     2,
E     -     3,
E       ]
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === mapper_par
2025-08-04 22:45:25,193 - flujo - INFO - LoopStep 'mapper_par': Starting Iteration 1/1
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [0, 1, 2, 3]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [0, 1, 2, 3]
[DEBUG] actual_source_value: [0, 1, 2, 3]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [0, 1, 2, 3]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [0, 1, 2, 3]
[DEBUG] actual_source_value: [0, 1, 2, 3]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,204 - flujo - WARNING - Step 'mapper_par' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'mapper_par': Starting Iteration 1/1
WARNING  flujo:telemetry.py:54 Step 'mapper_par' failed. Halting pipeline execution.
______________________ test_map_over_reusable_after_empty ______________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_step.py:81: in test_map_over_reusable_after_empty
    assert second.step_history[-1].output == [6, 8]
E   assert [6] == [6, 8]
E     
E     Right contains one more item: 8
E     
E     Full diff:
E       [
E           6,
E     -     8,
E       ]
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === mapper_reuse
2025-08-04 22:45:25,211 - flujo - INFO - LoopStep 'mapper_reuse': Starting Iteration 1/1
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': []}
[DEBUG] Processing field: nums
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': []}
[DEBUG] Processing field: nums
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[KIRO DEBUG] === HANDLE LOOP STEP === mapper_reuse
2025-08-04 22:45:25,211 - flujo - INFO - LoopStep 'mapper_reuse': Starting Iteration 1/1
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [3, 4]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [3, 4]
[DEBUG] actual_source_value: [3, 4]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [3, 4]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [3, 4]
[DEBUG] actual_source_value: [3, 4]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,212 - flujo - WARNING - Step 'mapper_reuse' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'mapper_reuse': Starting Iteration 1/1
INFO     flujo:telemetry.py:54 LoopStep 'mapper_reuse': Starting Iteration 1/1
WARNING  flujo:telemetry.py:54 Step 'mapper_reuse' failed. Halting pipeline execution.
________________________ test_map_over_concurrent_runs _________________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_step.py:98: in test_map_over_concurrent_runs
    assert r1.step_history[-1].output == [2, 4]
E   assert [2] == [2, 4]
E     
E     Right contains one more item: 4
E     
E     Full diff:
E       [
E           2,
E     -     4,
E       ]
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === mapper_concurrent
2025-08-04 22:45:25,215 - flujo - INFO - LoopStep 'mapper_concurrent': Starting Iteration 1/1
[KIRO DEBUG] === HANDLE LOOP STEP === mapper_concurrent
2025-08-04 22:45:25,215 - flujo - INFO - LoopStep 'mapper_concurrent': Starting Iteration 1/1
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [1, 2]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [1, 2]
[DEBUG] actual_source_value: [1, 2]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [1, 2]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [1, 2]
[DEBUG] actual_source_value: [1, 2]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,216 - flujo - WARNING - Step 'mapper_concurrent' failed. Halting pipeline execution.
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [5, 6]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [5, 6]
[DEBUG] actual_source_value: [5, 6]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_step.Ctx'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'nums': [5, 6]}
[DEBUG] Processing field: nums
[DEBUG] current_value: [5, 6]
[DEBUG] actual_source_value: [5, 6]
[DEBUG] Merging lists for field: nums
[DEBUG] No new items to add to list field: nums
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,216 - flujo - WARNING - Step 'mapper_concurrent' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'mapper_concurrent': Starting Iteration 1/1
INFO     flujo:telemetry.py:54 LoopStep 'mapper_concurrent': Starting Iteration 1/1
WARNING  flujo:telemetry.py:54 Step 'mapper_concurrent' failed. Halting pipeline execution.
WARNING  flujo:telemetry.py:54 Step 'mapper_concurrent' failed. Halting pipeline execution.
___________________ test_map_over_with_context_updates_basic ___________________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_with_context_updates.py:125: in test_map_over_with_context_updates_basic
    assert result.step_history[-1].success is True
E   AssertionError: assert False is True
E    +  where False = StepResult(name='basic_map', output=['processed_item1'], success=False, attempts=1, latency_s=0.0009840839775279164, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_6b2993ab8497425eaf481436421b0e95', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': 'processed_item1'}, current_item='item1', processing_history=['processed_item1']), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === basic_map
2025-08-04 22:45:25,219 - flujo - INFO - LoopStep 'basic_map': Starting Iteration 1/1
2025-08-04 22:45:25,219 - flujo - INFO - Counting string output as 1 token for step 'map_item_step': 'processed_item1'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_6b2993ab8497425eaf481436421b0e95', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': 'processed_item1'}, 'current_item': 'item1', 'processing_history': ['processed_item1']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_6b2993ab8497425eaf481436421b0e95
[DEBUG] actual_source_value: run_6b2993ab8497425eaf481436421b0e95
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': 'processed_item1'}
[DEBUG] actual_source_value: {'item1': 'processed_item1'}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: ['processed_item1']
[DEBUG] actual_source_value: ['processed_item1']
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_6b2993ab8497425eaf481436421b0e95', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': 'processed_item1'}, 'current_item': 'item1', 'processing_history': ['processed_item1']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_6b2993ab8497425eaf481436421b0e95
[DEBUG] actual_source_value: run_6b2993ab8497425eaf481436421b0e95
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': 'processed_item1'}
[DEBUG] actual_source_value: {'item1': 'processed_item1'}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: ['processed_item1']
[DEBUG] actual_source_value: ['processed_item1']
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,220 - flujo - WARNING - Step 'basic_map' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'basic_map': Starting Iteration 1/1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'map_item_step': 'processed_item1'
WARNING  flujo:telemetry.py:54 Step 'basic_map' failed. Halting pipeline execution.
______________ test_map_over_with_context_updates_error_handling _______________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_with_context_updates.py:159: in test_map_over_with_context_updates_error_handling
    assert result.step_history[-1].success is True
E   AssertionError: assert False is True
E    +  where False = StepResult(name='error_map', output=['processed_item1'], success=False, attempts=1, latency_s=0.0009755000355653465, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_603541bbf1264022aea61f6a9731ba96', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': 'processed_item1'}, current_item='item1', processing_history=['attempted_item1']), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === error_map
2025-08-04 22:45:25,226 - flujo - INFO - LoopStep 'error_map': Starting Iteration 1/1
2025-08-04 22:45:25,226 - flujo - INFO - Counting string output as 1 token for step 'map_with_error_step': 'processed_item1'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_603541bbf1264022aea61f6a9731ba96', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': 'processed_item1'}, 'current_item': 'item1', 'processing_history': ['attempted_item1']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_603541bbf1264022aea61f6a9731ba96
[DEBUG] actual_source_value: run_603541bbf1264022aea61f6a9731ba96
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': 'processed_item1'}
[DEBUG] actual_source_value: {'item1': 'processed_item1'}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: ['attempted_item1']
[DEBUG] actual_source_value: ['attempted_item1']
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_603541bbf1264022aea61f6a9731ba96', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': 'processed_item1'}, 'current_item': 'item1', 'processing_history': ['attempted_item1']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_603541bbf1264022aea61f6a9731ba96
[DEBUG] actual_source_value: run_603541bbf1264022aea61f6a9731ba96
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': 'processed_item1'}
[DEBUG] actual_source_value: {'item1': 'processed_item1'}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: ['attempted_item1']
[DEBUG] actual_source_value: ['attempted_item1']
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,227 - flujo - WARNING - Step 'error_map' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'error_map': Starting Iteration 1/1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'map_with_error_step': 'processed_item1'
WARNING  flujo:telemetry.py:54 Step 'error_map' failed. Halting pipeline execution.
_____________ test_map_over_with_context_updates_context_dependent _____________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_with_context_updates.py:195: in test_map_over_with_context_updates_context_dependent
    assert result.step_history[-1].success is True
E   AssertionError: assert False is True
E    +  where False = StepResult(name='context_dependent_map', output=['early_processed_item1'], success=False, attempts=1, latency_s=0.0009080420131795108, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_4756b5422f344233be04a9e0239a8d29', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': 'early_processed_item1'}, current_item='item1', processing_history=['context_dependent_item1']), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === context_dependent_map
2025-08-04 22:45:25,232 - flujo - INFO - LoopStep 'context_dependent_map': Starting Iteration 1/1
2025-08-04 22:45:25,232 - flujo - INFO - Counting string output as 1 token for step 'map_with_context_dependent_step': 'early_processed_item1'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_4756b5422f344233be04a9e0239a8d29', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': 'early_processed_item1'}, 'current_item': 'item1', 'processing_history': ['context_dependent_item1']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_4756b5422f344233be04a9e0239a8d29
[DEBUG] actual_source_value: run_4756b5422f344233be04a9e0239a8d29
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': 'early_processed_item1'}
[DEBUG] actual_source_value: {'item1': 'early_processed_item1'}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: ['context_dependent_item1']
[DEBUG] actual_source_value: ['context_dependent_item1']
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_4756b5422f344233be04a9e0239a8d29', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': 'early_processed_item1'}, 'current_item': 'item1', 'processing_history': ['context_dependent_item1']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_4756b5422f344233be04a9e0239a8d29
[DEBUG] actual_source_value: run_4756b5422f344233be04a9e0239a8d29
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': 'early_processed_item1'}
[DEBUG] actual_source_value: {'item1': 'early_processed_item1'}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: ['context_dependent_item1']
[DEBUG] actual_source_value: ['context_dependent_item1']
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,233 - flujo - WARNING - Step 'context_dependent_map' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'context_dependent_map': Starting Iteration 1/1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'map_with_context_dependent_step': 'early_processed_item1'
WARNING  flujo:telemetry.py:54 Step 'context_dependent_map' failed. Halting pipeline execution.
______________ test_map_over_with_context_updates_nested_context _______________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_with_context_updates.py:229: in test_map_over_with_context_updates_nested_context
    assert result.step_history[-1].success is True
E   assert False is True
E    +  where False = StepResult(name='nested_context_map', output=["{'original_item': 'item1', 'processed_count': 1, 'history_length': 0}"], success=False, attempts=1, latency_s=0.0009553750278428197, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_a05b3834bf66480c9c79b025082a3143', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': {'original_item': 'item1', 'processed_count': 1, 'history_length': 0}}, current_item='item1', processing_history=['nested_item1']), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === nested_context_map
2025-08-04 22:45:25,238 - flujo - INFO - LoopStep 'nested_context_map': Starting Iteration 1/1
2025-08-04 22:45:25,238 - flujo - INFO - Counting string output as 1 token for step 'map_with_nested_context_step': '{'original_item': 'item1', 'processed_count': 1, '...'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_a05b3834bf66480c9c79b025082a3143', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': {'original_item': 'item1', 'processed_count': 1, 'history_length': 0}}, 'current_item': 'item1', 'processing_history': ['nested_item1']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_a05b3834bf66480c9c79b025082a3143
[DEBUG] actual_source_value: run_a05b3834bf66480c9c79b025082a3143
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': {'original_item': 'item1', 'processed_count': 1, 'history_length': 0}}
[DEBUG] actual_source_value: {'item1': {'original_item': 'item1', 'processed_count': 1, 'history_length': 0}}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: ['nested_item1']
[DEBUG] actual_source_value: ['nested_item1']
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_a05b3834bf66480c9c79b025082a3143', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': {'original_item': 'item1', 'processed_count': 1, 'history_length': 0}}, 'current_item': 'item1', 'processing_history': ['nested_item1']}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_a05b3834bf66480c9c79b025082a3143
[DEBUG] actual_source_value: run_a05b3834bf66480c9c79b025082a3143
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': {'original_item': 'item1', 'processed_count': 1, 'history_length': 0}}
[DEBUG] actual_source_value: {'item1': {'original_item': 'item1', 'processed_count': 1, 'history_length': 0}}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: ['nested_item1']
[DEBUG] actual_source_value: ['nested_item1']
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,239 - flujo - WARNING - Step 'nested_context_map' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'nested_context_map': Starting Iteration 1/1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'map_with_nested_context_step': '{'original_item': 'item1', 'processed_count': 1, '...'
WARNING  flujo:telemetry.py:54 Step 'nested_context_map' failed. Halting pipeline execution.
______________ test_map_over_with_context_updates_state_isolation ______________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_with_context_updates.py:277: in test_map_over_with_context_updates_state_isolation
    assert result.step_history[-1].success is True
E   assert False is True
E    +  where False = StepResult(name='isolation_map', output=["{'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}"], success=False, attempts=1, latency_s=0.0009082499891519547, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_8e3ebd36c1914089808009c49e092006', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': {'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}}, current_item='item1', processing_history=[]), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === isolation_map
2025-08-04 22:45:25,244 - flujo - INFO - LoopStep 'isolation_map': Starting Iteration 1/1
2025-08-04 22:45:25,244 - flujo - INFO - Counting string output as 1 token for step 'isolation_map_step': '{'item': 'item1', 'total_processed_at_start': 0, '...'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_8e3ebd36c1914089808009c49e092006', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': {'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}}, 'current_item': 'item1', 'processing_history': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_8e3ebd36c1914089808009c49e092006
[DEBUG] actual_source_value: run_8e3ebd36c1914089808009c49e092006
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': {'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}}
[DEBUG] actual_source_value: {'item1': {'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_8e3ebd36c1914089808009c49e092006', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': {'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}}, 'current_item': 'item1', 'processing_history': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_8e3ebd36c1914089808009c49e092006
[DEBUG] actual_source_value: run_8e3ebd36c1914089808009c49e092006
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': {'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}}
[DEBUG] actual_source_value: {'item1': {'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,245 - flujo - WARNING - Step 'isolation_map' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'isolation_map': Starting Iteration 1/1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'isolation_map_step': '{'item': 'item1', 'total_processed_at_start': 0, '...'
WARNING  flujo:telemetry.py:54 Step 'isolation_map' failed. Halting pipeline execution.
____________ test_map_over_with_context_updates_complex_aggregation ____________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_with_context_updates.py:329: in test_map_over_with_context_updates_complex_aggregation
    assert result.step_history[-1].success is True
E   assert False is True
E    +  where False = StepResult(name='aggregation_map', output=["{'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}"], success=False, attempts=1, latency_s=0.0008920839754864573, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_a9c5b648e430450baffe05ab07983f95', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': {'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}}, current_item='item1', processing_history=[]), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === aggregation_map
2025-08-04 22:45:25,250 - flujo - INFO - LoopStep 'aggregation_map': Starting Iteration 1/1
2025-08-04 22:45:25,250 - flujo - INFO - Counting string output as 1 token for step 'aggregation_map_step': '{'item': 'item1', 'running_avg_length': 5.0, 'tota...'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_a9c5b648e430450baffe05ab07983f95', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': {'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}}, 'current_item': 'item1', 'processing_history': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_a9c5b648e430450baffe05ab07983f95
[DEBUG] actual_source_value: run_a9c5b648e430450baffe05ab07983f95
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': {'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}}
[DEBUG] actual_source_value: {'item1': {'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_a9c5b648e430450baffe05ab07983f95', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'item1': {'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}}, 'current_item': 'item1', 'processing_history': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_a9c5b648e430450baffe05ab07983f95
[DEBUG] actual_source_value: run_a9c5b648e430450baffe05ab07983f95
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'item1': {'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}}
[DEBUG] actual_source_value: {'item1': {'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,251 - flujo - WARNING - Step 'aggregation_map' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'aggregation_map': Starting Iteration 1/1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'aggregation_map_step': '{'item': 'item1', 'running_avg_length': 5.0, 'tota...'
WARNING  flujo:telemetry.py:54 Step 'aggregation_map' failed. Halting pipeline execution.
____________ test_map_over_with_context_updates_metadata_conflicts _____________
[gw1] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_map_over_with_context_updates.py:380: in test_map_over_with_context_updates_metadata_conflicts
    assert result.step_history[-1].success is True
E   AssertionError: assert False is True
E    +  where False = StepResult(name='metadata_map', output=['metadata_processed_item1'], success=False, attempts=1, latency_s=0.0009404589654877782, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_ad3ecf61591540e19d2d4c7ec9ed5c9a', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'metadata_item1': {'map_index': 0, 'map_item': 'item1', 'map_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'item1'}}}, current_item='item1', processing_history=[]), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
----------------------------- Captured stdout call -----------------------------
[KIRO DEBUG] === HANDLE LOOP STEP === metadata_map
2025-08-04 22:45:25,256 - flujo - INFO - LoopStep 'metadata_map': Starting Iteration 1/1
2025-08-04 22:45:25,256 - flujo - INFO - Counting string output as 1 token for step 'metadata_map_step': 'metadata_processed_item1'
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_ad3ecf61591540e19d2d4c7ec9ed5c9a', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'metadata_item1': {'map_index': 0, 'map_item': 'item1', 'map_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'item1'}}}, 'current_item': 'item1', 'processing_history': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_ad3ecf61591540e19d2d4c7ec9ed5c9a
[DEBUG] actual_source_value: run_ad3ecf61591540e19d2d4c7ec9ed5c9a
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'metadata_item1': {'map_index': 0, 'map_item': 'item1', 'map_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'item1'}}}
[DEBUG] actual_source_value: {'metadata_item1': {'map_index': 0, 'map_item': 'item1', 'map_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'item1'}}}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] safe_merge_context_updates called
[DEBUG] target_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] source_context type: <class 'tests.integration.test_map_over_with_context_updates.MapContext'>
[DEBUG] excluded_fields: {'command_log'}
[DEBUG] source_fields: {'run_id': 'run_ad3ecf61591540e19d2d4c7ec9ed5c9a', 'initial_prompt': 'test', 'scratchpad': {'status': 'running'}, 'hitl_history': [], 'command_log': [], 'items': ['item1', 'item2', 'item3', 'item4'], 'processed_items': ['item1'], 'total_processed': 1, 'map_results': {'metadata_item1': {'map_index': 0, 'map_item': 'item1', 'map_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'item1'}}}, 'current_item': 'item1', 'processing_history': []}
[DEBUG] Processing field: run_id
[DEBUG] current_value: run_ad3ecf61591540e19d2d4c7ec9ed5c9a
[DEBUG] actual_source_value: run_ad3ecf61591540e19d2d4c7ec9ed5c9a
[DEBUG] Field unchanged: run_id
[DEBUG] Processing field: initial_prompt
[DEBUG] current_value: test
[DEBUG] actual_source_value: test
[DEBUG] Field unchanged: initial_prompt
[DEBUG] Processing field: scratchpad
[DEBUG] current_value: {'status': 'running'}
[DEBUG] actual_source_value: {'status': 'running'}
[DEBUG] Merging dictionaries for field: scratchpad
[DEBUG] Processing field: hitl_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: hitl_history
[DEBUG] No new items to add to list field: hitl_history
[DEBUG] Skipping excluded field: command_log
[DEBUG] Processing field: items
[DEBUG] current_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] actual_source_value: ['item1', 'item2', 'item3', 'item4']
[DEBUG] Merging lists for field: items
[DEBUG] No new items to add to list field: items
[DEBUG] Processing field: processed_items
[DEBUG] current_value: ['item1']
[DEBUG] actual_source_value: ['item1']
[DEBUG] Merging lists for field: processed_items
[DEBUG] No new items to add to list field: processed_items
[DEBUG] Processing field: total_processed
[DEBUG] current_value: 1
[DEBUG] actual_source_value: 1
[DEBUG] Field unchanged: total_processed
[DEBUG] Processing field: map_results
[DEBUG] current_value: {'metadata_item1': {'map_index': 0, 'map_item': 'item1', 'map_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'item1'}}}
[DEBUG] actual_source_value: {'metadata_item1': {'map_index': 0, 'map_item': 'item1', 'map_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'item1'}}}
[DEBUG] Merging dictionaries for field: map_results
[DEBUG] Processing field: current_item
[DEBUG] current_value: item1
[DEBUG] actual_source_value: item1
[DEBUG] Field unchanged: current_item
[DEBUG] Processing field: processing_history
[DEBUG] current_value: []
[DEBUG] actual_source_value: []
[DEBUG] Merging lists for field: processing_history
[DEBUG] No new items to add to list field: processing_history
[DEBUG] Total fields updated: 0
[DEBUG] Validation errors: []
[DEBUG] Iteration 1: Checking condition - limits: False, iteration_count: 1, max_iterations: 1
2025-08-04 22:45:25,257 - flujo - WARNING - Step 'metadata_map' failed. Halting pipeline execution.
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 LoopStep 'metadata_map': Starting Iteration 1/1
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'metadata_map_step': 'metadata_processed_item1'
WARNING  flujo:telemetry.py:54 Step 'metadata_map' failed. Halting pipeline execution.
=========================== short test summary info ============================
FAILED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_preservation - KeyError: 'existing_key'
FAILED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_multiple_branches_context_updates - AssertionError: assert 'support' in {'billing': 'billing:Need both billing and support'}
 +  where {'billing': 'billing:Need both billing and support'} = DynamicRouterContext(run_id='run_7389d41a989b4f06804b7cdeb0cd706b', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:Need both billing and support'}, execution_count=1, router_called=True, context_updates=['billing_processed']).branch_results
 +    where DynamicRouterContext(run_id='run_7389d41a989b4f06804b7cdeb0cd706b', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:Need both billing and support'}, execution_count=1, router_called=True, context_updates=['billing_processed']) = PipelineResult(step_history=[StepResult(name='dynamic_router', output={'billing': {'billing_result': 'billing:Need both billing and support'}}, success=True, attempts=1, latency_s=0.0006684580002911389, token_counts=0, cost_usd=0.0, feedback='All 1 branches executed successfully', branch_context=DynamicRouterContext(run_id='run_7389d41a989b4f06804b7cdeb0cd706b', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:Need both billing and support'}, execution_count=1, router_called=True, context_updates=['billing_processed']), metadata_={}, step_history=[])], total_cost_usd=0.0, total_tokens=0, final_pipeline_context=DynamicRouterContext(run_id='run_7389d41a989b4f06804b7cdeb0cd706b', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:Need both billing and support'}, execution_count=1, router_called=True, context_updates=['billing_processed']), trace_tree=Span(span_id='89148e75-490e-4f3b-a205-335ecff185ea', name='pipeline_root', start_time=286532.097636166, end_time=286532.099230125, parent_span_id=None, attributes={'initial_input': 'Need both billing and support'}, children=[Span(span_id='cdba873c-0e46-45e8-a3e4-9685a1268016', name='dynamic_router', start_time=286532.097698333, end_time=286532.098686833, parent_span_id='89148e75-490e-4f3b-a205-335ecff185ea', attributes={'step_type': 'DynamicParallelRouterStep[~ContextModelT]', 'step_input': 'Need both billing and support', 'success': True, 'attempts': 1, 'latency_s': 0.0006684580002911389, 'cost_usd': 0.0, 'token_counts': 0}, children=[], status='completed')], status='completed')).final_pipeline_context
FAILED tests/application/core/test_executor_core_hitl_step_migration.py::TestExecutorCoreHITLStepMigration::test_handle_hitl_step_context_isolation - KeyError: 'key1'
FAILED tests/integration/test_dynamic_parallel_router_with_context_updates.py::test_dynamic_router_branch_failure_context_preservation - AssertionError: assert 'support_failed' in ['router_called', 'billing_processed']
 +  where ['router_called', 'billing_processed'] = DynamicRouterContext(run_id='run_0c5afacfea6542d696920db4ba519008', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:test'}, execution_count=0, router_called=True, context_updates=['router_called', 'billing_processed']).context_updates
 +    where DynamicRouterContext(run_id='run_0c5afacfea6542d696920db4ba519008', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:test'}, execution_count=0, router_called=True, context_updates=['router_called', 'billing_processed']) = PipelineResult(step_history=[StepResult(name='dynamic_router', output={'billing': {'billing_result': 'billing:test'}}, success=True, attempts=1, latency_s=0.0012312500039115548, token_counts=0, cost_usd=0.0, feedback='All 1 branches executed successfully', branch_context=DynamicRouterContext(run_id='run_0c5afacfea6542d696920db4ba519008', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:test'}, execution_count=0, router_called=True, context_updates=['router_called', 'billing_processed']), metadata_={}, step_history=[])], total_cost_usd=0.0, total_tokens=0, final_pipeline_context=DynamicRouterContext(run_id='run_0c5afacfea6542d696920db4ba519008', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_selection=[], branch_results={'billing': 'billing:test'}, execution_count=0, router_called=True, context_updates=['router_called', 'billing_processed']), trace_tree=Span(span_id='adcf6105-50a1-4822-8c6b-b1a06018213e', name='pipeline_root', start_time=286532.110641958, end_time=286532.113114833, parent_span_id=None, attributes={'initial_input': 'test'}, children=[Span(span_id='cb64a7e8-5b76-46c5-80dd-10f11d16b351', name='dynamic_router', start_time=286532.110696333, end_time=286532.112554125, parent_span_id='adcf6105-50a1-4822-8c6b-b1a06018213e', attributes={'step_type': 'DynamicParallelRouterStep[~ContextModelT]', 'step_input': 'test', 'success': True, 'attempts': 1, 'latency_s': 0.0012312500039115548, 'cost_usd': 0.0, 'token_counts': 0}, children=[], status='completed')], status='completed')).final_pipeline_context
FAILED tests/integration/test_dynamic_router_bug_fix.py::test_dynamic_router_multiple_branches_context_fix - AssertionError: assert 'support' in {'billing': 'billing:Need both billing and support'}
 +  where {'billing': 'billing:Need both billing and support'} = DynamicRouterTestContext(run_id='run_01e31851fa2b4e6ca2309c9eec759c4c', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_called=True, branch_results={'billing': 'billing:Need both billing and support'}, context_updates=['router_executed', 'billing_processed']).branch_results
 +    where DynamicRouterTestContext(run_id='run_01e31851fa2b4e6ca2309c9eec759c4c', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_called=True, branch_results={'billing': 'billing:Need both billing and support'}, context_updates=['router_executed', 'billing_processed']) = PipelineResult(step_history=[StepResult(name='dynamic_router', output={'billing': {'billing_result': 'billing:Need both billing and support'}}, success=True, attempts=1, latency_s=0.0005972090293653309, token_counts=0, cost_usd=0.0, feedback='All 1 branches executed successfully', branch_context=DynamicRouterTestContext(run_id='run_01e31851fa2b4e6ca2309c9eec759c4c', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_called=True, branch_results={'billing': 'billing:Need both billing and support'}, context_updates=['router_executed', 'billing_processed']), metadata_={}, step_history=[])], total_cost_usd=0.0, total_tokens=0, final_pipeline_context=DynamicRouterTestContext(run_id='run_01e31851fa2b4e6ca2309c9eec759c4c', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], router_called=True, branch_results={'billing': 'billing:Need both billing and support'}, context_updates=['router_executed', 'billing_processed']), trace_tree=Span(span_id='030c1bc2-ff80-4231-b07c-2ebb8b372399', name='pipeline_root', start_time=286532.157646416, end_time=286532.159075625, parent_span_id=None, attributes={'initial_input': 'Need both billing and support'}, children=[Span(span_id='4a041c20-c41e-4f6b-a545-26618ca39fe9', name='dynamic_router', start_time=286532.157711166, end_time=286532.15858775, parent_span_id='030c1bc2-ff80-4231-b07c-2ebb8b372399', attributes={'step_type': 'DynamicParallelRouterStep[~ContextModelT]', 'step_input': 'Need both billing and support', 'success': True, 'attempts': 1, 'latency_s': 0.0005972090293653309, 'cost_usd': 0.0, 'token_counts': 0}, children=[], status='completed')], status='completed')).final_pipeline_context
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_successful_run_no_retries - AssertionError: Expected 'apply_prompt' to have been called once. Called 0 times.
FAILED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_error_handling - assert "branch 'failing_branch' failed" in "parallel step failed: branch 'failing_branch': agent execution failed with runtimeerror: intentional router branch failure"
 +  where "parallel step failed: branch 'failing_branch': agent execution failed with runtimeerror: intentional router branch failure" = <built-in method lower of str object at 0x112cf6c30>()
 +    where <built-in method lower of str object at 0x112cf6c30> = "Parallel step failed: Branch 'failing_branch': Agent execution failed with RuntimeError: Intentional router branch failure".lower
 +      where "Parallel step failed: Branch 'failing_branch': Agent execution failed with RuntimeError: Intentional router branch failure" = StepResult(name='error_router', output={'failing_branch': StepResult(name='error_router_failing_branch', output=None, success=False, attempts=1, latency_s=0.0003892080276273191, token_counts=0, cost_usd=0.0, feedback='Agent execution failed with RuntimeError: Intentional router branch failure', branch_context=RouterContext(run_id='run_2f102aaa3d234437b947a514a023cd95', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], router_state='executed_failing', branch_executed='failing_branch', branch_count=2, total_updates=0, router_metadata={}, branch_results={}), metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.000594375014770776, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'failing_branch': Agent execution failed with RuntimeError: Intentional router branch failure", branch_context=RouterContext(run_id='run_2f102aaa3d234437b947a514a023cd95', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], router_state='', branch_executed='', branch_count=0, total_updates=0, router_metadata={}, branch_results={}), metadata_={}, step_history=[]).feedback
FAILED tests/integration/test_embedding_cost_integration.py::TestEmbeddingCostIntegration::test_embedding_cost_tracking_strict_mode_failure - Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_validator_failure_triggers_retry - AssertionError: assert 1 == 4
 +  where 1 = StepResult(name='test_step', output='processed output', success=False, attempts=1, latency_s=0.00034433399559929967, token_counts=1, cost_usd=0.0, feedback='Validation failed: Validation failed', branch_context=None, metadata_={}, step_history=[]).attempts
FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_routes_loopstep_to_new_handler - TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_parameter_passing - TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_legacy_import_removed - TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_usage_limit_exceeded_error_propagates - Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_error_propagation - AssertionError: Regex pattern did not match.
 Regex: 'Test error'
 Input: "ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'"
FAILED tests/application/core/test_executor_core_loop_step_dispatch.py::TestExecutorCoreLoopStepDispatch::test_execute_complex_step_loopstep_telemetry_logging - TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
FAILED tests/integration/test_embedding_regression.py::TestEmbeddingRegression::test_strict_mode_behavior_unchanged - Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_missing_agent_error - Failed: DID NOT RAISE <class 'flujo.exceptions.MissingAgentError'>
FAILED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_max_iterations - AssertionError: assert 2 == 3
 +  where 2 = StepResult(name='test_loop', output=None, success=False, attempts=2, latency_s=0.0010872079874388874, token_counts=10, cost_usd=0.1, feedback='Loop terminated after reaching max_loops (2), but last iteration body failed: Loop body failed: Agent execution failed with IndexError: No more outputs available', branch_context=LoopTestContext(counter=0, messages=[], data={}, original_value=None), metadata_={'iterations': 2, 'exit_reason': 'max_iterations_with_body_failure'}, step_history=[]).attempts
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_mock_object_detection - Failed: DID NOT RAISE <class 'TypeError'>
FAILED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_token_limits - Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_validation_failure_with_feedback - AssertionError: assert 1 == 3
 +  where 1 = StepResult(name='test_step', output='processed output', success=False, attempts=1, latency_s=0.0004451249842531979, token_counts=1, cost_usd=0.0, feedback='Plugin failed: Plugin validation failed: Invalid format', branch_context=None, metadata_={}, step_history=[]).attempts
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_plugin_failure_propagates - AssertionError: assert 'Plugin validation failed: Plugin execution error' in 'Plugin failed: Plugin execution error'
 +  where 'Plugin failed: Plugin execution error' = StepResult(name='test_step', output='processed output', success=False, attempts=1, latency_s=0.0003417499829083681, token_counts=1, cost_usd=0.0, feedback='Plugin failed: Plugin execution error', branch_context=None, metadata_={}, step_history=[]).feedback
FAILED tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_component_interface_optimization - AssertionError: Serializer should be called
assert 0 > 0
 +  where 0 = <tests.integration.test_executor_core_architecture_validation.MockSerializer object at 0x11533af50>.serialize_calls
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_usage_tracking - AssertionError: Expected 'guard' to have been called once. Called 0 times.
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreSimpleStep::test_pricing_not_configured_error_propagates - Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_successful_fallback_execution - flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
FAILED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_streaming - AssertionError: assert <async_generator object TestExecutorCoreFallbackIntegration.test_real_fallback_with_streaming.<locals>.StreamingAgent.stream at 0x114afca00> == 'fallback stream'
 +  where <async_generator object TestExecutorCoreFallbackIntegration.test_real_fallback_with_streaming.<locals>.StreamingAgent.stream at 0x114afca00> = StepResult(name='stream_primary', output=<async_generator object TestExecutorCoreFallbackIntegration.test_real_fallback_with_streaming.<locals>.StreamingAgent.stream at 0x114afca00>, success=True, attempts=3, latency_s=0.0002477919915691018, token_counts=0, cost_usd=0.0, feedback='', branch_context=None, metadata_={'fallback_triggered': True, 'original_error': 'Agent execution failed with Exception: Primary failed'}, step_history=[]).output
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_metric_accounting - AssertionError: assert 24 == 38
 +  where 24 = StepResult(name='primary_step', output='fallback success', success=True, attempts=2, latency_s=0.10039637496229262, token_counts=24, cost_usd=0.2, feedback='', branch_context=None, metadata_={'fallback_triggered': True, 'original_error': 'Validation failed: Validation failed'}, step_history=[]).token_counts
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFallbackLogic::test_fallback_execution_exception_handling - Exception: Fallback execution failed
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_plugin_steps - AssertionError: Plugin step classification mismatch: old=True, new=False
assert True == False
FAILED tests/application/core/test_executor_core.py::TestExecutorCoreFunctionalEquivalence::test_functional_equivalence_comprehensive_coverage - AssertionError: Basic test failed for <Mock name='plugin_step.name' id='4462333584'> (Mock): old=True, new=False
assert True == False
FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_signature - AssertionError: assert ['step', 'dat..._setter', ...] == ['conditional...ntext_setter']
  
  At index 0 diff: 'step' != 'conditional_step'
  Left contains one more item: 'fallback_depth'
  
  Full diff:
    [
  -     'conditional_step',
  +     'step',
        'data',
        'context',
        'resources',
        'limits',
        'context_setter',
  +     'fallback_depth',
    ]
FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_parameter_passing - AssertionError: assert ExecutionFrame(step=<Mock spec='Step' id='4474855376'>, data='test_data', context=<Mock id='4436917968'>, resources=<Mock id='4474858768'>, limits=UsageLimits(total_cost_usd_limit=10.0, total_tokens_limit=None), stream=False, on_chunk=None, breach_event=None, context_setter=<Mock id='4474849104'>, result=None, _fallback_depth=0) == <Mock spec='Step' id='4474855376'>
FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_with_limits_and_context_setter - AssertionError: Expected 'mock' to have been called once. Called 0 times.
FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_integration_with_execute_complex_step - TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_routes_conditionalstep_to_new_handler - TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_parameter_passing - TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_no_legacy_import - TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_error_propagation - AssertionError: Regex pattern did not match.
 Regex: 'Test error'
 Input: "ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'"
FAILED tests/application/core/test_executor_core_conditional_step_dispatch.py::TestExecutorCoreConditionalStepDispatch::test_execute_complex_step_conditionalstep_telemetry_logging - TypeError: ExecutorCore._execute_complex_step() got an unexpected keyword argument 'context'
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_not_found_no_default - assert "No branch found for key 'nonexistent_branch'" in "No branch matches condition 'nonexistent_branch' and no default branch provided"
 +  where "No branch matches condition 'nonexistent_branch' and no default branch provided" = StepResult(name='test_conditional', output='test_data', success=False, attempts=1, latency_s=9.487499482929707e-05, token_counts=0, cost_usd=0.0, feedback="No branch matches condition 'nonexistent_branch' and no default branch provided", branch_context=None, metadata_={}, step_history=[]).feedback
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_output_mapper_exception - AssertionError: assert 'Branch output mapper raised an exception' in 'Error executing conditional logic or branch: Mapper failed'
 +  where 'Error executing conditional logic or branch: Mapper failed' = StepResult(name='test_conditional', output=None, success=False, attempts=1, latency_s=0.0002668750239536166, token_counts=0, cost_usd=0.0, feedback='Error executing conditional logic or branch: Mapper failed', branch_context=None, metadata_={'executed_branch_key': 'branch_a'}, step_history=[]).feedback
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_metrics_accumulation - assert 0.0005757910548709333 == 1.5
 +  where 0.0005757910548709333 = StepResult(name='test_conditional', output='test_output', success=True, attempts=1, latency_s=0.0005757910548709333, token_counts=100, cost_usd=0.01, feedback="Branch 'branch_a' executed successfully", branch_context=PipelineContext(run_id='run_c4053cedd0ab4695b0a75bcaf7daf933', initial_prompt='test_data', scratchpad={}, hitl_history=[], command_log=[]), metadata_={'executed_branch_key': 'branch_a'}, step_history=[]).latency_s
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_context_setter_called_on_success - AssertionError: Expected 'mock' to have been called once. Called 0 times.
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_with_resources - KeyError: 'resources'
FAILED tests/integration/test_executor_core_optimization_integration.py::TestErrorHandlingIntegration::test_error_recovery_integration - pydantic_core._pydantic_core.ValidationError: 1 validation error for StepResult
name
  Input should be a valid string [type=string_type, input_value=<Mock name='mock.step.name' id='4614630864'>, input_type=Mock]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_with_limits - KeyError: 'limits'
FAILED tests/integration/test_agentic_loop_recipe.py::test_pause_and_resume_in_loop - AssertionError: assert 'failed' == 'paused'
  
  - paused
  + failed
FAILED tests/integration/test_executor_core_optimization_integration.py::TestErrorHandlingIntegration::test_circuit_breaker_integration - pydantic_core._pydantic_core.ValidationError: 1 validation error for StepResult
name
  Input should be a valid string [type=string_type, input_value=<Mock name='mock.step.name' id='4652068304'>, input_type=Mock]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
FAILED tests/integration/test_agentic_loop_recipe.py::test_sync_resume - flujo.exceptions.OrchestratorError: Pipeline is not paused
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_none_feedback - flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_execution_exception_handling - Exception: Fallback execution failed
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_usage_limits - flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_metadata_preservation - flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
FAILED tests/integration/test_parallel_step.py::test_parallel_overwrite_multi_branch_order - KeyError: 'v'
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_critical_exceptions - flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_pricing_not_configured - flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_missing_agent_error - flujo.application.core.ultra_executor.MockDetectionError: Step 'primary_step' returned a Mock object. This is usually due to an unconfigured mock in a test.
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_usage_meter_tracking - AssertionError: assert 0 == 1
 +  where 0 = <AsyncMock name='mock.add' id='4473468240'>.call_count
 +    where <AsyncMock name='mock.add' id='4473468240'> = <AsyncMock id='4474084496'>.add
 +      where <AsyncMock id='4474084496'> = <flujo.application.core.ultra_executor.ExecutorCore object at 0x10aa9b850>._usage_meter
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_processor_pipeline - AssertionError: assert False is True
 +  where False = StepResult(name='primary_step', output=None, success=False, attempts=4, latency_s=0.00036854203790426254, token_counts=0, cost_usd=0.0, feedback='Original error: Agent execution failed with Exception: Primary failed; Fallback error: Agent execution failed with Exception: Primary failed', branch_context=None, metadata_={'fallback_triggered': True}, step_history=[]).success
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_plugin_runner - AssertionError: assert False is True
 +  where False = StepResult(name='primary_step', output=None, success=False, attempts=4, latency_s=0.0003055409761145711, token_counts=0, cost_usd=0.0, feedback='Original error: Agent execution failed with Exception: Primary failed; Fallback error: Agent execution failed with Exception: Primary failed', branch_context=None, metadata_={'fallback_triggered': True}, step_history=[]).success
FAILED tests/integration/test_as_step_state_persistence.py::test_as_step_state_persistence_and_resumption - assert 0 == 1
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_cache_backend - pydantic_core._pydantic_core.ValidationError: 1 validation error for StepResult
name
  Input should be a valid string [type=string_type, input_value=<Mock name='mock.fallback...p.name' id='4474187152'>, input_type=Mock]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
FAILED tests/integration/test_caching_and_fallbacks.py::test_caching_pipeline_speed_and_hits - AssertionError: assert ({'cache_hit': True} is None or 'cache_hit' not in {'cache_hit': True})
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_with_telemetry - AssertionError: assert False is True
 +  where False = StepResult(name='primary_step', output=None, success=False, attempts=4, latency_s=0.00025141704827547073, token_counts=0, cost_usd=0.0, feedback='Original error: Agent execution failed with Exception: Primary failed; Fallback error: Agent execution failed with Exception: Primary failed', branch_context=None, metadata_={'fallback_triggered': True}, step_history=[]).success
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_integration_real_pipeline - AssertionError: assert 'primary success' == 'fallback success'
  
  - fallback success
  + primary success
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_on_plugin_failure - AttributeError: 'StepResult' object has no attribute 'metadata'
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_on_validator_failure - AssertionError: assert 'processed: primary success' == 'fallback success'
  
  - fallback success
  + processed: primary success
FAILED tests/application/core/test_executor_core_fallback.py::TestExecutorCoreFallback::test_fallback_on_complex_failure_chain - AssertionError: assert 'Primary agent failed' in ''
 +  where '' = StepResult(name='primary_step', output='fallback success', success=True, attempts=2, latency_s=0.0002532079815864563, token_counts=1, cost_usd=0.0, feedback='', branch_context=None, metadata_={'fallback_triggered': True, 'original_error': 'Agent execution failed with Exception: Primary agent failed'}, step_history=[]).feedback
FAILED tests/integration/test_redirect_loop_unhashable.py::test_redirect_loop_detected_with_unhashable_agents - Failed: DID NOT RAISE <class 'flujo.exceptions.InfiniteRedirectError'>
FAILED tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_error_handling - AssertionError: assert 'loop exited by condition, but last iteration body failed' in 'loop terminated after reaching max_loops (5)'
 +  where 'loop terminated after reaching max_loops (5)' = <built-in method lower of str object at 0x10aaa60d0>()
 +    where <built-in method lower of str object at 0x10aaa60d0> = 'Loop terminated after reaching max_loops (5)'.lower
 +      where 'Loop terminated after reaching max_loops (5)' = StepResult(name='error_refine', output='refined_content_6', success=False, attempts=5, latency_s=0.007084375014528632, token_counts=10, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (5)', branch_context=RefineContext(run_id='run_e3e13301ae15417582ba3eaf2baf03cf', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], refinement_count=6, total_iterations=11, refinement_history=['refinement_1', 'refinement_2', 'refinement_3', 'refinement_4', 'refinement_5', 'refinement_6'], current_quality=0.16999999999999998, best_quality=0.16999999999999998, refinement_data={}), metadata_={'iterations': 5, 'exit_reason': 'max_iterations'}, step_history=[]).feedback
FAILED tests/integration/test_refine_until_with_context_updates.py::test_refine_until_with_context_updates_metadata_conflicts - AssertionError: assert 'reached max_loops' in 'loop terminated after reaching max_loops (3)'
 +  where 'loop terminated after reaching max_loops (3)' = <built-in method lower of str object at 0x10aa14c30>()
 +    where <built-in method lower of str object at 0x10aa14c30> = 'Loop terminated after reaching max_loops (3)'.lower
 +      where 'Loop terminated after reaching max_loops (3)' = StepResult(name='metadata_refine', output='metadata_content_3', success=False, attempts=3, latency_s=0.0046120419865474105, token_counts=6, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (3)', branch_context=RefineContext(run_id='run_cc9f76f7fa414399836d222c5dee8764', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], refinement_count=3, total_iterations=6, refinement_history=[], current_quality=0.11, best_quality=0.11, refinement_data={'metadata_1': {'refine_index': 1, 'refine_iteration': 1, 'refine_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'refinement_1'}}, 'metadata_2': {'refine_index': 2, 'refine_iteration': 3, 'refine_metadata': {'iteration': 2, 'timestamp': 'now', 'data': 'refinement_2'}}, 'metadata_3': {'refine_index': 3, 'refine_iteration': 5, 'refine_metadata': {'iteration': 3, 'timestamp': 'now', 'data': 'refinement_3'}}}), metadata_={'iterations': 3, 'exit_reason': 'max_iterations'}, step_history=[]).feedback
FAILED tests/integration/test_cost_tracking_bug_fixes.py::TestBug2FixParallelStepRaceCondition::test_fix_parallel_steps_with_atomic_usage_tracking - assert False
 +  where False = any([None, None, None, None, None])
FAILED tests/integration/test_parallel_step_enhancements.py::test_context_include_keys_optimization - AttributeError: 'LargeContext' object has no attribute 'initial_prompt'
FAILED tests/integration/test_cost_tracking_critical_bugs.py::TestBug2ParallelStepRaceCondition::test_parallel_steps_with_atomic_usage_tracking - assert False
 +  where False = any([None, None, None, None, None])
FAILED tests/integration/test_parallel_step_enhancements.py::test_context_include_keys_isolation - AttributeError: 'LargeContext' object has no attribute 'initial_prompt'
FAILED tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_on_failure_case - Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
FAILED tests/integration/test_cost_tracking_integration.py::TestStrictPricingModeIntegration::test_strict_mode_on_with_unknown_provider - Failed: DID NOT RAISE <class 'flujo.exceptions.PricingNotConfiguredError'>
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_filename_conflicts_handling - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_filename_conflicts_with_existing_files - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_rename_failure_fallback - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_crash_recovery.py::test_resume_after_crash_file_backend - assert 0 == 1
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_remove_failure_handling - AssertionError: Regex pattern did not match.
 Regex: 'Database corruption recovery failed'
 Input: 'file is not a database'
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_special_characters_in_filename - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_very_long_filename - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_crash_recovery.py::test_resume_after_crash_sqlite_backend - assert 0 == 1
FAILED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_configuration_management - AttributeError: 'dict' object has no attribute 'current_config'
FAILED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationIntegration::test_performance_recommendations - AssertionError: assert 'type' in 'Consider increasing cache size for better performance'
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_concurrent_backup_operations - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_parallel_step_enhancements.py::test_proactive_cancellation_with_multiple_branches - assert 0.5455325419898145 < 0.3
FAILED tests/unit/test_agentic_loop_logging.py::TestAgenticLoopLogging::test_pause_and_resume_logging - flujo.exceptions.OrchestratorError: Pipeline is not paused
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_infinite_loop_bug_fix - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_infinite_loop_bug_with_continue_fix - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_parallel_step_enhancements.py::test_proactive_cancellation_token_limits - AssertionError: Execution took too long: 0.549s (threshold: 0.400s). This indicates proactive cancellation may not be working correctly.
assert 0.5490237920312211 < 0.4
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_path_update_after_cleanup - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_parallel_step_enhancements.py::test_backward_compatibility_no_usage_limits - AttributeError: 'str' object has no attribute 'value'
FAILED tests/integration/test_parallel_step_enhancements.py::test_context_include_keys_with_nonexistent_fields - AttributeError: 'LargeContext' object has no attribute 'initial_prompt'
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_race_condition_in_backup_creation - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_readonly_directory_fallback - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_pattern_glob_handling - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_stat_error_handling - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_breach_detection - AssertionError: At least one operation should detect the breach
assert False
 +  where False = any([None, None, None, None, None, None, ...])
FAILED tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_token_limits - AssertionError: At least one operation should detect the token breach
assert False
 +  where False = any([None, None, None, None, None, None, ...])
FAILED tests/integration/test_parallel_usage_governor_stress.py::TestParallelUsageGovernorStress::test_stress_parallel_usage_governor_no_limits - AttributeError: 'NoneType' object has no attribute 'total_cost_usd_limit'
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_unlink_error_handling - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_min_function_with_none_values - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_empty_directory_handling - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_max_attempts_exceeded_handling - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_continue_statement_effectiveness - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_persistence_backends.py::test_file_backend_resume_after_crash - AssertionError: assert 2 == 3
 +  where 2 = WorkflowState(run_id='run_file', pipeline_id='unknown', pipeline_name='unknown', pipeline_version='latest', current_step_index=2, pipeline_context={'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_file', 'scratchpad': {'status': 'completed'}}, last_step_output='mid done', step_history=[{'attempts': 1, 'branch_context': {'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_file', 'scratchpad': {'status': 'completed'}}, 'cost_usd': 0.0, 'feedback': None, 'latency_s': 0.0003927089856006205, 'metadata_': {}, 'name': 's1', 'output': 'mid', 'step_history': [], 'success': True, 'token_counts': 1}, {'attempts': 1, 'branch_context': {'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_file', 'scratchpad': {'status': 'completed'}}, 'cost_usd': 0.0, 'feedback': None, 'latency_s': 0.0001551249879412353, 'metadata_': {}, 'name': 's2', 'output': 'mid done', 'step_history': [], 'success': True, 'token_counts': 1}], status='completed', created_at=datetime.datetime(2025, 8, 4, 22, 45, 5, 67415), updated_at=datetime.datetime(2025, 8, 4, 22, 45, 5, 76673)).current_step_index
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_path_reset_after_cleanup - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_counter_reset_after_cleanup - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_persistence_backends.py::test_sqlite_backend_resume_after_crash - AssertionError: assert 2 == 3
 +  where 2 = WorkflowState(run_id='run_sqlite', pipeline_id='unknown', pipeline_name='unknown', pipeline_version='latest', current_step_index=2, pipeline_context={'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_sqlite', 'scratchpad': {'status': 'completed'}}, last_step_output='mid done', step_history=[{'attempts': 1, 'branch_context': {'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_sqlite', 'scratchpad': {'status': 'completed'}}, 'cost_usd': 0.0, 'feedback': None, 'latency_s': 0.0003035829868167639, 'metadata_': {}, 'name': 's1', 'output': 'mid', 'step_history': [], 'success': True, 'token_counts': 1}, {'attempts': 1, 'branch_context': {'command_log': [], 'hitl_history': [], 'initial_prompt': 'x', 'run_id': 'run_sqlite', 'scratchpad': {'status': 'completed'}}, 'cost_usd': 0.0, 'feedback': None, 'latency_s': 0.00019966700347140431, 'metadata_': {}, 'name': 's2', 'output': 'mid done', 'step_history': [], 'success': True, 'token_counts': 1}], status='completed', created_at=datetime.datetime(2025, 8, 4, 22, 45, 5, 544094), updated_at=datetime.datetime(2025, 8, 4, 22, 45, 5, 558920)).current_step_index
FAILED tests/unit/test_bug_regression.py::TestCircularReferenceSerialization::test_self_reference - AssertionError: assert {'children': [], 'name': 'self', 'parent': None} is None
FAILED tests/integration/test_pipeline_context_updates.py::test_update_list_of_nested_models - AssertionError: assert 1 == [1, 2]
 +  where 1 = StepResult(name='reader', output=1, success=True, attempts=1, latency_s=6.312504410743713e-05, token_counts=0, cost_usd=0.0, feedback=None, branch_context=ContextWithNesting(counter=0, nested_item=None, list_of_items=[NestedModel(value=1, name='nested'), NestedModel(value=2, name='nested')]), metadata_={}, step_history=[]).output
FAILED tests/unit/test_bug_regression.py::TestFailedStepRecording::test_failed_step_recording - sqlite3.OperationalError: no such column: start_time
FAILED tests/integration/test_pipeline_runner.py::test_runner_respects_max_retries - assert 1 == 3
 +  where 1 = <flujo.testing.utils.StubAgent object at 0x11551ff90>.call_count
FAILED tests/integration/test_pipeline_runner.py::test_feedback_enriches_prompt - assert 1 == 2
 +  where 1 = <flujo.testing.utils.StubAgent object at 0x115539650>.call_count
FAILED tests/unit/test_bug_regression.py::TestLambdaSerializationNullHandling::test_lambda_null_handling - sqlite3.OperationalError: no such column: start_time
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_infinite_loop_prevention - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_pipeline_runner.py::test_timeout_and_redirect_loop_detection - Failed: DID NOT RAISE <class 'flujo.exceptions.InfiniteRedirectError'>
FAILED tests/integration/test_pipeline_runner_with_resources.py::test_resources_passed_to_plugin - assert False
 +  where False = StepResult(name='plugin_step', output='queried_products', success=False, attempts=1, latency_s=0.0005497080273926258, token_counts=1, cost_usd=0.0, feedback="Plugin failed: Plugin ResourceUsingPlugin failed: ResourceUsingPlugin.validate() missing 1 required keyword-only argument: 'resources'", branch_context=None, metadata_={}, step_history=[]).success
FAILED tests/integration/test_processors.py::test_prompt_processor_modifies_input - AssertionError: assert 'hello' == 'hello world'
  
  - hello world
  + hello
FAILED tests/integration/test_processors.py::test_processor_receives_context - AssertionError: assert False
 +  where False = <built-in method startswith of str object at 0x1144c81b0>('X:')
 +    where <built-in method startswith of str object at 0x1144c81b0> = 'hello'.startswith
FAILED tests/unit/test_ci_compatibility.py::TestCIErrorRecovery::test_partial_failure_handling - AssertionError: assert False
 +  where False = isinstance('<unserializable: dict>', dict)
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_cleanup_attempts_limit - sqlite3.DatabaseError: file is not a database
FAILED tests/unit/test_default_backend.py::test_runner_uses_sqlite_by_default - assert False
 +  where False = isinstance(<tests.conftest.NoOpStateBackend object at 0x10c30f950>, SQLiteBackend)
 +    where <tests.conftest.NoOpStateBackend object at 0x10c30f950> = <flujo.application.runner.Flujo object at 0x10c3c8f10>.state_backend
FAILED tests/unit/test_execution_manager.py::TestUsageGovernor::test_check_usage_limits_cost_exceeded - AssertionError: Regex pattern did not match.
 Regex: 'Cost limit of \\$10.0 exceeded'
 Input: 'Cost limit of $10 exceeded'
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_stat_exception_handling - sqlite3.DatabaseError: file is not a database
FAILED tests/unit/test_executor_components.py::TestDefaultPluginRunner::test_plugin_execution - AssertionError: assert PluginOutcome(success=True, feedback=None, redirect_to=None, new_solution='processed data') == 'processed data'
FAILED tests/unit/test_executor_components.py::TestDefaultPluginRunner::test_plugin_with_priority_ordering - AssertionError: assert PluginOutcome(success=True, feedback=None, redirect_to=None, new_solution='processed by plugin2') == 'processed by plugin1'
FAILED tests/unit/test_executor_core_parallel_migration.py::TestExecutorCoreParallelMigration::test_executor_core_dynamic_router_delegates_to_parallel - TypeError: ExecutorCore._handle_dynamic_router_step() got an unexpected keyword argument 'router_step'
FAILED tests/unit/test_fallback.py::test_successful_fallback_correctly_sets_metrics - AssertionError: assert '' is None
 +  where '' = StepResult(name='p', output='success', success=True, attempts=2, latency_s=0.00023125007282942533, token_counts=6, cost_usd=0.2, feedback='', branch_context=PipelineContext(run_id='run_c16de9d4d413457dbf0e7e5dba08b554', initial_prompt='in', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'fallback_triggered': True, 'original_error': 'Plugin failed: primary failed'}, step_history=[]).feedback
FAILED tests/unit/test_fallback_edge_cases.py::test_fallback_with_high_cost_agents - AssertionError: assert 1999.98 == 999.99
 +  where 1999.98 = StepResult(name='p', output='fallback', success=True, attempts=2, latency_s=0.00022508297115564346, token_counts=1999998, cost_usd=1999.98, feedback='', branch_context=PipelineContext(run_id='run_f432b6128ad04b4ca8508059718e85f3', initial_prompt='in', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'fallback_triggered': True, 'original_error': 'Plugin failed: primary failed'}, step_history=[]).cost_usd
FAILED tests/unit/test_fallback_edge_cases.py::test_fallback_with_negative_metrics - AssertionError: assert -0.2 == -0.1
 +  where -0.2 = StepResult(name='p', output='negative', success=True, attempts=2, latency_s=0.00021824997384101152, token_counts=-10, cost_usd=-0.2, feedback='', branch_context=PipelineContext(run_id='run_03ee61b834244d07acfdfcc168389ad7', initial_prompt='in', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'fallback_triggered': True, 'original_error': 'Plugin failed: primary failed'}, step_history=[]).cost_usd
FAILED tests/unit/test_fallback_edge_cases.py::test_fallback_with_none_feedback - AssertionError: assert 'Agent execution failed' in 'Original error: Plugin failed: None; Fallback error: Plugin failed: None'
 +  where 'Original error: Plugin failed: None; Fallback error: Plugin failed: None' = StepResult(name='p', output='oops', success=False, attempts=2, latency_s=0.0002603329485282302, token_counts=6, cost_usd=0.2, feedback='Original error: Plugin failed: None; Fallback error: Plugin failed: None', branch_context=None, metadata_={'fallback_triggered': True}, step_history=[]).feedback
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_unlink_exception_handling - sqlite3.DatabaseError: file is not a database
FAILED tests/unit/test_fallback_edge_cases.py::test_fallback_with_complex_metadata - AssertionError: assert 'Plugin faile...omplex failed' == 'complex failed'
  
  - complex failed
  + Plugin failed: complex failed
FAILED tests/unit/test_file_sqlite_backends.py::test_sqlite_backend_migrates_existing_db - sqlite3.OperationalError: table workflow_state has no column named pipeline_name
FAILED tests/unit/test_file_sqlite_backends.py::test_backends_serialize_pydantic - TypeError: Type is not JSON serializable: MyModel
FAILED tests/unit/test_file_sqlite_backends.py::test_backends_deserialize_special_types - AssertionError: assert None == 'inf'
FAILED tests/unit/test_lens_cli.py::test_lens_commands - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_lens_cli.py::test_lens_commands_with_filters - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_lens_cli.py::test_lens_show_detailed_run - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_lens_cli.py::test_lens_commands_with_empty_database - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_lens_cli.py::test_lens_commands_with_failed_run - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_lens_cli.py::test_lens_commands_with_environment_configuration - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_lens_cli.py::test_lens_cli_relative_path_resolution - AssertionError: Error accessing backend: no such column: start_time
  
assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_lens_cli.py::test_lens_show_with_verbose_options - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_glob_exception_handling - sqlite3.DatabaseError: file is not a database
FAILED tests/unit/test_monitor_integration.py::TestInMemoryMonitorUsage::test_monitor_records_failed_calls - AssertionError: assert 'test_input\n... Test failure' == 'test_input'
  
  - test_input
  + test_input
  ?           +
  + Agent execution failed on attempt 1: Test failure
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_basic_parallel_execution_no_merge - assert False
 +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c4678d0>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c4678d0>, metadata_={}, step_history=[]), 'branch2': StepResult(name='test_parallel_branch2', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c467150>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c467150>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0008629999938420951, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c4678d0>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type; Branch 'branch2': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c467150>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=None, metadata_={}, step_history=[]).success
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_with_context_include_keys - AttributeError: MockContext has no attribute 'initial_prompt'
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_cost_limit_breach - Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_token_limit_breach - Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_overwrite - assert False
 +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c1ebb90>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c1ebb90>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0005666249780915678, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c1ebb90>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c2b9250>, metadata_={}, step_history=[]).success
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_scratchpad - assert False
 +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c152710>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c152710>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0005230410024523735, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c152710>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c1eaa10>, metadata_={}, step_history=[]).success
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_fallback_timestamp_naming - sqlite3.DatabaseError: file is not a database
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_scratchpad_no_scratchpad - assert False
 +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c264a90>, input_type=TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext object at 0x10c264a90>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.000687791034579277, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c264a90>, input_type=TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_scratchpad_no_scratchpad.<locals>.NoScratchpadContext object at 0x10c266990>, metadata_={}, step_history=[]).success
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_custom_merge_strategy - assert False
 +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback='Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c29a110>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type', branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c29a110>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0005427919677458704, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 1 validation error for PipelineResult\nfinal_pipeline_context\n  Input should be a valid dictionary or instance of BaseModel [type=model_type, input_value=<tests.unit.test_parallel...t object at 0x10c29a110>, input_type=MockContext]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type", branch_context=<tests.unit.test_parallel_step_strategies.MockContext object at 0x10c264450>, metadata_={}, step_history=[]).success
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_task_cancellation - Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
FAILED tests/unit/test_parallel_step_strategies.py::TestParallelStepExecution::test_parallel_execution_merge_context_update - assert False
 +  where False = StepResult(name='test_parallel', output={'branch1': StepResult(name='test_parallel_branch1', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback="Branch execution failed: 'Pipeline' object has no attribute 'name'", branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_context_update.<locals>.TestContext object at 0x10c1c61d0>, metadata_={}, step_history=[]), 'branch2': StepResult(name='test_parallel_branch2', output=None, success=False, attempts=1, latency_s=0.0, token_counts=0, cost_usd=0.0, feedback="Branch execution failed: 'Pipeline' object has no attribute 'name'", branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_context_update.<locals>.TestContext object at 0x10c13a650>, metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.00023750000400468707, token_counts=0, cost_usd=0.0, feedback="Parallel step failed: Branch 'branch1': Branch execution failed: 'Pipeline' object has no attribute 'name'; Branch 'branch2': Branch execution failed: 'Pipeline' object has no attribute 'name'", branch_context=<tests.unit.test_parallel_step_strategies.TestParallelStepExecution.test_parallel_execution_merge_context_update.<locals>.TestContext object at 0x10bfbaa90>, metadata_={}, step_history=[]).success
FAILED tests/unit/test_parallel_step_strategies.py::test_parallel_usage_limit_enforced_atomically - Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
FAILED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_frequency_optimization - sqlite3.OperationalError: no such column: start_time
FAILED tests/unit/test_persistence_edge_cases.py::TestPersistenceOptimizationEdgeCases::test_persistence_on_step_failure - sqlite3.OperationalError: no such column: start_time
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_all_slots_undeletable_fallback - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_stat_always_raises - sqlite3.DatabaseError: file is not a database
FAILED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_default_backend_performance_overhead - AssertionError: Default persistence overhead (722.93%) exceeds 35.0% limit
assert 722.9315621198879 <= 35.0
FAILED tests/unit/test_persistence_performance.py::TestPersistencePerformanceOverhead::test_persistence_overhead_with_large_context - AssertionError: Persistence overhead with large context (585.07%) exceeds 35.0%
assert 585.0727111312593 <= 35.0
FAILED tests/unit/test_pipeline_context.py::test_plugin_receives_context_and_strict_plugin_errors - assert None == {}
 +  where None = <tests.unit.test_pipeline_context.KwargsPlugin object at 0x10c315110>.kwargs
FAILED tests/unit/test_prompt_formatter.py::test_prompt_robust_serialization - AssertionError: assert '<unserializable: Unknown>' in 'Value: <unserializable: Wrapper>'
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_glob_always_raises - sqlite3.DatabaseError: file is not a database
FAILED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_corrupted_database - sqlite3.DatabaseError: file is not a database
FAILED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_schema_version_changes - sqlite3.OperationalError: table runs has no column named execution_time_ms
FAILED tests/unit/test_schema_migration_robustness.py::TestSchemaMigrationRobustness::test_migration_handles_foreign_key_constraints - sqlite3.OperationalError: no such column: start_time
FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_lens_list_with_large_mixed_database - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_custom_types_edge_cases - Failed: DID NOT RAISE <class 'TypeError'>
FAILED tests/unit/test_serialization_edge_cases.py::TestSerializationEdgeCases::test_collections_edge_cases - TypeError: Object of type OrderedDict is not serializable. Register a custom serializer using register_custom_serializer(OrderedDict, lambda obj: obj.__dict__) or provide a default_serializer.
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_unlink_always_raises - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_sqlite_backend_edge_cases.py::TestSQLiteBackendEdgeCases::test_backup_permission_and_race_conditions - sqlite3.DatabaseError: file is not a database
FAILED tests/integration/test_stateful_hitl.py::test_stateful_hitl_resume - KeyError: 'pre'
FAILED tests/integration/test_streaming_pipeline.py::test_basic_streaming - AssertionError: assert '' == 'hi'
  
  - hi
FAILED tests/integration/test_streaming_pipeline.py::test_context_and_resources_in_stream - AssertionError: assert 0 == 1
 +  where 0 = Ctx(count=0).count
 +    where Ctx(count=0) = PipelineResult(step_history=[StepResult(name='s', output=<async_generator object CtxStreamAgent.stream at 0x10aae5a80>, success=True, attempts=1, latency_s=0.00010258401744067669, token_counts=0, cost_usd=0.0, feedback=None, branch_context=Ctx(count=0), metadata_={}, step_history=[])], total_cost_usd=0.0, total_tokens=0, final_pipeline_context=Ctx(count=0), trace_tree=Span(span_id='7f058838-99bb-4f54-b335-da14edde44c4', name='pipeline_root', start_time=286542.660477625, end_time=286542.660805583, parent_span_id=None, attributes={'initial_input': '3'}, children=[Span(span_id='7e57c0aa-7be8-41fa-93d4-fe3ef975b8c3', name='s', start_time=286542.660532541, end_time=286542.660732541, parent_span_id='7f058838-99bb-4f54-b335-da14edde44c4', attributes={'step_type': 'Step', 'step_input': '3', 'success': True, 'attempts': 1, 'latency_s': 0.00010258401744067669, 'cost_usd': 0.0, 'token_counts': 0}, children=[], status='completed')], status='completed')).final_pipeline_context
FAILED tests/integration/test_streaming_pipeline.py::test_pipeline_handles_streaming_agent_failure_gracefully - AssertionError: assert [] == ['H', 'e', 'l']
  
  Right contains 3 more items, first extra item: 'H'
  
  Full diff:
  + []
  - [
  -     'H',
  -     'e',
  -     'l',
  - ]
FAILED tests/integration/test_strict_validation.py::test_non_strict_validation_pass_through - AssertionError: assert False is True
 +  where False = StepResult(name='validate', output='ok', success=False, attempts=1, latency_s=0.00015708297723904252, token_counts=1, cost_usd=0.0, feedback='Validation failed: Validation failed: bad', branch_context=None, metadata_={}, step_history=[]).success
FAILED tests/integration/test_strict_validation.py::test_strict_validation_drops_output - AssertionError: assert 'bad' is None
 +  where 'bad' = StepResult(name='validate', output='bad', success=False, attempts=1, latency_s=0.0001559159718453884, token_counts=1, cost_usd=0.0, feedback='Validation failed: Validation failed: bad', branch_context=None, metadata_={}, step_history=[]).output
FAILED tests/integration/test_unified_error_handling.py::test_streaming_step_returns_stepresult_on_failure - AssertionError: assert not True
 +  where True = StepResult(name='failing', output=<async_generator object FailingAgent.stream at 0x10c121380>, success=True, attempts=1, latency_s=7.270800415426493e-05, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).success
FAILED tests/integration/test_unified_error_handling.py::test_critical_exceptions_are_re_raised - Failed: DID NOT RAISE <class 'flujo.exceptions.InfiniteFallbackError'>
FAILED tests/integration/test_unified_error_handling.py::test_consistent_api_contract - AssertionError: assert not True
 +  where True = StepResult(name='failing', output=<async_generator object FailingAgent.stream at 0x10c122880>, success=True, attempts=1, latency_s=5.162501474842429e-05, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).success
FAILED tests/integration/test_usage_governor.py::test_governor_with_loop_step - Failed: DID NOT RAISE <class 'flujo.exceptions.UsageLimitExceededError'>
FAILED tests/integration/test_usage_governor.py::test_governor_halts_loop_step_mid_iteration - AssertionError: assert not True
 +  where True = StepResult(name='breach_loop', output=3, success=True, attempts=2, latency_s=0.0022381669841706753, token_counts=200, cost_usd=0.2, feedback='', branch_context=PipelineContext(run_id='run_8c2d3d5f032245efb77072ac83f742db', initial_prompt='0', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'iterations': 2, 'exit_reason': 'usage_limit_exceeded'}, step_history=[]).success
FAILED tests/integration/test_usage_governor.py::test_governor_loop_with_nested_parallel_limit - AssertionError: assert not True
 +  where True = StepResult(name='outer_loop', output={'a': 1, 'b': 1}, success=True, attempts=2, latency_s=0.0033984999754466116, token_counts=400, cost_usd=0.4, feedback='', branch_context=PipelineContext(run_id='run_3df897314f0d49c696206775fcf74c67', initial_prompt='0', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[]), metadata_={'iterations': 2, 'exit_reason': 'usage_limit_exceeded'}, step_history=[]).success
FAILED tests/integration/test_validation_persistence.py::test_persist_feedback_and_results - assert ([])
 +  where [] = Ctx(feedback_history=[], validation_history=[]).feedback_history
FAILED tests/integration/test_validation_persistence.py::test_persist_results_on_success - assert 0 == 1
 +  where 0 = len([])
 +    where [] = Ctx(feedback_history=[], validation_history=[]).validation_history
FAILED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_metrics_accumulation_regression - assert 0.00018525001360103488 == 1.0
 +  where 0.00018525001360103488 = StepResult(name='test_conditional', output='test_output', success=True, attempts=1, latency_s=0.00018525001360103488, token_counts=100, cost_usd=0.01, feedback="Branch 'branch_a' executed successfully", branch_context=PipelineContext(run_id='run_be1a040b650b49d88cf51e1555d47050', initial_prompt='test_data', scratchpad={}, hitl_history=[], command_log=[]), metadata_={'executed_branch_key': 'branch_a'}, step_history=[]).latency_s
FAILED tests/regression/test_conditional_step_regression.py::TestConditionalStepRegression::test_context_handling_regression - assert False is True
FAILED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationBackwardCompatibility::test_configuration_serialization - AttributeError: type object 'OptimizationConfig' has no attribute 'from_dict'
FAILED tests/regression/test_executor_core_optimization_regression.py::TestOptimizationErrorHandling::test_invalid_step_handling - pydantic_core._pydantic_core.ValidationError: 1 validation error for StepResult
name
  Input should be a valid string [type=string_type, input_value=<Mock name='mock.step.name' id='4498202256'>, input_type=Mock]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
FAILED tests/unit/test_serialization_properties.py::TestSerializationProperties::test_unsupported_types_raise - Failed: DID NOT RAISE <class 'TypeError'>
FAILED tests/unit/test_sqlite_backend_robustness.py::TestSQLiteBackendRobustness::test_schema_migration_handles_corrupted_database - sqlite3.DatabaseError: file is not a database
FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_special_characters_in_filename - assert 0 == 1
 +  where 0 = len([])
FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_long_filename - assert 0 == 1
 +  where 0 = len([])
FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_no_write_permissions - AssertionError: assert not True
 +  where True = exists()
 +    where exists = PosixPath('/private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw0/test_backup_with_no_write_perm0/test.db').exists
FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_lens_list_with_various_filters - assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendBackupEdgeCases::test_backup_with_disk_full_error - AssertionError: assert not True
 +  where True = exists()
 +    where exists = PosixPath('/private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1175/popen-gw0/test_backup_with_disk_full_err0/test.db').exists
FAILED tests/unit/test_sqlite_edge_cases.py::TestSQLiteBackendConcurrencyEdgeCases::test_concurrent_backup_operations - assert 0 >= 1
 +  where 0 = len([])
FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_text_streaming_agent_protocol - AssertionError: assert <async_generator object StubTextStreamingAgent.stream at 0x11e1cb060> == 'hello world!'
 +  where <async_generator object StubTextStreamingAgent.stream at 0x11e1cb060> = StepResult(name='test_step', output=<async_generator object StubTextStreamingAgent.stream at 0x11e1cb060>, success=True, attempts=1, latency_s=0.00013991701416671276, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_binary_streaming_agent_protocol - AssertionError: assert <async_generator object StubBinaryStreamingAgent.stream at 0x10cb90ac0> == b'data1data2data3'
 +  where <async_generator object StubBinaryStreamingAgent.stream at 0x10cb90ac0> = StepResult(name='test_step', output=<async_generator object StubBinaryStreamingAgent.stream at 0x10cb90ac0>, success=True, attempts=1, latency_s=0.00011037499643862247, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_legacy_streaming_agent_fallback - AssertionError: assert <async_generator object StubLegacyStreamingAgent.stream at 0x10caba180> == 'hello world!'
 +  where <async_generator object StubLegacyStreamingAgent.stream at 0x10caba180> = StepResult(name='test_step', output=<async_generator object StubLegacyStreamingAgent.stream at 0x10caba180>, success=True, attempts=1, latency_s=0.00010820897296071053, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_legacy_binary_streaming_agent_fallback - AssertionError: assert <async_generator object StubLegacyStreamingAgent.stream at 0x10cc69460> == b'data1data2data3'
 +  where <async_generator object StubLegacyStreamingAgent.stream at 0x10cc69460> = StepResult(name='test_step', output=<async_generator object StubLegacyStreamingAgent.stream at 0x10cc69460>, success=True, attempts=1, latency_s=0.00012870796490460634, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_mixed_stream_types_handled_gracefully - assert <async_generator object StubLegacyStreamingAgent.stream at 0x10cc68e40> == "['text', b'binary', 'more_text']"
 +  where <async_generator object StubLegacyStreamingAgent.stream at 0x10cc68e40> = StepResult(name='test_step', output=<async_generator object StubLegacyStreamingAgent.stream at 0x10cc68e40>, success=True, attempts=1, latency_s=0.00011845800327137113, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_empty_stream_handled_correctly - AssertionError: assert <async_generator object StubTextStreamingAgent.stream at 0x10caa19a0> == ''
 +  where <async_generator object StubTextStreamingAgent.stream at 0x10caa19a0> = StepResult(name='test_step', output=<async_generator object StubTextStreamingAgent.stream at 0x10caa19a0>, success=True, attempts=1, latency_s=0.00011470797471702099, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_single_bytes_chunk - AssertionError: assert <async_generator object StubBinaryStreamingAgent.stream at 0x10d85adc0> == b'single_chunk'
 +  where <async_generator object StubBinaryStreamingAgent.stream at 0x10d85adc0> = StepResult(name='test_step', output=<async_generator object StubBinaryStreamingAgent.stream at 0x10d85adc0>, success=True, attempts=1, latency_s=0.0001040829811245203, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
FAILED tests/unit/test_streaming_bytes_bug.py::TestStreamingBytesBug::test_large_bytes_stream - AssertionError: assert <async_generator object StubBinaryStreamingAgent.stream at 0x10cb90f20> == b'chunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkc...unkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunkchunk'
 +  where <async_generator object StubBinaryStreamingAgent.stream at 0x10cb90f20> = StepResult(name='test_step', output=<async_generator object StubBinaryStreamingAgent.stream at 0x10cb90f20>, success=True, attempts=1, latency_s=0.00033425004221498966, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).output
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_streaming_execution - AssertionError: assert [] == ['chunk1', 'chunk2', 'chunk3']
  
  Right contains 3 more items, first extra item: 'chunk1'
  
  Full diff:
  + []
  - [
  -     'chunk1',
  -     'chunk2',
  -     'chunk3',
  - ]
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_cache_key_generation - AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_agent_identification_stability - AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_stability - AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_step_with_plugins_validators_fallbacks - assert False is True
 +  where False = StepResult(name='step_with_plugins', output='test_output', success=False, attempts=1, latency_s=0.0009979999740608037, token_counts=1, cost_usd=0.0, feedback="Plugin failed: unsupported operand type(s) for +=: 'float' and 'Mock'", branch_context=None, metadata_={}, step_history=[]).success
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_with_resources - AssertionError: assert 'cache_hit' not in (({'cache_hit': True}))
 +  where {'cache_hit': True} = StepResult(name='test_step', output='test output 1', success=True, attempts=1, latency_s=0.0001176249934360385, token_counts=1, cost_usd=0.0, feedback=None, branch_context=None, metadata_={'cache_hit': True}, step_history=[]).metadata_
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_key_includes_all_components - AssertionError: assert '0322bc00cdaa9a3baf7b8908cdefa0610676e2a0f1713a70e95a0a2bd266937f' != '0322bc00cdaa9a3baf7b8908cdefa0610676e2a0f1713a70e95a0a2bd266937f'
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_agent_identification_includes_module - AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_cache_performance_with_module_dataclasses - assert 0 > 0
 +  where 0 = len(OrderedDict())
 +    where OrderedDict() = _LRUCache(max_size=1000, ttl=3600, _store=OrderedDict())._store
 +      where _LRUCache(max_size=1000, ttl=3600, _store=OrderedDict()) = <flujo.application.core.ultra_executor.ExecutorCore object at 0x10c4151d0>.cache
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_consistent_agent_config_hashing - AssertionError: assert 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65' != 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_unified_error_handling_contract - AssertionError: assert not True
 +  where True = StepResult(name='streaming', output=<async_generator object TestUltraStepExecutor.test_unified_error_handling_contract.<locals>.FailingAgent.stream at 0x10ad9ab20>, success=True, attempts=1, latency_s=6.870803190395236e-05, token_counts=0, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).success
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_agent_identification_handles_edge_cases - AssertionError: Different agent types should generate different keys
assert 1 == 4
 +  where 1 = len({'d0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'})
 +    where {'d0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'} = set(['d0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'])
 +  and   4 = len(['d0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65', 'd0ed1dc5cf0bab45f698fbf51e0ee7659e90a88815719f5eeff338a2e523ef65'])
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_input_validation_ultra_executor - Failed: DID NOT RAISE <class 'ValueError'>
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_independent_latency_measurement - AssertionError: start_time should be captured inside the retry loop
assert False
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_regression_constructor_validation_preserved - AssertionError: UltraStepExecutor should validate cache_ttl
assert 'if cache_ttl < 0:' in '"""\nUltra-optimized step executor v2 with modular, policy-driven architecture.\n\nThis is a complete rewrite of the UltraStepExecutor with:\n- Modular design with clear separation of concerns\n- Deterministic behavior across processes and restarts\n- Pluggable components via dependency injection\n- Robust isolation between concerns\n- Exhaustive accounting of successful and failed attempts\n- Backward compatibility with existing SDK signatures\n\nAuthor: Flujo Team\nVersion: 2.0\n"""\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextvars\nimport time\nimport hashlib\nimport copy\nimport multiprocessing\nfrom abc import abstractmethod\nfrom collections import OrderedDict, defaultdict\nfrom dataclasses import dataclass, field\nfrom functools import cached_property, wraps\nfrom multiprocessing import cpu_count\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Generic,\n    Optional,\n    Protocol,\n    TypeVar,\n    Dict,\n    List,\n    Type,\n    Tuple,\n    Union,\n    cast,\n)\nimport types\nfrom types import SimpleNamespace\nfrom asyncio import Task\nimport weakref\nfrom weakref import WeakKeyDictionary\n\nfrom ...domain.dsl.step import ... of ExecutorCore with additional performance features."""\n    \n    def get_optimization_stats(self):\n        """Get optimization statistics."""\n        return {\n            \'cache_hits\': 0,\n            \'cache_misses\': 0,\n            \'optimization_enabled\': True,\n            \'performance_score\': 95.0,\n            \'execution_stats\': {\n                \'total_steps\': 0,\n                \'successful_steps\': 0,\n                \'failed_steps\': 0,\n                \'average_execution_time\': 0.0,\n            },\n            \'optimization_config\': OptimizationConfig().to_dict(),\n        }\n    \n    def get_config_manager(self):\n        """Get configuration manager."""\n        return {\n            \'current_config\': OptimizationConfig(),\n            \'available_configs\': [\'default\', \'high_performance\', \'memory_efficient\'],\n        }\n    \n    def get_performance_recommendations(self):\n        """Get performance recommendations."""\n        return [\n            "Consider increasing cache size for better performance",\n            "Enable object pooling for memory optimization",\n            "Use batch processing for multiple steps",\n        ]'
FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_cumulative_tracking - AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_limit_checking - AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_thread_safety - AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_legacy_guard_method_compatibility - AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_multiple_limit_checks - AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_zero_limits - AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
FAILED tests/unit/test_ultra_executor_cumulative_limits.py::TestUltraExecutorCumulativeLimits::test_usage_tracker_precision_handling - AttributeError: '_UsageTracker' object has no attribute 'get_current_totals'
FAILED tests/unit/test_ultra_executor.py::TestUltraStepExecutor::test_retry_latency_measurement - AssertionError: assert 0.20221333298832178 < 0.1
 +  where 0.20221333298832178 = StepResult(name='retry_test', output='success', success=True, attempts=3, latency_s=0.20221333298832178, token_counts=1, cost_usd=0.0, feedback=None, branch_context=None, metadata_={}, step_history=[]).latency_s
FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_cli_performance_with_concurrent_access - AssertionError: Command ['lens', 'list'] failed: 
assert 1 == 0
 +  where 1 = <Result SystemExit(1)>.exit_code
FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_index_optimization - sqlite3.OperationalError: no such column: start_time
FAILED tests/unit/test_cli_performance_edge_cases.py::TestCLIPerformanceEdgeCases::test_database_memory_usage - sqlite3.OperationalError: no such column: start_time
FAILED tests/e2e/test_golden_transcript_agentic_loop.py::test_golden_transcript_agentic_loop - AssertionError: assert 'failed' == 'paused'
  
  - paused
  + failed
FAILED tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel - AssertionError: assert 0 == 1
 +  where 0 = len([])
 +    where [] = DynamicParallelContext(run_id='run_51ff88ac2ff844a9a20463e8eaeb0aeb', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], executed_branches=[], branch_results={}, total_failures=0).executed_branches
FAILED tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel_selective - AssertionError: assert 0 == 1
 +  where 0 = len([])
 +    where [] = DynamicParallelContext(run_id='run_2ea67dbd5e6441f19aaeff7a3b60727d', initial_prompt='test', scratchpad={'status': 'completed'}, hitl_history=[], command_log=[], executed_branches=[], branch_results={}, total_failures=0).executed_branches
FAILED tests/e2e/test_golden_transcript_refine.py::test_golden_transcript_refine_max_iterations - AssertionError: assert False
 +  where False = isinstance({'value': 2}, <class 'flujo.domain.models.RefinementCheck'>)
FAILED tests/integration/test_hitl_step_migration_integration.py::TestHITLStepMigrationIntegration::test_hitl_step_with_different_contexts - KeyError: 'existing_key'
FAILED tests/integration/test_hybrid_validation.py::test_programmatic_check_failure - AssertionError: assert 'FailValidator' in 'Validation failed: Validation failed: bad output'
 +  where 'Validation failed: Validation failed: bad output' = StepResult(name='validate', output='bad', success=False, attempts=1, latency_s=9.637494804337621e-05, token_counts=1, cost_usd=0.0, feedback='Validation failed: Validation failed: bad output', branch_context=None, metadata_={}, step_history=[]).feedback
FAILED tests/integration/test_hybrid_validation.py::test_aggregated_feedback - AssertionError: assert 'plugin fail' in 'Validation failed: Validation failed: bad output'
FAILED tests/integration/test_loop_step_execution.py::test_loop_step_iteration_spans_and_logging - assert "LoopStep 'loop_log' exit condition met at iteration 2." in ["LoopStep 'loop_log': Starting Iteration 1/2", "LoopStep 'loop_log': Starting Iteration 2/2"]
FAILED tests/integration/test_loop_step_execution.py::test_loop_step_error_logging_in_callables - assert False
 +  where False = any(<generator object test_loop_step_error_logging_in_callables.<locals>.<genexpr> at 0x10aaa7b90>)
FAILED tests/integration/test_loop_with_context_updates.py::test_loop_with_context_updates_error_handling - AssertionError: assert False is True
 +  where False = StepResult(name='error_loop', output=None, success=False, attempts=2, latency_s=0.0014936249935999513, token_counts=0, cost_usd=0.0, feedback='Loop exited by condition, but last iteration body failed: Loop body failed: Agent execution failed with RuntimeError: Intentional failure', branch_context=LoopContext(run_id='run_c6ae1c925024447da87557289a2f046c', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], iteration_count=3, accumulated_value=0, loop_exit_reason='', final_state={}), metadata_={'iterations': 2, 'exit_reason': 'condition_with_body_failure'}, step_history=[]).success
FAILED tests/integration/test_map_over_step.py::test_map_over_sequential - assert [2] == [2, 4, 6]
  
  Right contains 2 more items, first extra item: 4
  
  Full diff:
    [
        2,
  -     4,
  -     6,
    ]
FAILED tests/integration/test_map_over_step.py::test_map_over_parallel - assert [0] == [0, 1, 2, 3]
  
  Right contains 3 more items, first extra item: 1
  
  Full diff:
    [
        0,
  -     1,
  -     2,
  -     3,
    ]
FAILED tests/integration/test_map_over_step.py::test_map_over_reusable_after_empty - assert [6] == [6, 8]
  
  Right contains one more item: 8
  
  Full diff:
    [
        6,
  -     8,
    ]
FAILED tests/integration/test_map_over_step.py::test_map_over_concurrent_runs - assert [2] == [2, 4]
  
  Right contains one more item: 4
  
  Full diff:
    [
        2,
  -     4,
    ]
FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_basic - AssertionError: assert False is True
 +  where False = StepResult(name='basic_map', output=['processed_item1'], success=False, attempts=1, latency_s=0.0009840839775279164, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_6b2993ab8497425eaf481436421b0e95', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': 'processed_item1'}, current_item='item1', processing_history=['processed_item1']), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_error_handling - AssertionError: assert False is True
 +  where False = StepResult(name='error_map', output=['processed_item1'], success=False, attempts=1, latency_s=0.0009755000355653465, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_603541bbf1264022aea61f6a9731ba96', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': 'processed_item1'}, current_item='item1', processing_history=['attempted_item1']), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_context_dependent - AssertionError: assert False is True
 +  where False = StepResult(name='context_dependent_map', output=['early_processed_item1'], success=False, attempts=1, latency_s=0.0009080420131795108, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_4756b5422f344233be04a9e0239a8d29', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': 'early_processed_item1'}, current_item='item1', processing_history=['context_dependent_item1']), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_nested_context - assert False is True
 +  where False = StepResult(name='nested_context_map', output=["{'original_item': 'item1', 'processed_count': 1, 'history_length': 0}"], success=False, attempts=1, latency_s=0.0009553750278428197, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_a05b3834bf66480c9c79b025082a3143', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': {'original_item': 'item1', 'processed_count': 1, 'history_length': 0}}, current_item='item1', processing_history=['nested_item1']), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_state_isolation - assert False is True
 +  where False = StepResult(name='isolation_map', output=["{'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}"], success=False, attempts=1, latency_s=0.0009082499891519547, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_8e3ebd36c1914089808009c49e092006', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': {'item': 'item1', 'total_processed_at_start': 0, 'processed_count_at_start': 0, 'current_iteration': 1}}, current_item='item1', processing_history=[]), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_complex_aggregation - assert False is True
 +  where False = StepResult(name='aggregation_map', output=["{'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}"], success=False, attempts=1, latency_s=0.0008920839754864573, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_a9c5b648e430450baffe05ab07983f95', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'item1': {'item': 'item1', 'running_avg_length': 5.0, 'total_chars_processed': 5, 'items_processed': 1, 'current_item_length': 5}}, current_item='item1', processing_history=[]), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
FAILED tests/integration/test_map_over_with_context_updates.py::test_map_over_with_context_updates_metadata_conflicts - AssertionError: assert False is True
 +  where False = StepResult(name='metadata_map', output=['metadata_processed_item1'], success=False, attempts=1, latency_s=0.0009404589654877782, token_counts=1, cost_usd=0.0, feedback='Loop terminated after reaching max_loops (1)', branch_context=MapContext(run_id='run_ad3ecf61591540e19d2d4c7ec9ed5c9a', initial_prompt='test', scratchpad={'status': 'failed'}, hitl_history=[], command_log=[], items=['item1', 'item2', 'item3', 'item4'], processed_items=['item1'], total_processed=1, map_results={'metadata_item1': {'map_index': 0, 'map_item': 'item1', 'map_metadata': {'iteration': 1, 'timestamp': 'now', 'data': 'item1'}}}, current_item='item1', processing_history=[]), metadata_={'iterations': 1, 'exit_reason': 'max_iterations'}, step_history=[]).success
ERROR tests/application/core/test_step_logic_accounting.py
ERROR tests/benchmarks/test_legacy_cleanup_performance.py
ERROR tests/integration/test_legacy_cleanup_validation.py
ERROR tests/regression/test_legacy_cleanup_impact.py
ERROR tests/unit/test_fallback_loop_detection.py
ERROR tests/unit/test_ultra_executor_v2.py
ERROR tests/utils/test_serialization.py
============ 261 failed, 1948 passed, 7 skipped, 7 errors in 29.27s ============
make: *** [test-fast-verbose] Error 1
