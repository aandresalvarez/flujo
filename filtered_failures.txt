tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_merge 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_merge 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_preservation 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_context_preservation 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_mapper_errors 
[gw0] [  5%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_object_oriented_property_detection 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_mapper_errors 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_exit_condition_errors 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_without_is_complex_property 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_exit_condition_errors 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_body_step_failures 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_body_step_failures 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_cost_limits 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_cost_limits 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_token_limits 
[gw0] [  5%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_without_is_complex_property 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_token_limits 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_false_is_complex_property 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_limits_accumulation 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_handle_loop_step_limits_accumulation 
[gw0] [  5%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_false_is_complex_property 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_complex_pipeline_integration 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_complex_pipeline_integration 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_true_is_complex_property 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_nested_control_flow 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_nested_control_flow 
[gw0] [  5%] PASSED tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_steps_with_true_is_complex_property 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_with_caching 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_with_caching 
tests/application/core/test_executor_core.py::TestExecutorCoreObjectOrientedComplexStep::test_validation_steps_backward_compatibility 
tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_with_telemetry 
[gw1] [  5%] PASSED tests/application/core/test_executor_core_loop_step_migration.py::TestLoopStepMigration::test_loopstep_with_telemetry 
--
tests/integration/test_local_tracer.py::test_custom_console_tracer_instance 
[gw0] [ 18%] PASSED tests/integration/test_local_tracer.py::test_custom_console_tracer_instance 
tests/integration/test_local_tracer.py::test_tracer_outputs_info_level 
[gw1] [ 18%] PASSED tests/benchmarks/test_performance_optimizations 2.py::TestPerfCounterPrecision::test_perf_counter_ns_precision 
tests/benchmarks/test_performance_optimizations 2.py::TestSerializationPerformance::test_json_serialization 
[gw0] [ 18%] PASSED tests/integration/test_local_tracer.py::test_tracer_outputs_info_level 
tests/integration/test_local_tracer.py::test_tracer_outputs_debug_level 
[gw0] [ 18%] PASSED tests/integration/test_local_tracer.py::test_tracer_outputs_debug_level 
tests/integration/test_loop_context_update_regression.py::test_regression_first_principles_guarantee 
[gw0] [ 18%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_first_principles_guarantee 
tests/integration/test_loop_context_update_regression.py::test_regression_assertion_catches_merge_failures 
[gw0] [ 18%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_assertion_catches_merge_failures 
tests/integration/test_loop_context_update_regression.py::test_regression_parallel_step_context_updates 
[gw0] [ 18%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_parallel_step_context_updates 
tests/integration/test_loop_context_update_regression.py::test_regression_conditional_step_context_updates 
[gw0] [ 18%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_conditional_step_context_updates 
tests/integration/test_loop_context_update_regression.py::test_regression_edge_case_deep_copy_isolation 
[gw0] [ 18%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_edge_case_deep_copy_isolation 
tests/integration/test_loop_context_update_regression.py::test_regression_serialization_edge_cases 
[gw0] [ 18%] PASSED tests/integration/test_loop_context_update_regression.py::test_regression_serialization_edge_cases 
tests/integration/test_loop_context_update_regression.py::test_regression_performance_under_load 
[gw3] [ 18%] PASSED tests/integration/test_executor_core_architecture_validation.py::TestPerformanceRegression::test_memory_regression 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_execution 
[gw3] [ 18%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_execution 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_failure 
[gw3] [ 18%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_failure 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_not_triggered_on_success 
[gw3] [ 18%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_not_triggered_on_success 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_complex_data_types 
[gw3] [ 18%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_complex_data_types 
tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_context_updates 
[gw3] [ 18%] PASSED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_context_updates 
--
[gw1] [ 99%] FAILED tests/integration/test_pipeline_runner.py::test_runner_unpacks_agent_result 
tests/integration/test_pipeline_runner.py::test_step_config_temperature_passed 
[gw1] [ 99%] PASSED tests/integration/test_pipeline_runner.py::test_step_config_temperature_passed 
tests/integration/test_pipeline_runner.py::test_step_config_temperature_omitted 
[gw1] [ 99%] PASSED tests/integration/test_pipeline_runner.py::test_step_config_temperature_omitted 
tests/integration/test_pipeline_runner.py::test_pipeline_with_temperature_setting 
[gw1] [ 99%] PASSED tests/integration/test_pipeline_runner.py::test_pipeline_with_temperature_setting 
tests/integration/test_pipeline_runner.py::test_step_config_top_k_passed 
[gw1] [100%] PASSED tests/integration/test_pipeline_runner.py::test_step_config_top_k_passed 

=================================== FAILURES ===================================
___________________ test_golden_transcript_dynamic_parallel ____________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/e2e/test_golden_transcript_dynamic_parallel.py:128: in test_golden_transcript_dynamic_parallel
    assert len(final_context.executed_branches) == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = len([])
E    +    where [] = DynamicParallelContext(run_id='run_52fa4a7656d04d73ac6539950cb55e9a', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], executed_branches=[], branch_results={}, total_failures=0).executed_branches
----------------------------- Captured stdout call -----------------------------
[DEBUG] Branch branch1 called with data: test_input
[DEBUG] Branch branch1 context is None: False
[DEBUG] Branch branch1 context.executed_branches before: []
[DEBUG] Branch branch1 context.executed_branches after: ['branch1']
[DEBUG] Branch branch1 context.branch_results: {'branch1': 'branch1_processed_test_input'}
2025-08-07 09:13:57,386 - flujo - INFO - Counting string output as 1 token for step 'branch1': 'branch1_processed_test_input'
[DEBUG] Branch branch2 called with data: test_input
[DEBUG] Branch branch2 context is None: False
[DEBUG] Branch branch2 context.executed_branches before: []
[DEBUG] Branch branch2 context.executed_branches after: ['branch2']
[DEBUG] Branch branch2 context.branch_results: {'branch2': 'branch2_processed_test_input'}
2025-08-07 09:13:57,387 - flujo - INFO - Counting string output as 1 token for step 'branch2': 'branch2_processed_test_input'
[DEBUG] Branch branch3 called with data: test_input
[DEBUG] Branch branch3 context is None: False
[DEBUG] Branch branch3 will fail intentionally
2025-08-07 09:13:57,387 - flujo - WARNING - Step 'branch3' agent execution attempt 1 failed: Intentional failure in branch3
[DEBUG] Branch branch3 called with data: test_input
[DEBUG] Branch branch3 context is None: False
[DEBUG] Branch branch3 will fail intentionally
--
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'branch1': 'branch1_processed_test_input'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'branch2': 'branch2_processed_test_input'
WARNING  flujo:telemetry.py:54 Step 'branch3' agent execution attempt 1 failed: Intentional failure in branch3
ERROR    flujo:telemetry.py:54 Step 'branch3' agent failed after 2 attempts
______________ test_golden_transcript_dynamic_parallel_selective _______________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/e2e/test_golden_transcript_dynamic_parallel.py:204: in test_golden_transcript_dynamic_parallel_selective
    assert len(final_context.executed_branches) == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = len([])
E    +    where [] = DynamicParallelContext(run_id='run_2ac6cc30d3164925afdacf8552333499', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], executed_branches=[], branch_results={}, total_failures=0).executed_branches
----------------------------- Captured stdout call -----------------------------
[DEBUG] Branch branch1 called with data: selective_input
[DEBUG] Branch branch1 context is None: False
[DEBUG] Branch branch1 context.executed_branches before: []
[DEBUG] Branch branch1 context.executed_branches after: ['branch1']
[DEBUG] Branch branch1 context.branch_results: {'branch1': 'branch1_processed_selective_input'}
2025-08-07 09:13:57,505 - flujo - INFO - Counting string output as 1 token for step 'branch1': 'branch1_processed_selective_input'
[DEBUG] Branch branch3 called with data: selective_input
[DEBUG] Branch branch3 context is None: False
[DEBUG] Branch branch3 context.executed_branches before: []
[DEBUG] Branch branch3 context.executed_branches after: ['branch3']
[DEBUG] Branch branch3 context.branch_results: {'branch3': 'branch3_processed_selective_input'}
2025-08-07 09:13:57,506 - flujo - INFO - Counting string output as 1 token for step 'branch3': 'branch3_processed_selective_input'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'branch1': 'branch1_processed_selective_input'
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'branch3': 'branch3_processed_selective_input'
_________________ test_golden_transcript_refine_max_iterations _________________
[gw2] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/e2e/test_golden_transcript_refine.py:142: in test_golden_transcript_refine_max_iterations
    assert isinstance(final_output, RefinementCheck)
--
E   assert 0 == 1
----------------------------- Captured stdout call -----------------------------
2025-08-07 09:13:59,933 - flujo - INFO - Logfire telemetry is disabled or failed to initialize. Using standard Python logging.
2025-08-07 09:14:01,037 - flujo - INFO - Initialized SQLite database at /private/var/folders/hh/df2rcx0n7nnb2y0hxn4dzps40000gn/T/pytest-of-alvaro/pytest-1501/popen-gw3/test_resume_after_crash_sqlite0/state.db
2025-08-07 09:14:01,055 - flujo - INFO - Saved state for run_id=sqlite_run
2025-08-07 09:14:01,057 - flujo - INFO - Counting string output as 1 token for step 'transform': 'middle'
___________ test_dynamic_router_with_context_updates_error_handling ____________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_dynamic_router_with_context_updates.py:348: in test_dynamic_router_with_context_updates_error_handling
    assert "branch 'failing_branch' failed" in result.step_history[-1].feedback.lower()
E   assert "branch 'failing_branch' failed" in 'parallel step failed with 1 branch failures'
E    +  where 'parallel step failed with 1 branch failures' = <built-in method lower of str object at 0x10a2c66d0>()
E    +    where <built-in method lower of str object at 0x10a2c66d0> = 'Parallel step failed with 1 branch failures'.lower
E    +      where 'Parallel step failed with 1 branch failures' = StepResult(name='error_router', output={'failing_branch': StepResult(name='error_router_failing_branch', output=None, success=False, attempts=1, latency_s=0.00037183298263698816, token_counts=0, cost_usd=0.0, feedback='Agent execution failed with RuntimeError: Intentional router branch failure', branch_context=RouterContext(run_id='run_f37dae9848454c7bba83922545a63da2', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], router_state='executed_failing', branch_executed='failing_branch', branch_count=2, total_updates=0, router_metadata={}, branch_results={}), metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0007107920246198773, token_counts=0, cost_usd=0.0, feedback='Parallel step failed with 1 branch failures', branch_context=RouterContext(run_id='run_f37dae9848454c7bba83922545a63da2', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], router_state='executed_failing', branch_executed='failing_branch', branch_count=2, total_updates=0, router_metadata={}, branch_results={}), metadata_={'executed_branches': ['failing_branch']}, step_history=[]).feedback
----------------------------- Captured stdout call -----------------------------
2025-08-07 09:14:01,527 - flujo - WARNING - Step 'failing_branch' agent execution attempt 1 failed: Intentional router branch failure
2025-08-07 09:14:01,528 - flujo - WARNING - Step 'error_router' failed. Halting pipeline execution.
----------------------------- Captured stderr call -----------------------------
2025-08-07 09:14:01,528 - flujo - ERROR - Step 'failing_branch' agent failed after 2 attempts
------------------------------ Captured log call -------------------------------
WARNING  flujo:telemetry.py:54 Step 'failing_branch' agent execution attempt 1 failed: Intentional router branch failure
ERROR    flujo:telemetry.py:54 Step 'failing_branch' agent failed after 2 attempts
WARNING  flujo:telemetry.py:54 Step 'error_router' failed. Halting pipeline execution.
________ TestComponentIntegration.test_component_interface_optimization ________
[gw3] darwin -- Python 3.11.0 /Users/alvaro/Documents/Code/flujo/.venv/bin/python3
tests/integration/test_executor_core_architecture_validation.py:149: in test_component_interface_optimization
    assert serializer.serialize_calls > 0, "Serializer should be called"
E   AssertionError: Serializer should be called
E   assert 0 > 0
E    +  where 0 = <tests.integration.test_executor_core_architecture_validation.MockSerializer object at 0x10a5890d0>.serialize_calls
----------------------------- Captured stdout call -----------------------------
2025-08-07 09:14:01,729 - flujo - INFO - Counting string output as 1 token for step 'test_step': 'interface_test'
------------------------------ Captured log call -------------------------------
INFO     flujo:telemetry.py:54 Counting string output as 1 token for step 'test_step': 'interface_test'
--
tests/utils/test_serialization_registry.py:44
tests/utils/test_serialization_registry.py:44
tests/utils/test_serialization_registry.py:44
  /Users/alvaro/Documents/Code/flujo/tests/utils/test_serialization_registry.py:44: PytestCollectionWarning: cannot collect test class 'TestDataclass' because it has a __init__ constructor (from: tests/utils/test_serialization_registry.py)
    @dataclass

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel - AssertionError: assert 0 == 1
 +  where 0 = len([])
 +    where [] = DynamicParallelContext(run_id='run_52fa4a7656d04d73ac6539950cb55e9a', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], executed_branches=[], branch_results={}, total_failures=0).executed_branches
FAILED tests/e2e/test_golden_transcript_dynamic_parallel.py::test_golden_transcript_dynamic_parallel_selective - AssertionError: assert 0 == 1
 +  where 0 = len([])
 +    where [] = DynamicParallelContext(run_id='run_2ac6cc30d3164925afdacf8552333499', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], executed_branches=[], branch_results={}, total_failures=0).executed_branches
FAILED tests/e2e/test_golden_transcript_refine.py::test_golden_transcript_refine_max_iterations - AssertionError: assert False
 +  where False = isinstance({'feedback': 'value_2', 'original_input': {'initial': 'data'}}, <class 'flujo.domain.models.RefinementCheck'>)
FAILED tests/application/core/test_executor_core_loop_step.py::TestExecutorCoreLoopStep::test_handle_loop_step_signature - AssertionError: assert False
 +  where False = <built-in method issubset of set object at 0x106ed3840>({'_fallback_depth', 'context', 'context_setter', 'data', 'limits', 'resources', ...})
 +    where <built-in method issubset of set object at 0x106ed3840> = {'context', 'context_setter', 'data', 'limits', 'loop_step', 'resources'}.issubset
FAILED tests/application/core/test_step_logic_accounting.py::test_successful_fallback_preserves_metrics - assert 2 == 1
 +  where 2 = <test_step_logic_accounting.StubAgent object at 0x107609310>.call_count
FAILED tests/application/core/test_step_logic_accounting.py::test_failed_fallback_accumulates_metrics - assert 2 == 1
 +  where 2 = <test_step_logic_accounting.StubAgent object at 0x1051b8b10>.call_count
FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_error_handling - AssertionError: assert 'Error executing conditional logic or branch' in 'Error executing conditional step: Test error'
 +  where 'Error executing conditional step: Test error' = StepResult(name='test_conditional', output=None, success=False, attempts=1, latency_s=0.0010809999657794833, token_counts=0, cost_usd=0.0, feedback='Error executing conditional step: Test error', branch_context=None, metadata_={}, step_history=[]).feedback
FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_parameter_passing - AssertionError: assert <Mock name='mock.model_copy()' id='4946054288'> == <Mock id='4945174864'>
FAILED tests/application/core/test_executor_core_conditional_step.py::TestExecutorCoreConditionalStep::test_handle_conditional_step_with_limits_and_context_setter - AssertionError: Expected 'mock' to have been called once. Called 0 times.
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_condition_evaluation_failure - AssertionError: assert 'Error executing conditional logic or branch' in 'Error executing conditional step: Condition failed'
 +  where 'Error executing conditional step: Condition failed' = StepResult(name='test_conditional', output=None, success=False, attempts=1, latency_s=2.0916981156915426e-05, token_counts=0, cost_usd=0.0, feedback='Error executing conditional step: Condition failed', branch_context=None, metadata_={}, step_history=[]).feedback
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_not_found_no_default - assert "No branch found for key 'nonexistent_branch'" in ''
 +  where '' = StepResult(name='test_conditional', output=None, success=False, attempts=1, latency_s=0.00011029199231415987, token_counts=0, cost_usd=0.0, feedback='', branch_context=None, metadata_={}, step_history=[]).feedback
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_execution_failure - assert "Failure in branch 'branch_a'" in 'Step failed'
 +  where 'Step failed' = StepResult(name='test_conditional', output=None, success=False, attempts=1, latency_s=9.404204320162535e-05, token_counts=0, cost_usd=0.0, feedback='Step failed', branch_context=None, metadata_={'executed_branch_key': 'branch_a'}, step_history=[]).feedback
FAILED tests/application/core/test_executor_core_conditional_step_logic.py::TestExecutorCoreConditionalStepLogic::test_branch_output_mapping - AssertionError: Expected 'branch_output_mapper' to be called once. Called 0 times.
--
FAILED tests/integration/test_agentic_loop_recipe.py::test_pause_preserves_command_log - AssertionError: assert 1 == 0
 +  where 1 = len([AskHumanCommand(type='ask_human', question='Need input')])
 +    where [AskHumanCommand(type='ask_human', question='Need input')] = PipelineContext(run_id='run_8b0b35672d604dd3b97045efdada251f', initial_prompt='goal', scratchpad={'status': 'running', 'paused_step_input': AskHumanCommand(type='ask_human', question='Need input')}, hitl_history=[], command_log=[AskHumanCommand(type='ask_human', question='Need input')]).command_log
FAILED tests/integration/test_agentic_loop_recipe.py::test_sync_resume - flujo.exceptions.OrchestratorError: Pipeline is not paused
FAILED tests/integration/test_agentic_loop_recipe.py::test_max_loops_failure - assert 1 >= 2
 +  where 1 = len([ExecutedCommandLog(turn=1, generated_command=RunAgentCommand(type='run_agent', agent_name='x', input_data=1), execution_result="Error: Agent 'x' not found.", timestamp=datetime.datetime(2025, 8, 7, 16, 14, 0, 355071, tzinfo=datetime.timezone.utc))])
 +    where [ExecutedCommandLog(turn=1, generated_command=RunAgentCommand(type='run_agent', agent_name='x', input_data=1), execution_result="Error: Agent 'x' not found.", timestamp=datetime.datetime(2025, 8, 7, 16, 14, 0, 355071, tzinfo=datetime.timezone.utc))] = PipelineContext(run_id='run_b69c9aeff8844ef5a6d5391f2c9d2aa2', initial_prompt='goal', scratchpad={'status': 'running'}, hitl_history=[], command_log=[ExecutedCommandLog(turn=1, generated_command=RunAgentCommand(type='run_agent', agent_name='x', input_data=1), execution_result="Error: Agent 'x' not found.", timestamp=datetime.datetime(2025, 8, 7, 16, 14, 0, 355071, tzinfo=datetime.timezone.utc))]).command_log
FAILED tests/integration/test_as_step_composition.py::test_as_step_context_inheritance_error - Failed: DID NOT RAISE <class 'flujo.exceptions.ContextInheritanceError'>
FAILED tests/integration/test_as_step_composition.py::test_direct_context_inheritance_error - Failed: DID NOT RAISE <class 'flujo.exceptions.ContextInheritanceError'>
FAILED tests/integration/test_crash_recovery.py::test_resume_after_crash_sqlite_backend - assert 0 == 1
FAILED tests/integration/test_dynamic_router_with_context_updates.py::test_dynamic_router_with_context_updates_error_handling - assert "branch 'failing_branch' failed" in 'parallel step failed with 1 branch failures'
 +  where 'parallel step failed with 1 branch failures' = <built-in method lower of str object at 0x10a2c66d0>()
 +    where <built-in method lower of str object at 0x10a2c66d0> = 'Parallel step failed with 1 branch failures'.lower
 +      where 'Parallel step failed with 1 branch failures' = StepResult(name='error_router', output={'failing_branch': StepResult(name='error_router_failing_branch', output=None, success=False, attempts=1, latency_s=0.00037183298263698816, token_counts=0, cost_usd=0.0, feedback='Agent execution failed with RuntimeError: Intentional router branch failure', branch_context=RouterContext(run_id='run_f37dae9848454c7bba83922545a63da2', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], router_state='executed_failing', branch_executed='failing_branch', branch_count=2, total_updates=0, router_metadata={}, branch_results={}), metadata_={}, step_history=[])}, success=False, attempts=1, latency_s=0.0007107920246198773, token_counts=0, cost_usd=0.0, feedback='Parallel step failed with 1 branch failures', branch_context=RouterContext(run_id='run_f37dae9848454c7bba83922545a63da2', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], router_state='executed_failing', branch_executed='failing_branch', branch_count=2, total_updates=0, router_metadata={}, branch_results={}), metadata_={'executed_branches': ['failing_branch']}, step_history=[]).feedback
FAILED tests/integration/test_executor_core_architecture_validation.py::TestComponentIntegration::test_component_interface_optimization - AssertionError: Serializer should be called
assert 0 > 0
 +  where 0 = <tests.integration.test_executor_core_architecture_validation.MockSerializer object at 0x10a5890d0>.serialize_calls
FAILED tests/integration/test_legacy_cleanup_validation.py::TestFunctionRemovalValidation::test_dynamic_router_logic_removal - AssertionError: Missing parameter: step
assert 'step' in mappingproxy(OrderedDict([('router_step', <Parameter "router_step: 'DynamicParallelRouterStep[Any]'">), ('data', <Parameter "data: 'Any'">), ('context', <Parameter "context: 'Optional[TContext_w_Scratch]'">), ('resources', <Parameter "resources: 'Optional[Any]'">), ('limits', <Parameter "limits: 'Optional[UsageLimits]'">), ('context_setter', <Parameter "context_setter: 'Optional[Callable[[PipelineResult[Any], Optional[Any]], None]]'">)]))
 +  where mappingproxy(OrderedDict([('router_step', <Parameter "router_step: 'DynamicParallelRouterStep[Any]'">), ('data', <Parameter "data: 'Any'">), ('context', <Parameter "context: 'Optional[TContext_w_Scratch]'">), ('resources', <Parameter "resources: 'Optional[Any]'">), ('limits', <Parameter "limits: 'Optional[UsageLimits]'">), ('context_setter', <Parameter "context_setter: 'Optional[Callable[[PipelineResult[Any], Optional[Any]], None]]'">)])) = <Signature (router_step: 'DynamicParallelRouterStep[Any]', data: 'Any', context: 'Optional[TContext_w_Scratch]', resources: 'Optional[Any]', limits: 'Optional[UsageLimits]', context_setter: 'Optional[Callable[[PipelineResult[Any], Optional[Any]], None]]') -> 'StepResult'>.parameters
FAILED tests/integration/test_legacy_cleanup_validation.py::TestLegacyFunctionIntegration::test_backward_compatibility_maintained - assert 'loop_step' in mappingproxy(OrderedDict([('step', <Parameter "step: 'Any'">), ('data', <Parameter "data: 'Any'">), ('context', <Parameter "context: 'Optional[Any]'">), ('resources', <Parameter "resources: 'Optional[Any]'">), ('limits', <Parameter "limits: 'Optional[UsageLimits]'">), ('context_setter', <Parameter "context_setter: 'Optional[Callable[[Any, Optional[Any]], None]]'">), ('_fallback_depth', <Parameter "_fallback_depth: 'int' = 0">)]))
 +  where mappingproxy(OrderedDict([('step', <Parameter "step: 'Any'">), ('data', <Parameter "data: 'Any'">), ('context', <Parameter "context: 'Optional[Any]'">), ('resources', <Parameter "resources: 'Optional[Any]'">), ('limits', <Parameter "limits: 'Optional[UsageLimits]'">), ('context_setter', <Parameter "context_setter: 'Optional[Callable[[Any, Optional[Any]], None]]'">), ('_fallback_depth', <Parameter "_fallback_depth: 'int' = 0">)])) = <Signature (step: 'Any', data: 'Any', context: 'Optional[Any]', resources: 'Optional[Any]', limits: 'Optional[UsageLimits]', context_setter: 'Optional[Callable[[Any, Optional[Any]], None]]', _fallback_depth: 'int' = 0) -> 'StepResult'>.parameters
FAILED tests/integration/test_legacy_cleanup_validation.py::TestLegacyCleanupSafety::test_error_handling_preserved - assert False
 +  where False = any(<generator object TestLegacyCleanupSafety.test_error_handling_preserved.<locals>.<genexpr> at 0x126adf140>)
FAILED tests/integration/test_executor_core_fallback_integration.py::TestExecutorCoreFallbackIntegration::test_real_fallback_with_validation_failure - assert False is True
 +  where False = StepResult(name='validation_primary', output='invalid output', success=False, attempts=2, latency_s=0.00042929197661578655, token_counts=1, cost_usd=0.0, feedback="Validation failed after max retries: Validator FailingValidator failed: TestExecutorCoreFallbackIntegration.test_real_fallback_with_validation_failure.<locals>.FailingValidator.validate() got an unexpected keyword argument 'context'", branch_context=None, metadata_={}, step_history=[]).success
FAILED tests/integration/test_loop_context_update_regression.py::test_regression_performance_under_load - AssertionError: assert False is True
 +  where False = RegressionTestContext(run_id='run_96b46bc958664beeb0f7dff1da5d0bc1', initial_prompt='test', scratchpad={'status': 'running'}, hitl_history=[], command_log=[], iteration_count=2, accumulated_value=2, is_complete=False, debug_data={'performance_item_1_0': 0, 'performance_item_1_1': 1, 'performance_item_1_2': 2, 'performance_item_1_3': 3, 'performance_item_1_4': 4, 'performance_item_1_5': 5, 'performance_item_1_6': 6, 'performance_item_1_7': 7, 'performance_item_1_8': 8, 'performance_item_1_9': 9, 'performance_item_1_10': 10, 'performance_item_1_11': 11, 'performance_item_1_12': 12, 'performance_item_1_13': 13, 'performance_item_1_14': 14, 'performance_item_1_15': 15, 'performance_item_1_16': 16, 'performance_item_1_17': 17, 'performance_item_1_18': 18, 'performance_item_1_19': 19, 'performance_item_1_20': 20, 'performance_item_1_21': 21, 'performance_item_1_22': 22, 'performance_item_1_23': 23, 'performance_item_1_24': 24, 'performance_item_1_25': 25, 'performance_item_1_26': 26, 'performance_item_1_27': 27, 'performance_item_1_28': 28, 'performance_item_1_29': 29, 'performance_item_1_30': 30, 'performance_item_1_31': 31, 'performance_item_1_32': 32, 'performance_item_1_33': 33, 'p...ce_item_2_961': 961, 'performance_item_2_962': 962, 'performance_item_2_963': 963, 'performance_item_2_964': 964, 'performance_item_2_965': 965, 'performance_item_2_966': 966, 'performance_item_2_967': 967, 'performance_item_2_968': 968, 'performance_item_2_969': 969, 'performance_item_2_970': 970, 'performance_item_2_971': 971, 'performance_item_2_972': 972, 'performance_item_2_973': 973, 'performance_item_2_974': 974, 'performance_item_2_975': 975, 'performance_item_2_976': 976, 'performance_item_2_977': 977, 'performance_item_2_978': 978, 'performance_item_2_979': 979, 'performance_item_2_980': 980, 'performance_item_2_981': 981, 'performance_item_2_982': 982, 'performance_item_2_983': 983, 'performance_item_2_984': 984, 'performance_item_2_985': 985, 'performance_item_2_986': 986, 'performance_item_2_987': 987, 'performance_item_2_988': 988, 'performance_item_2_989': 989, 'performance_item_2_990': 990, 'performance_item_2_991': 991, 'performance_item_2_992': 992, 'performance_item_2_993': 993, 'performance_item_2_994': 994, 'performance_item_2_995': 995, 'performance_item_2_996': 996, 'performance_item_2_997': 997, 'performance_item_2_998': 998, 'performance_item_2_999': 999}).is_complete
FAILED tests/integration/test_as_step_state_persistence.py::test_as_step_state_persistence_and_resumption - assert 0 == 1
FAILED tests/integration/test_loop_step_execution.py::test_iteration_mapper_not_called_on_max_loops - assert [1, 2] == [1]
  
  Left contains one more item: 2
  
  Full diff:
